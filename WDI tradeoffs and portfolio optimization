---
title: "Tradeoffs, risk, and optimization of investments across strategic objectives"
author: "Ben Schiek"
date: "August 29, 2019"
output:
  pdf_document:
    toc: true
    toc_depth: 4
    latex_engine: xelatex
mainfont: Calibri Light

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## 1. Introduction
### 1.1 Problem background
At the most abstract levels of strategic planning, it is customary for large development agencies and donor organizations to carefully define a manageable number of thematic areas for investment that broadly cover all aspects of development. The 17 Sustainable Development Goals (SDG), for example, are grouped  Persons (poverty reduction and human rights), Environmental Sustainability, Economic Prosperity, Peace, and Alliances ().

* Economic growth

* Economic equality, poverty reduction

* Food and nutritional security

* Environmental protection, conservation, reduced C02 emissions, etc.

* Health


...

### 1.2 Proposed solution

In this paper, I propose a method to identify and quantify such tradeoffs in the SDG Tracker historical data. I then combine this with methods from financial analysis to determine a donor's optimal allocation of investments across the indicators given the donor's level of risk tolerance.

#### 1.2.1 Eigendecomposition of SDG Tracker data

The method for identifying and quantifying tradeoffs is based on the eigendecomposition of the correlation matrix of SDG Tracker data. First, signals are separated from noise using a method developed in the analysis of physical systems (). The components of the retained eigenvectors are then interpreted as the magnitude and direction of the influence (a.k.a. "loading" or "score") of each SDG Tracker over the average movement of the dataset. Loadings with opposite signs indicate a tradeoff, while loadings with the same sign are indicative of synergy.

Beyond tradeoffs, this method is also useful for identifying just a few key crosscutting tendencies that are broadly characteristic of the entire dataset. In other words, the loadings on each eigenvector tend to be thematically organized, such that each eigenvector captures a particular aspect of the overall evolution of the system. This effectively reduces the problem of making sense of several dozen complexly interacting indicators to the more mangeable problem of making sense of just a few crosscutting trends.

#### 1.2.2 Risk adjusted portfolio optimization

The identification of tradeoffs and key crosscutting trends is useful in and of itself for stimulating and orienting high level policy discussion. However, longstanding methods used in the analogous decisionmaking context of finance are motivation to aim for more. In particular, a variant of Minimum Variance (MV) analysis can be applied to determine the optimal allocation of a budget across the---potentially several dozen---SDG indicators, given a certain risk tolerance.

It is worth taking a moment to emphasize that this goes far beyond the "priority setting", with which many donors, development agencies, and research for development (R4D) institutions may be familiar. The method proposed here, adapted from finance, determines, _precisely_, the portion of the donor's/development agency's/R4D research institution's budget that must be allocated in order to maximize net benefit, given a certain risk tolerance specified by the donor. (The budget can be a budget in the literal sense, or an "attention" or "enthusiasm", etc., budget.)

MV analysis currently suffers under longstanding methodological issues that severely limit its usefulness in any context, financial or otherwise (). In a nutshell, noise in the data makes accurate estimation of returns and correlations difficult. This means that one is optimizing over a great deal of noise. In the present adaptation of MV analyis, I propose to define the returns vector as a linear combination of the retained correlation matrix eigenvectors. This, in turn, automatically purges the correlation matrix of noisy elements. The problem is then set up for optimization over signal, not noise.

non-market valuation of each of the cross-cutting tendencies associated with each eigenvector---i.e. it endogenously determines the development agency's implicit willingness to pay for more of this or that cross-cutting tendency. This could be used to determine a given agency's investment preferences based on their observed spending. It could also become important later on in the formation of SDG markets (analogous to carbon markets) and/or initial coin offerings.

Many in the international development community may be unfamiliar with MV analysis. To build motivation and intuition for its application in the development context, I first walk through an example of its application in the financial context where it originated.

### 1.3 Summary of results

In the SDG Tracker data, three signals are pulled from the noise. Loadings on the first eigenvector are evenly distributed. Loadings on the second eigenvector eigenvector captures agricultural and/or natural resource intensive development, associated with relatively higher emissions and low industrial development, trade, and overall GDP growth.
...

Before addressing the problem of tradeoffs and optimization within the motivating context of this paper, it is instructive to first address these issues in the financial context where the methods employed here are rooted.


## 2. Methodological precedents

### 2.1 Separating signal from noise

Much of the present work is inspired by two papers in particular. In the first of these, so and so () showed how the important eigenvectors from noisy.... Their work, in turn, is adapted from the work of so and so (), who were interested in identifying the meaningful components of complex physical systems. A key theorem that emerged from this work says that the distribution of the eigenvalues of any random correlation matrix is a function of the dimensions of the underlying matrix (). It is therefore easy to compare the eigenvalue distribution of a given data correlation matrix against that of a randomly generated matrix. The eigenvalues of the data correlation matrix that lie outside of the random matrix eigenvalue distribution can be identified as corresponding to components of the system that can be meaningfully distinguished from noise.

An important implication of the theorem is that signal extraction is as much a function of data quality (i.e. whether there really is structure in the observed system, and how well this is captured in the data) as it is of data quantity. If none or very few of the eigenvalues of a given data correlation matrix can be distinguished from noise, this may be either because there is no structure to be found in the observed system, or because there are not enough observations to fully flesh out the structure. Just as a low resolution image of a person is difficult to distinguish from an image of randomly shaded pixels, underlying structure in the SDG Tracker data may be difficult to discern due to the low number of observations currently available. As resolution (number of observations) increases, the components of the system become more clearly rendered. 

It is surprising that the rigorous extraction method described above has not seen more uptake among data scientists. The problem of signal extraction extends far beyond the specific physical and financial settings addressed by so and so and so and so. It comes up every time someone conducts a principle components analysis (PCA) or factor analysis (FA). Moreover, the extraction methods appearing in published, peer reviewed PCA and FA studies are, by comparison, little more than arbitrary rules of thumb. "Keep only the signals that describe 90% of the variation", for example, or "the elbow rule", or "keep only those with eigenvalues greater than 1". The perils of such cavalier methods have been documented (), and yet they persist. Their persistence is made more perplexing by the availability---since the 1960s---of the rigorous extraction method described above.

### 2.2 Signal characterization

So and so () did not go on to interpret the signals after presenting their method for extracting them. In the second paper inspiring the present work, so and so () addressed this question .

Here I adapt and combine these two advances into a single workflow.

### 2.3 Minimum variance analysis, and adaptation

$$R = \mathbf{w \cdot r}, \:\:\: C=\mathbf{w\cdot1}, \:\:\: V = \mathbf{w\cdot K \cdot w} \tag{1}$$


$$\max_{\mathbf{w}}{R} \:\:\:\:s.t. \:\:\: C=\overline{C} \:, \:\:\: V = \overline{V} \tag{2}$$

$$\mathcal{L} = R - \lambda_C(C - \overline{C}) - \lambda_{V}(V - \overline{V}) \tag{3}$$

$$\nabla \mathcal{L} = \mathbf{r} - \lambda_C \mathbf{1} - 2 \lambda_{V} K \cdot \mathbf{w} = \mathbf{0} \tag{4}$$

$$R^* = \lambda_C \overline{C} + 2 \lambda_V \overline{V} \tag{5}$$
Note this implies that the risk shadow price is proportional to the expected reward to risk ratio.

$$\lambda_V = \frac{1}{2} \frac{R^* - \lambda_C \overline{C}}{\overline{V}} \tag{6}$$


$$K^{-1} \cdot (\mathbf{r} - \lambda_C \mathbf{1}) = 2 \lambda_{V} \mathbf{w}^* \tag{5}$$

$$K^{-1} \cdot [\mathbf{r}, \: \mathbf{1}] \left[\begin{matrix} 1 \\ -\lambda_C \\ \end{matrix} \right] = 2 \lambda_{V} \mathbf{w}^* \tag{6}$$

$$ M\left[\begin{matrix}
1 \\
-\lambda_C \\
\end{matrix} \right] = 2 \lambda_{V} \left[\begin{matrix}
R \\
\bar{C} \\
\end{matrix} \right] \tag{7}$$

where 

$$M = [\mathbf{r}, \: \mathbf{1}]' \cdot K^{-1} \cdot [\mathbf{r}, \: \mathbf{1}] \tag{8}$$

$$\left[\begin{matrix}
1 \\
-\lambda_C \\
\end{matrix} \right] = 2 \lambda_{V} M^{-1} \left[\begin{matrix}
R \\
\bar{C} \\
\end{matrix} \right] \tag{9}$$

However, mean returns are notoriously difficult to measure.

$$\mathbf{r} = \tilde{P} \tilde{\Lambda} \cdot \mathbf{1}= \lambda_{1} \mathbf{p_1} + \lambda_{2} \mathbf{p_2} + \cdots +\lambda_{k} \mathbf{p_k} \tag{10}$$

$$K = P \Lambda P' \tag{11}$$

$$M = [\tilde{P}, \: \mathbf{1}]' \cdot K^{-1} \cdot [\tilde{P}, \: \mathbf{1}] \tag{12}$$

or

$$M = \left[\begin{matrix} 1/ \lambda_{1} & 0 & \cdots & 0 & 1/ \lambda_1 \mathbf{p_1} \cdot \mathbf{1} \\ 0 & 1/ \lambda_{2} & \cdots & 0 & 1/ \lambda_2 \mathbf{p_2} \cdot \mathbf{1} \\ \vdots & \vdots & \ddots & 0 & \vdots \\ 0 & 0 & 0 & 1/ \lambda_{k} & 1/ \lambda_k \mathbf{p_k} \cdot \mathbf{1}  \\ 1/ \lambda_1 \mathbf{1} \cdot \mathbf{p_1} & 1/ \lambda_2 \mathbf{1} \cdot \mathbf{p_2} & \cdots & 1/ \lambda_k \mathbf{1} \cdot \mathbf{p_k} & \mathbf{1} \cdot K \cdot \mathbf{1} \end{matrix} \right]$$

<!-- $$\left[\begin{matrix} \gamma_{1} \\ \gamma_{2} \\ \vdots \\ \gamma_{k} \\ -\gamma_C \\ \end{matrix} \right] = 2 \gamma_v \tilde{\Lambda} \left[\begin{matrix} r_1 \\ r_2 \\ \vdots \\ r_k \\ \bar{C} \\ \end{matrix} \right] - \gamma_c \left[ \begin{matrix} \mathbf{p}_1 \cdot \mathbf{1} \end{matrix} \right] \tag{13}$$ -->


$$\gamma_i = 2 \gamma_v \lambda_i r_i + \gamma_c \mathbf{p}_i \cdot \mathbf{1} \: ; \:\:\: i=1,2,\cdots,k \tag{}$$

$$\gamma_c =  \gamma_1 / \lambda_1 \mathbf{1} \cdot \mathbf{p_1} + \gamma_2 / \lambda_2 \mathbf{1} \cdot \mathbf{p_2} + \cdots + \gamma_k / \lambda_k \mathbf{1} \cdot \mathbf{p_k} + \gamma_c \mathbf{1} \cdot K \cdot \mathbf{1}$$

$$ -\frac{1}{2 \gamma_v}[\tilde{P}\tilde{\Lambda}^{-1}, \mathbf{1}] \left[\begin{matrix} \gamma_{1} \\ \gamma_{2} \\ \vdots \\ \gamma_{k} \\ -\gamma_c \\ \end{matrix} \right] = \mathbf{w}^* \tag{14}$$

$$-\frac{1}{2 \gamma_v}[\tilde{P}\tilde{\Lambda}^{-1}, \mathbf{1}] \cdot \mathbf{\gamma} =  \mathbf{w}^* \tag{5}$$


## 3. Data

Yahoo data ...The data below are intended to be representative of what a broad-minded investor might have on their radar screen these days, covering a wide range of asset classes.

WDI data . This is the data that the UN (SDG Tracker) uses to track progress towards SDGs. As mentioned above...low resolution...we are coming to a point where there are enough years of enough variables that we can begin to glimpse the contours of the main components. It will be exciting...as the picture comes slowly into greater and greater focus.


```{r, fig.width=14, fig.height=14, fig.align='center', echo = FALSE}

#setwd("D:/OneDrive - CGIAR/Documents")
#options(warn = -1); options(scipen = 999)
#-------------------------------------------------------------
#devtools::install_github("thomasp85/patchwork")
library(plyr)
# library(tidyverse)
# library(ggplot2)
library(zoo)
# library(FactoMineR)
# library(factoextra)
# library(Hmisc)
# library(corrplot)
library(tidyquant)
library(patchwork)
#=======================================================================
# Resources:
# Correlation matrices:
# http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software
# https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html
#=======================================================================
# Define functions
#=======================================================================
signals_from_noise <- function(mat_pctDiff, fun_env = NULL){
  
  if(is.null(fun_env)){
    eigenvalue_density_plot = T
    signals_plot = T
    pca_var_plot = T
    pca_ind_plot = T
    group_info = NULL
    quietly = F
  }else{
    eigenvalue_density_plot = fun_env[[1]]
    signals_plot = fun_env[[2]]
    pca_var_plot = fun_env[[3]]
    pca_ind_plot = fun_env[[4]]
    group_info = fun_env[[5]]
    quietly = fun_env[[6]]
    
  }
  #---------------------------------------------------------
  # Separate signals from noise
  #---------------------------------------------------------
  res <- FactoMineR::PCA(mat_pctDiff, ncp = ncol(mat_pctDiff), graph = F)
  eigvals <- as.data.frame(res$eig)$eigenvalue
  eigval_max <- max(eigvals)
  mat_loads <- res$var$coord
  mat_loads_rot <- varimax(mat_loads)[[1]]
  mat_eigvecs <- mat_loads %*% diag(1 / sqrt(eigvals))
  #---------------------------------------------------------
  # Apply random matrix theory () to determine eigenvalue distribution of a 
  # correlation matrix of random data.
  n_obs <- nrow(mat_pctDiff)
  n_items <- ncol(mat_pctDiff)
  Q <- n_obs / n_items
  s_sq <- 1 - eigval_max / n_items
  #s_sq <- 1
  eigval_rand_max <- s_sq * (1 + 1 / Q + 2 / sqrt(Q))
  eigval_rand_min <- s_sq * (1 + 1 / Q - 2 / sqrt(Q))
  lam <- seq(eigval_rand_min, eigval_rand_max, 0.001)
  eigval_rand_density <- Q / (2 * pi * s_sq) * sqrt((eigval_rand_max - lam) * (lam - eigval_rand_min)) / lam
  df_plot <- data.frame(Eigenvalues = eigvals)
  
  #   N_t <- nrow(mat_pctDiff)
  # N_c <- ncol(mat_pctDiff)
  # Q <- N_t / N_c
  # s_sq <- 1 - eigval_max / N_c
  # #s_sq <- 1
  # eigval_rand_max <- s_sq * (1 + 1 / Q + 2 / sqrt(Q))
  # eigval_rand_min <- s_sq * (1 + 1 / Q - 2 / sqrt(Q))
  # lam <- seq(eigval_rand_min, eigval_rand_max, 0.001)
  # dens_rand <- Q / (2 * pi * s_sq) * sqrt((eigval_rand_max - lam) * (lam - eigval_rand_min)) / lam
  # df_e <- data.frame(eigenvalues = eigvals)
  
  
  #---------------------------------------------------------
  # Plot eigenvalue density vs. random matrix eigenvalue density
  if(eigenvalue_density_plot){
    gg <- ggplot()
    gg <- gg + geom_density(data = df_plot, aes(x = Eigenvalues, color = "Correlation Matrix"), lwd = 1.1)
    gg <- gg + geom_line(data = data.frame(x = lam, y = eigval_rand_density), aes(x = x, y = y, color = "Random matrix"), lwd = 1.1)
    gg <- gg + scale_colour_manual(name = "Eigenvalue density", 
                                   values = c(`Correlation Matrix` = "blue", `Random matrix` = "magenta"))
    gg_eigenval_density <- gg
    if(!signals_plot){print(gg_eigenval_density)}
  }
  #---------------------------------------------------------
  # Which data eigenvalues can be meaningfully distinguished from noise?
  ind_deviating_from_noise <- which(eigvals > eigval_rand_max) # (eigval_rand_max + 5 * 10^-1))
  #---------------------------------------------------------
  # Extract signal loadings matrix from noise
  mat_loads_sig <- mat_loads[, ind_deviating_from_noise]
  eigvals_sig <- eigvals[ind_deviating_from_noise]
  mat_loads_rot_sig <- mat_loads_rot[, ind_deviating_from_noise]
  #---------------------------------------------------------
  n_signals <- length(eigvals_sig)
  if(!quietly){print(paste("Number of signals: ", n_signals))}
  #---------------------------------------------------------
  # Get dimensionally reduced version of original input data
  mat_eigvecs_sig <- mat_eigvecs[, ind_deviating_from_noise]
  mat_inData_sig <- mat_pctDiff %*% mat_eigvecs_sig
  if(n_signals == 1){
    mat_inData_sig <- mat_inData_sig / eigvals_sig
  }else{
    mat_inData_sig <- mat_inData_sig %*% diag(1 / eigvals_sig)
  }
  #---------------------------------------------------------
  # Set sign of eigenvectors such that they
  # best conform to the input time series
  inData_avg <- rowMeans(mat_pctDiff)
  if(n_signals == 1){
    mse <- mean((mat_inData_sig - inData_avg)^2)
    mse_neg <- mean((-mat_inData_sig - inData_avg)^2)
    if(mse_neg < mse){
      mat_eigvecs <- -mat_eigvecs
      mat_inData_sig <- -mat_inData_sig
    }
  }else{
    for(i in 1:n_signals){
      mse <- mean((mat_inData_sig[, i] - inData_avg)^2)
      mse_neg <- mean((-mat_inData_sig[, i] - inData_avg)^2)
      if(mse_neg < mse){
        mat_eigvecs_sig[, i] <- -mat_eigvecs_sig[, i]
        mat_inData_sig[, i] <- -mat_inData_sig[, i]
      }
    }
    
  }
  #---------------------------------------------------------
  # Other plots:
  # -Dimensionally reduced plot of data (signal plots)
  # -Cluster plots (PCA)
  #---------------------------------------------------------
  # These plots include grouping info, if available
  varNames_ordered <- row.names(mat_loads_sig)
  if(!is.null(group_info)){
    list_groups <- group_info[[1]]
    group_names <- group_info[[2]]
    group_vec <- rep(NA, n_items)
    for(i in 1:length(list_groups)){
      this_group_vec <- list_groups[[i]]
      this_group_name <- group_names[i]
      group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
      
    }
  }
  #---------------------------------------------------------
  # Plot signal data against average
  if(signals_plot){ 
    date_vec <- row.names(mat_pctDiff)
    df_plot1 <- data.frame(Date = date_vec, inData_avg)
    df_plot2 <- data.frame(Date = date_vec, mat_inData_sig)
    df_plot1$Date <- factor(format(df_plot1$Date, format = "%y-%m-%d%"), ordered = T)
    df_plot2$Date <- factor(format(df_plot2$Date, format = "%y-%m-%d%"), ordered = T)
    xAxis_labels <- df_plot1$Date[seq(1, nrow(df_plot1), length.out = 10)]
    signal_id <- paste("Signal", c(1:n_signals))
    colnames(df_plot2)[2:(n_signals + 1)] <- signal_id
    gathercols <- signal_id
    df_plot2 <- df_plot2 %>% gather_("Signal", "Value", gathercols)
    gg <- ggplot()
    gg <- gg + geom_line(data = df_plot1, aes(x = Date, y = inData_avg, group = 1), color = "orange", lwd = 2)
    gg <- gg + geom_line(data = df_plot2, aes(x = Date, y = Value, group = 1))
    gg <- gg + scale_x_discrete(breaks = xAxis_labels)
    gg <- gg + facet_wrap(~ Signal, ncol = 1)
    gg <- gg + theme(axis.title.y = element_blank(),
                     axis.text.x = element_text(angle = 60, hjust = 1))
    gg_signals <- gg
    if(!eigenvalue_density_plot){
      print(gg_signals)
    }else{
      # gg_together <- ggpubr::ggarrange(gg_eigenval_density, gg_signals,
      #                                  #align = "v",
      #                                  #labels = c("A", "B"),
      #                                  ncol = 2, nrow = 1)
      # print(gg_together)
      gg_together <- gg_eigenval_density + gg_signals + plot_layout(nrow = 1)
      print(gg_together)
      
    }
  }
  #---------------------------------------------------------
  # PCA cluster plots to examine natural grouping in the data
  #---------------------------------------------------------
  # By variable
  if(pca_var_plot){
    if(n_signals == 1){
      print("Only 1 signal, can't do PCA plots.")
    }else{
      gg <- factoextra::fviz_pca_var(res, habillage = factor(group_vec))
      print(gg)
    }
    
  }
  #---------------------------------------------------------
  # By individual
  if(pca_ind_plot){
    if(n_signals == 1){
      print("Only 1 signal, can't do PCA plots.")
    }else{
      res <- FactoMineR::PCA(t(mat_pctDiff), graph = F)
      gg <- factoextra::fviz_pctDiffd(res, habillage = factor(group_vec), addEllipses = T)
      print(gg)
    }
    
  }
  #---------------------------------------------------------
  # Cluster plot using Mclust()
  # mc <- mclust::Mclust(t(mat_pctDiff))
  # summary(mc)
  # View(mc$classification)
  # factoextra::fviz_cluster(mc, frame.type = "norm", geom = "text")
  #---------------------------------------------------------
  list_out <- list(mat_loads_sig, mat_loads_rot_sig, mat_loads, mat_loads_rot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
  return(list_out)
  
}
#=======================================================================
# plot_signals <- function(mat_pctDiff, mat_eigvecs_sig, eigvecs_sig){
#   #---------------------------------------------------------
#   inData_avg <- rowMeans(mat_pctDiff)
#   n_signals <- length(eigvecs_sig)
#   mat_inData_sig <- mat_pctDiff %*% mat_eigvecs_sig
#   if(n_signals == 1){
#     mat_inData_sig <- mat_inData_sig / eigvals_sig
#   }else{
#     mat_inData_sig <- mat_inData_sig %*% diag(1 / eigvals_sig)
#   }
#   #---------------------------------------------------------
#   # Plot signal data against average
#   date_vec <- row.names(mat_pctDiff)
#   df_plot1 <- data.frame(Date = date_vec, inData_avg)
#   df_plot2 <- data.frame(Date = date_vec, mat_inData_sig)
#   df_plot1$Date <- factor(format(df_plot1$Date, format = "%y-%m-%d%"), ordered = T)
#   df_plot2$Date <- factor(format(df_plot2$Date, format = "%y-%m-%d%"), ordered = T)
#   xAxis_labels <- df_plot1$Date[seq(1, nrow(df_plot1), length.out = 10)]
#   signal_id <- paste("Signal", c(1:n_signals))
#   colnames(df_plot2)[2:(n_signals + 1)] <- signal_id
#   gathercols <- signal_id
#   df_plot2 <- df_plot2 %>% gather_("Signal", "Value", gathercols)
#   gg <- ggplot()
#   gg <- gg + geom_line(data = df_plot1, aes(x = Date, y = inData_avg, group = 1), color = "orange", lwd = 2)
#   gg <- gg + geom_line(data = df_plot2, aes(x = Date, y = Value, group = 1))
#   gg <- gg + scale_x_discrete(breaks = xAxis_labels)
#   gg <- gg + facet_wrap(~ Signal, ncol = 1)
#   gg <- gg + theme(axis.title.y = element_blank(),
#                    axis.text.x = element_text(angle = 60))
#   gg
#   print(gg)
#   
# }
#=======================================================================
interpret_loadings <- function(mat_loads_rot_sig, fun_env = NULL){
  #---------------------------------------------------------
  if(is.null(fun_env)){
    group_info = NULL
    signal_names = NULL
  }else{
    group_info = fun_env[[1]]
    signal_names =fun_env[[2]]
  }
  #---------------------------------------------------------
  if(class(mat_loads_rot_sig) == "numeric"){
    n_items <- length(mat_loads_rot_sig)
    n_signals <- 1
    varNames_ordered <- names(mat_loads_rot_sig)
  }
  if(class(mat_loads_rot_sig) == "matrix"){
    n_items <- nrow(mat_loads_rot_sig)
    n_signals <- ncol(mat_loads_rot_sig)
    varNames_ordered <- row.names(mat_loads_rot_sig)
  }
  #------------------------------------------------------------
  # Plot loadings barcharts
  df_plot <- data.frame(id = varNames_ordered, mat_loads_rot_sig)
  #--------------
  # Name the signals, if names provided
  if(is.null(signal_names)){
    signal_id <- paste("Signal", c(1:n_signals))
  }else{
    signal_id <- signal_names
  }
  #--------------
  colnames(df_plot)[2:(n_signals + 1)] <- signal_id
  gathercols <- as.character(signal_id) 
  df_plot <- gather_(df_plot, "Signal", "Loading", gathercols)
  df_plot <- transform(df_plot,
                       Signal = factor(Signal, levels = gathercols))
  #--------------
  # Group the vars if group info is provided
  if(!is.null(group_info)){
    list_groups <- group_info[[1]]
    group_names <- group_info[[2]]
    group_vec <- rep(NA, n_items)
    for(i in 1:length(list_groups)){
      this_group_vec <- list_groups[[i]]
      this_group_name <- group_names[i]
      group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
    }
    #--------------
    df_plot$Type <- factor(group_vec)
    xx <- df_plot$Type
    df_plot$id <- factor(df_plot$id, levels = unique(df_plot$id[order(xx)]))
    gg <- ggplot(df_plot, aes(x = id, y = Loading, fill = Type))
  }else{
    gg <- ggplot(df_plot, aes(x = id, y = Loading))
  }
  gg <- gg + geom_bar(stat = "identity", position = "dodge")
  gg <- gg + facet_wrap(~ Signal, nrow = 1)
  gg <- gg + theme(axis.text.y = element_text(face = "bold", size = 10),
                   axis.text.x = element_text(face = "bold", size = 10),
                   axis.title.y = element_blank(),
                   axis.title.x = element_text(face = "bold", size = 10))
  gg <- gg + coord_equal()
  gg <- gg + coord_flip()
  print(gg)
  
}
#=======================================================================

historical_returns_and_corr_plot <- function(mat_pctDiff,
                                             mat_pctDiff_test = NULL,
                                             group_info = NULL,
                                             returns_plot = F,
                                             corr_plot = F,
                                             group_colors = NULL,
                                             fig_title_returns = NULL,
                                             fig_title_corrplot = NULL,
                                             fig_title_corrplot_test = NULL,
                                             returns_plot_range = NULL,
                                             corrplot_options = list(
                                               plot_with_pvals = F,
                                               plot_with_corrCoefs = F,
                                               corr_coef_size = 0.75)
){
  #------------------------------------------------------------
  if(is.null(fig_title_returns)){fig_title_returns <- "Historical Returns"}
  #------------------------------------------------------------
  # Group the vars if group info is provided
  if(!is.null(group_info)){
    if(is.null(group_colors)){
      group_colors <- randomcoloR::distinctColorPalette(k = length(group_names))
      #group_colors <- viridis::viridis_pal(option = "D")(length(group_names))
    }
    n_groups <- length(list_groups)
    n_items <- ncol(mat_pctDiff)
    varNames_ordered <- colnames(mat_pctDiff)
    list_groups <- group_info[[1]]
    group_names <- group_info[[2]]
    group_vec <- rep(NA, n_items)
    group_color_vec <- rep(NA, n_items)
    for(i in 1:n_groups){
      this_group_vec <- list_groups[[i]]
      this_group_name <- group_names[i]
      this_group_color <- group_colors[i]
      group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
      group_color_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_color
    }
    xx <- factor(group_vec)
    cols_ordered_by_group <- as.character(colnames(mat_pctDiff)[order(xx)])
    group_color_vec <- group_color_vec[order(xx)]
  }
  
  
  for(i in 1:length(list_groups)){
    this_group_vec <- list_groups[[i]]
    this_group_color <- group_colors[i]
    label_colors[which(cols_ordered_by_group %in% this_group_vec)] <- this_group_color
  }
  
  
  #------------------------------------------------------------
  from_date <- row.names(mat_pctDiff)[1]
  to_date <- row.names(mat_pctDiff)[nrow(mat_pctDiff)]
  from_date <- gsub("-", "/", from_date)
  to_date <- gsub("-", "/", to_date)
  date_interval <- paste(from_date, to_date, sep = " - ")
  fig_subtitle_returns <- date_interval
  nab_pctRet <- apply(mat_pctDiff, 2, function(x) prod(1 + x)) - 1
  #------------------------------------------------------------
  df_plot <- data.frame(Returns = nab_pctRet)
  df_plot$id <- row.names(df_plot)
  if(!is.null(group_info)){
    df_plot$Type <- factor(group_vec)
    xx <- df_plot$Type
    df_plot$id <- factor(df_plot$id, levels = cols_ordered_by_group)
  }
  #------------------------------------------------------------
  if(!is.null(mat_pctDiff_test)){
    #------
    data_type_train <- paste("Train data ", date_interval)
    df_plot$Data <- data_type_train
    #------
    from_date <- row.names(mat_pctDiff_test)[1]
    to_date <- row.names(mat_pctDiff_test)[nrow(mat_pctDiff_test)]
    from_date <- gsub("-", "/", from_date)
    to_date <- gsub("-", "/", to_date)
    date_interval <- paste(from_date, to_date, sep = " - ")
    data_type_test <- paste("Backtest data ", date_interval)
    nab_pctRet_test <- apply(mat_pctDiff_test, 2, function(x) prod(1 + x)) - 1
    #------------------------------------------------------------
    df_plot_test <- data.frame(Returns = nab_pctRet_test, Data = data_type_test)
    df_plot_test$id <- row.names(df_plot_test)
    if(!is.null(group_info)){
      df_plot_test$Type <- factor(group_vec)
      xx <- df_plot_test$Type
      df_plot_test$id <- factor(df_plot_test$id, levels = cols_ordered_by_group)
    }
    df_plot <- as.data.frame(rbind(df_plot, df_plot_test))
    
    df_plot$Data <- factor(df_plot$Data, levels = c(data_type_train, data_type_test))
    
  }
  
  #------------------------------------------------------------
  # Historical returns plot
  if(returns_plot){
    
    if(!is.null(group_info)){
      gg <- ggplot(df_plot, aes(x = id, y = Returns, fill = Type))
      #gg <- gg + scale_color_brewer(palette = "Dark2")
      gg <- gg + scale_fill_manual(values = group_colors)
    }else{
      gg <- ggplot(df_plot, aes(x = id, y = Returns))
    }
    gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
    if(!is.null(mat_pctDiff_test)){
      gg <- gg + facet_wrap(~Data, ncol = 1)
      gg <- gg + labs(title = fig_title_returns)
    }else{
      gg <- gg + labs(title = fig_title_returns, subtitle = fig_subtitle_returns)
    }
    gg <- gg + theme(axis.text.x = element_text(face = "bold", size = 10, angle = 60, hjust = 1),
                     axis.text.y = element_text(face = "bold", size = 10),
                     axis.title.x = element_blank(),
                     axis.title.y = element_text(face = "bold", size = 10),
                     plot.title = element_text(size = 11))
    if(!is.null(returns_plot_range)){
      gg <- gg + coord_cartesian(ylim = returns_plot_range)
    }
    #  gg <- gg + coord_equal()
    #  gg <- gg + coord_flip()
    print(gg)
    
  }
  #------------------------------------------------------------
  # Correlation matrix plot
  if(corr_plot){
    if(is.null(fig_title_corrplot)){fig_title_corrplot <- "Correlation matrix"}
    plot_with_pvals <- corrplot_options[["plot_with_pvals"]]
    plot_with_corrCoefs <- corrplot_options[["plot_with_corrCoefs"]]
    corr_coef_size <- corrplot_options[["corr_coef_size"]]
    if(!is.null(group_info)){
      mat_pctDiff_corrplot <- mat_pctDiff[, cols_ordered_by_group]
      if(!is.null(mat_pctDiff_test)){
        mat_pctDiff_corrplot_test <- mat_pctDiff_test[, cols_ordered_by_group]
      }else{
        mat_pctDiff_corrplot_test <- NULL
      }
      
    }else{
      mat_pctDiff_corrplot <- mat_pctDiff
      if(!is.null(mat_pctDiff_test)){
        mat_pctDiff_corrplot_test <- mat_pctDiff_test
      }else{
        mat_pctDiff_corrplot_test <- NULL
      }
      
      label_colors <- "black"
    }
    #-------------------
    corr_colorRamp <- colorRampPalette(c("orange", "white", "deepskyblue"))(50)
    #-------------------
    xx <- Hmisc::rcorr(mat_pctDiff_corrplot)
    cormat <- xx$r
    if(plot_with_pvals){
      pvals <- xx$P
    }else{
      pvals <- NULL
    }
    
    if(plot_with_corrCoefs){
      corrplot::corrplot.mixed(cormat, tl.pos = "lt", tl.col = group_color_vec,
                               tl.srt = 45, p.mat = pvals, number.cex = corr_coef_size,
                               title = fig_title_corrplot,
                               col = corr_colorRamp)
    }else{
      corrplot::corrplot(cormat, type = "lower", tl.col = group_color_vec, tl.srt = 45,
                         p.mat = pvals, title = fig_title_corrplot,
                         col = corr_colorRamp)
    }
    #-------------------
    if(!is.null(mat_pctDiff_corrplot_test)){
      if(is.null(fig_title_corrplot_test)){
        fig_title_corrplot_test <- "Correlation matrix, test data"
      }
      xx <- Hmisc::rcorr(mat_pctDiff_corrplot_test)
      cormat <- xx$r
      if(plot_with_pvals){
        pvals <- xx$P
      }else{
        pvals <- NULL
      }
      if(plot_with_corrCoefs){
        corrplot::corrplot.mixed(cormat, tl.pos = "lt", tl.col = group_color_vec,
                                 tl.srt = 45, p.mat = pvals, number.cex = corr_coef_size,
                                 title = fig_title_corrplot_test,
                                 col = corr_colorRamp)
      }else{
        corrplot::corrplot(cormat, type = "lower", tl.col = group_color_vec, tl.srt = 45,
                           p.mat = pvals, title = fig_title_corrplot_test,
                           col = corr_colorRamp)
      }
      
    }
    
  }
  #------------------------------------------------------------
  
}
#=======================================================================
# End function definition
#=======================================================================
#=======================================================================

spy_sector_symbs <- c("XLF", "XLC", "XLY", "XLP", "XLV", "XLK", "RWR",
                      "XLU", "XLI", "XBI", "IYT") #"TTEK"
spy_sector_detail <- c("Financials", "Communications", "Luxury goods", "Staple goods",
                       "Healthcare", "Technology", "Real estate", "Utilities", "Industrial",
                       "Biotechnology", "Transportation") #"Gov. foreign aid"
minerals_symbs <- c("GLD", "SLV", "PPLT", "JJC", "JJM") #"XME"
minerals_detail <- c("Gold", "Silver", "Platinum", "Copper", "Industrial metals") #"US metals and mining"
agriculture_symbs <- c("JJG", "BAL", "SOYB", "SGG", "JO", "NIB", "DBA")
agriculture_detail <- c("Grains", "Cotton", "Soybean", "Sugar", "Coffee", "Cacao", "General agriculture")
energy_symbs <- c("WTI", "FUE", "WOOD", "ICLN", "KOL", "UNG")
energy_detail <- c("Oil (W&T Offshore Inc.)", "Biofuels", "Timber", "Clean energy", "Coal", "US natural gas")
#currency_symbs <- c("EMLC", "UUP", "FXE", "FXY", "FXF", "FXC", "FXB", "FXA")
#currency_detail <- c("Emerging mkt currencies", "USD", "EUR", "JPY", "CHF", "CND", "GBP", "AUD")
currency_symbs <- c("EURUSD=X", "JPY=X", "CHF=X", "CAD=X",
                    "GBPUSD=X", "AUDUSD=X", "INR=X")
currency_detail <- c("EUR/USD", "USD/JPY",
                     "USD/CHF", "USD/CAD", "GBP/USD", "AUD/USD", "USD/INR")
emerg_mkt_symbs <- c("EMLC", "ELD", "BKF", "VWOB")
emerg_mkt_detail <- c("Emerg mkts debt", "Emerg mkts gov. bonds", "BRIC countries", "Emerging mkt currencies")
crypto_symbs <- c("BLOK", "LEGR", "BCNA")
crypto_detail <- c("Blockchain tech.", "Blockchain companies", "Blockchain in China")
Tbond_symbs <- c("IEI", "IEF", "TLT")#, "BIL"
Tbond_detail <- c("T-bond 3-7 yrs", "T-bond 7-10 yrs", "T-bond 20+ yrs") #"T-bond 1-3 months"

ts_symb_vec <- c(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
                 currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
ts_detail_vec <- c(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                   currency_detail, emerg_mkt_detail, crypto_detail, Tbond_detail)
df_symb_detail <- data.frame(symbol = ts_symb_vec, detail = ts_detail_vec)
#--
length(ts_symb_vec)
#--
# list_groups <- list(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
#                      currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
#-------------------------------------------
# Create grouping info for plots (required input into functions)
list_groups <- list(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                    currency_detail, emerg_mkt_detail, crypto_detail, Tbond_detail)
group_names <- c("US Sectors", "Minerals", "Agriculture", "Energy", "Major Currency Pairs",
                 "Emerging Markets", "Cryptocurrencies/Blockchain", "T-Bonds")
group_info <- list(list_groups, group_names)
n_groups <- length(group_names)
#-------------------------------------------
#per_ema <- 13
#fromdate = Sys.Date() - 360
fromdate = "2018-06-20"
todate = "2019-08-09"
tbl_ohlcv <- tq_get(ts_symb_vec, get = "stock.prices", from = fromdate, to = todate)
df_ohlcv <- as.data.frame(tbl_ohlcv)
#--
# Crypto ts can have duplicate date entries sometimes (BTC-USD)
# Get rid of these duplicates, if any
df_ohlcv <- as.data.frame(df_ohlcv %>% group_by(symbol) %>% mutate(dup = duplicated(date)))
df_ohlcv <- subset(df_ohlcv, dup == F)
df_ohlcv$dup <- NULL
#--

#df_ohlcv$p <- rowSums(df_ohlcv[, c(4:6)]) / 3
#--
#df <- df_ohlcv[, c("date", "symbol", "p")]
# df <- df_ohlcv %>% group_by(symbol) %>% tq_transmute(select = adjusted, 
#                      mutate_fun = periodReturn, 
#                      period     = "monthly")
#--
df <- df_ohlcv[, c("date", "symbol", "adjusted")]
df <- df %>% spread(symbol, adjusted)
ind_shift <- which(colnames(df) %in% currency_symbs)
df[, ind_shift] <- rbind(rep(NA, length(ind_shift)), df[-nrow(df), ind_shift])
df <- df[-c(1, nrow(df)), ]
date_vec <- df$date
mat_ts_dy <- na.approx(df[, -1])

#o <- apply(mat_ts_dy, 2, function(x) length(which(is.na(x))))
#table(o)
#which(o==1)
#xts_ts_dy <- xts(mat_ts_dy, date_vec)
#date_vec <- index(xts_ts_dy)
#-----
mat_pctDiff_dy <- diff(mat_ts_dy) / mat_ts_dy[-nrow(mat_ts_dy), ]
#mat_pctDiff_dy <- diff(log(mat_ts_dy))
row.names(mat_pctDiff_dy) <- as.character(date_vec[-1])
ts_avg_dy <- rowMeans(mat_pctDiff_dy)
mu_ret_dy <- colMeans(mat_pctDiff_dy)
sd_ret_dy <- apply(mat_pctDiff_dy, 2, sd)

mat_ts_wk <- to.weekly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_wk <- diff(mat_ts_wk) / mat_ts_wk[-nrow(mat_ts_wk), ]
mat_pctDiff_wk <- as.matrix(mat_pctDiff_wk[-1, ])
ts_avg_wk <- rowMeans(mat_pctDiff_wk)
mu_ret_wk <- colMeans(mat_pctDiff_wk)
sd_ret_wk <- apply(mat_pctDiff_wk, 2, sd)

mat_ts_mo <- to.monthly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_mo <- diff(mat_ts_mo) / mat_ts_mo[-nrow(mat_ts_mo), ]
mat_pctDiff_mo <- as.matrix(mat_pctDiff_mo[-1, ])
ts_avg_mo <- rowMeans(mat_pctDiff_mo)
mu_ret_mo <- colMeans(mat_pctDiff_mo)
sd_ret_mo <- apply(mat_pctDiff_mo, 2, sd)
#----------------------------------------------
# Compare histograms of daily, weekly, and monthly returns
# df_plot <- data.frame(ts_id = names(mu_ret_dy), mu_ret_dy, mu_ret_wk, mu_ret_mo)
# colnames(df_plot)[-1] <- c("Mean Daily Return", "Mean Weekly Return", "Mean Monthly Return")
# gathercols <- colnames(df_plot)[-1]
# df_plot <- df_plot %>% gather_("Return Type", "Value", gathercols)
# #gg <- ggplot(df_plot, aes(Value, fill = `Return Type`))
# gg <- ggplot(df_plot, aes(Value))
# gg <- gg + geom_density(alpha = 0.4)
# gg <- gg + facet_wrap(~`Return Type`, nrow = 1, scales = "free")
# #gg <- gg + coord_cartesian(xlim = c(-0.02, 0.02))
# gg
#----------------------------------------------
#ind_rm_ema <- 1:(per_ema - 1)
# mat_pctDiff <- apply(mat_pctDiff, 2, function(x) x - EMA(x, per_ema))
# mat_pctDiff <- mat_pctDiff[-ind_rm_ema, ]
# date_vec <- df$date[-c(ind_rm_ema, ind_rm_na)]
#----------------------------------------------
mat_ts_in <- mat_ts_dy
ts_avg_in <- ts_avg_dy
# nab_pctRet_in <- mu_ret_dy
# sd_ret_in <- sd_ret_dy
#----------------------------------------------
mat_pctDiff <- mat_pctDiff_dy
date_vec <- row.names(mat_pctDiff)
#length(date_vec)
row.names(mat_pctDiff) <- as.character(date_vec)
colnames(mat_pctDiff) <- as.character(df_symb_detail$detail[order(df_symb_detail$symbol)])
#----------------------------------------------
check_on_data <- F
if(check_on_data){
  df_plot <- as.data.frame(mat_pctDiff)
  gathercols <- colnames(df_plot)
  df_plot$Date <- date_vec
  df_plot <- df_plot %>% gather_("Security", "Weekly change (%)", gathercols)
  df_plot$Type <- NA
  df_plot$Type[which(df_plot$Security %in% spy_sector_detail)] <- "US Sectors"
  df_plot$Type[which(df_plot$Security %in% minerals_detail)] <- "Minerals"
  df_plot$Type[which(df_plot$Security %in% agriculture_detail)] <- "Agriculture"
  df_plot$Type[which(df_plot$Security %in% energy_detail)] <- "Energy"
  df_plot$Type[which(df_plot$Security %in% currency_detail)] <- "Major Currency Pairs"
  df_plot$Type[which(df_plot$Security %in% emerg_mkt_detail)] <- "Emerging Markets"
  df_plot$Type[which(df_plot$Security %in% crypto_detail)] <- "Cryptocurrencies/Blockchain"
  df_plot$Type[which(df_plot$Security %in% Tbond_detail)] <- "T-bonds"
  #------------------------------------------------------------
  # gg <- ggplot(df_plot, aes(x = Year, y = Value, group = Security, color = Security))
  # gg <- gg + geom_line()
  # gg <- gg + facet_wrap(~ Type, nrow = 3, scales = "free")
  # gg <- gg + theme(legend.position = "none")
  # gg
  #------------------------------------------------------------
  this_type <- "US Sectors"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Minerals"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Agriculture"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Energy"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Major Currency Pairs"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Emerging Markets"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Cryptocurrencies/Blockchain"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "T-bonds"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
}

```


## 4. Application to real world data

```{r, fig.width=11, fig.height=6, fig.align='center', echo=FALSE}
ind_train <- 1:round(nrow(mat_pctDiff) * 2 / 3)
ind_test <- setdiff(1:nrow(mat_pctDiff), ind_train)
mat_pctDiff_train <- mat_pctDiff[ind_train, ]
mat_pctDiff_test <- mat_pctDiff[ind_test, ]
# ts_avg_test <- ts_avg_in[ind_test]
# date_vec_test <- date_vec[ind_test]
#group_colors <- RColorBrewer::brewer.pal(n = n_groups, "Dark2")
# "Darjeeling"
group_colors <- wesanderson::wes_palette("Darjeeling1", n = n_groups, type = "continuous")
this_fig_title <- "Figure 1: Historical returns"
historical_returns_and_corr_plot(mat_pctDiff_train, mat_pctDiff_test, group_info,
                                 returns_plot = T,
                                 corr_plot = F,
                                 group_colors = group_colors,
                                 fig_title_returns = this_fig_title)


```

```{r, fig.width=12, fig.height=12, fig.align='center', echo=FALSE}

# (mat_pctDiff,
#                                              mat_pctDiff_test = NULL,
#                                              group_info = NULL,
#                                              returns_plot = F,
#                                              corr_plot = F,
#                                              fig_title_returns = NULL,
#                                              fig_title_corrplot = NULL,
#                                              fig_title_corrplot_test = NULL,
#                                              returns_plot_range = NULL,
#                                              corrplot_options = list(
#                                                plot_with_pvals = F,
#                                                plot_with_corrCoefs = F,
#                                                corr_coef_size = 0.75)
# )

corrplot_options = list(plot_with_pvals = F, plot_with_corrCoefs = F, corr_coef_size = 0.75)
fig_title_corrplot <- "Figure 2: Correlation matrix, train data"
fig_title_corrplot_test <- "Figure 3: Correlation matrix, test data"
historical_returns_and_corr_plot(mat_pctDiff_train, mat_pctDiff_test, group_info,
                                 returns_plot = F, 
                                 corr_plot = T, 
                                 group_colors = group_colors, 
                                 fig_title_corrplot = fig_title_corrplot,
                                 fig_title_corrplot_test = fig_title_corrplot_test,
                                 corrplot_options = corrplot_options)



```
### 4.1 The financial context

#### 4.1.1 Extraction of signals from noise

In figure ..., a density plot of the correlation matrix eigenvalues is compared against one of eigenvalues derived from a random matrix. In this plot, it is evident that most eigenvalues are small and cannot be distinguished from noise, but a few extend beyond the random matrix eigenvalue density plot. These correspond to the eigenvectors that can be meaningfully distinguished from noise.

The signal time series are plotted against the data average in figure .... Here it is evident that the first signal reflects the average, while the subsequent signals reflect forces pushing and pulling on the average.

```{r, fig.width=12, fig.height=8, fig.align='center', echo=FALSE}
fun_env = list(eigenvalue_density_plot = T,
               signals_plot = T,
               pca_var_plot = F,
               pca_ind_plot = F,
               group_info,
               quietly = F)
list_out <- signals_from_noise(mat_pctDiff_train, fun_env)
# list_out <- list(mat_loads_sig, mat_loads_rot_sig, mat_loads, mat_loads_rot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
mat_loads_rot_sig_train <- list_out[[2]]
```


#### 4.1.2 Interpretation and characterization of the signals

Average, mostly US sectors and.... Signal two 

```{r, fig.width=15, fig.height=12, fig.align='center', echo=FALSE}

signal_names <- c("US / Emerg. Markets,\nEnergy, Blockchain", "Precious metals", "FOREX", "US Bonds", "Agriculture", "Industrial metals")
fun_env <- list(group_info, signal_names)
interpret_loadings(mat_loads_rot_sig_train, fun_env)

```


#### 4.1.3 Portfolio optimization

The train data is 2/3 the whole data. The expected returns are calculated over a period equal in length to that of the test data.


```{r, fig.width=15, fig.height=12, fig.align='center', echo=FALSE}

# Get backtest data
fun_env <- list(eigenvalue_density_plot = T,
                signals_plot = T,
                pca_var_plot = F,
                pca_ind_plot = F,
                group_info,
                quietly = F)
list_out <- signals_from_noise(mat_pctDiff_test, fun_env)
mat_loads_rot_sig_test <- list_out[[2]]
# signal_names <- NULL
# fun_env <- list(group_info, signal_names)
# interpret_loadings(mat_loads_rot_sig_test, fun_env)


```



```{r, fig.width=8, fig.height=6, fig.align='center', echo = FALSE}
#=======================================================================
# Define portfolio optimization function
optimize_portfolio <- function(cormat, mat_nab, targ_vec,
                               utility_interpretation = F){
  cormat_inv <- solve(cormat)
  M <- t(mat_nab) %*% cormat_inv %*% mat_nab
  M_inv <- solve(M)
  x <- -2 * M_inv %*% targ_vec
  # Risk shadow price
  l_V <- 1 / x[1]
  #if(l_V > 0){l_V <- -l_V}
  #print(l_V)
  # Budget shadow price (l_C = lambdas[2], l_R normalized to = 1)
  lambdas <- l_V * x
  l_C <- lambdas[2]
  # Optimal budget weights
  wStar <- -1 / (2 * l_V) * cormat_inv %*% mat_nab %*% lambdas
  # sum(wStar)
  # Portfolio variance
  V <- t(wStar) %*% cormat %*% wStar
  # Rtarg <- targ_vec[1]
  Rtarg <- t(wStar) %*% mat_nab[, 1]
  #----------------------------------------------------
  # check <- t(lambdas) %*% targ_vec + 2*l_V * V
  # print(check)
  #----------------------------------------------------
  # Utility function interpretation of equations
  # (Makes all budget weights positive)
  if(utility_interpretation){
    Exp_wStar <- exp(wStar)
    K <- sum(Exp_wStar)
    wStar <- Exp_wStar / K
    Rtarg <- t(wStar) %*% mat_nab[, 1]
    V <- t(wStar) %*% cormat %*% wStar
  } 
  #if(l_V > 0){wStar <- rep(NA, length(wStar))}
  list_out <- list(wStar, Rtarg, V, l_V, l_C)
  return(list_out)
  
}
#=======================================================================
backtest_portfolio <- function(wStar, nab_pctRet_test, cormat_test,
                               include_benchmark = F){
  #----------------------------------------------------
  # Backtest
  R_test <- t(wStar) %*% nab_pctRet_test
  V_test <- t(wStar) %*% cormat_test %*% wStar
  if(include_benchmark){
    # Benchmark portfolio
    n_items <- ncol(cormat)
    wBmark <- rep(1 / n_items, n_items)
    R_test_bMark <- t(wBmark) %*% nab_pctRet_test
    V_test_bMark <- t(wBmark) %*% cormat_test %*% wBmark
  }else{
    R_test_bMark <- NULL
    V_test_bMark <- NULL
  }
  #----------------------------------------------------
  outvec <- c(R_test, V_test, R_test_bMark, V_test_bMark)
  return(outvec)
  
}
#=======================================================================
plot_frontier_wBacktest <- function(df_frontier,
                                    df_backtest,
                                    fig_title = NULL,
                                    separate_plots = F){
  #-------------------------------------------
  n_points_on_frontier <- nrow(df_frontier)
  color_vec <- c("#56B4E9", "black")
  #-------------------------------------------
  if(is.null(fig_title)){fig_title <- "Optimal Portfolio Frontier"}
  df_plot1 <- df_frontier[, c("Risk (variance)", "Return target")]
  df_plot2 <- df_backtest[, c("Risk backtest", "Return backtest")]
  df_plot1$Type <- "Optimal solution"
  df_plot2$Type <- "Backtest of solution"
  colnames(df_plot1)[2] <- "Return"
  colnames(df_plot2)[1:2] <- c("Risk (variance)", "Return")
  df_plot <- as.data.frame(do.call(rbind, list(df_plot1, df_plot2)))
  if(separate_plots){
    gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = `Return`))
  }else{
    gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = `Return`, group = Type, color = Type))
  }
  gg <- gg + geom_point()
  if(separate_plots){
    gg <- gg + facet_wrap(~Type, ncol = 1, scales = "free_y")
  }
  gg <- gg + scale_color_manual(values = color_vec)
  gg <- gg + labs(title = fig_title)
  gg <- gg + theme(plot.title = element_text(size = 10))
  gg <- gg + theme(legend.title = element_blank())
  gg <- gg + coord_cartesian(xlim = c(0, max(df_plot$`Risk (variance)`)))
  gg_frontier_wBacktest <- gg
  print(gg_frontier_wBacktest)
  
}
#=======================================================================
plot_frontier_and_budget <- function(df_frontier, df_wStar,
                                     n_points_on_frontier,
                                     varNames_ordered,
                                     fig_title = NULL,
                                     group_info = NULL){
  #-------------------------------------------
  # Frontier plot
  if(is.null(fig_title)){fig_title <- "Optimal Portfolio Frontier"}
  df_plot <- df_frontier[, c("Risk (variance)", "Return target")]
  gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = `Return target`))
  gg <- gg + labs(title = fig_title)
  gg <- gg + theme(axis.title.x = element_blank(),
                   axis.text.x = element_blank(),
                   plot.title = element_text(size = 10))
  gg <- gg + geom_point()
  gg_frontier <- gg
  #-------------------------------------------
  # Budget weights plot
  # Note df_w_in should have just the target risk column (i.e. don't also include the backtest risk column). At any rate, the budget risk column should be the same as the one used in the frontier plot.
  df_plot <- df_wStar
  gathercols <- colnames(df_plot)[-1]
  df_plot$portfolio_id <- 1:n_points_on_frontier
  df_match_V <- df_plot[, c("portfolio_id", "Risk (variance)")]
  df_plot <- df_plot %>% gather_("Item", "Budget weights", gathercols)
  if(!is.null(group_info)){
    list_groups <- group_info[[1]]
    group_names <- group_info[[2]]
    n_items <- nrow(mat_nab)
    group_vec <- rep(NA, n_items)
    for(i in 1:length(list_groups)){
      this_group_vec <- list_groups[[i]]
      this_group_name <- group_names[i]
      group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
    }
    df_match_group <- data.frame(Item = varNames_ordered, Type = group_vec)
    df_plot <- merge(df_plot, df_match_group, by = "Item")
    df_plot <- df_plot %>% group_by(portfolio_id, Type) %>% summarise(`Budget weights` = sum(`Budget weights`))
    df_plot <- merge(df_plot, df_match_V, by = "portfolio_id")
    colnames(df_plot)[2] <- "Item"
  }
  df_plot <- df_plot %>% group_by(Item) %>% mutate(mu = mean(`Budget weights`)) %>% as.data.frame(df_plot)
  df_plot$Item <- factor(df_plot$Item,
                         levels = unique(df_plot$Item[order(df_plot$mu, df_plot$Item, decreasing = T)]),
                         ordered = T)
  #df_plot <- arrange(df_plot, Item, `Risk (variance)`)
  gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = `Budget weights`, fill = Item))
  gg <- gg + geom_area(position = "stack")
  gg <- gg + theme(legend.title = element_blank())
  if(length(unique(df_plot$Item)) > 15){gg <- gg + theme(legend.position = "none")}
  gg_weights <- gg
  #-------------------------------------------
  gg_together <- gg_frontier + gg_weights + plot_layout(ncol = 1)
  print(gg_together)
  
}

#=======================================================================
get_optimal_frontier <- function(cormat, mat_nab,
                                 Rtarg_limits = c(0.001, 0.3),
                                 fig_title = NULL,
                                 fun_env = NULL
){
  #-------------------------------------------
  if(is.null(fun_env)){
    n_points_on_frontier = 50
    utility_interpretation = F
    backtest_info = NULL
    frontier_and_budget_plot = T
    group_info = NULL
  }else{
    n_points_on_frontier = fun_env[[1]]
    utility_interpretation = fun_env[[2]]
    backtest_info = fun_env[[3]]
    frontier_and_budget_plot = fun_env[[4]]
    group_info = fun_env[[5]]
    
  }
  #-------------------------------------------
  Rtarg_vec <- seq(Rtarg_limits[1], Rtarg_limits[2], length.out = n_points_on_frontier)
  list_wStar <- list()
  R_vec <- c()
  V_vec <- c()
  lV_vec <- c()
  lC_vec <- c()
  #-------------------------------------------
  for(i in 1:length(Rtarg_vec)){
    this_Rtarg <- Rtarg_vec[i]
    targ_vec <- c(this_Rtarg, C_targ)
    list_out <- optimize_portfolio(cormat, mat_nab, targ_vec,
                                   utility_interpretation)
    list_wStar[[i]] <- list_out[[1]]
    R_vec[i] <- list_out[[2]]
    V_vec[i] <- list_out[[3]]
    lV_vec[i] <- list_out[[4]]
    lC_vec[i] <- list_out[[5]]
  }
  #-------------------------------------------
  df_frontier <- data.frame(R_vec, V_vec, lV_vec, lC_vec)
  colnames(df_frontier) <- c("Return target",
                             "Risk (variance)",
                             "Risk shadow price",
                             "Budget shadow price")
  df_wStar <- data.frame(V_vec, t(do.call(cbind, list_wStar)))
  varNames_ordered <- row.names(mat_nab)
  colnames(df_wStar) <- c("Risk (variance)", varNames_ordered)
  #-------------------------------------------
  # Backtest
  if(!is.null(backtest_info)){
    nab_pctRet_test <- backtest_info[[1]]
    cormat_test <- backtest_info[[2]]
    list_outTest <- list()
    for(i in 1:n_points_on_frontier){
      wStar <- list_wStar[[i]]
      outvec <- backtest_portfolio(wStar, nab_pctRet_test, cormat_test,
                                   include_benchmark = F)
      list_outTest[[i]] <- outvec
      
    }
    df_backtest <- as.data.frame(do.call(rbind, list_outTest))
    colnames(df_backtest) <- c("Return backtest", "Risk backtest")
  }else{
    df_backtest <- NULL
  }
  #--------------------------------------
  if(frontier_and_budget_plot){
    plot_frontier_and_budget(df_frontier, df_wStar,
                             n_points_on_frontier = n_points_on_frontier,
                             varNames_ordered = varNames_ordered,
                             fig_title = fig_title,
                             group_info)
  }
  #--------------------------------------
  list_out <- list(df_wStar, df_frontier, df_backtest)
  return(list_out)
}
#=======================================================================
# 
# 
# prepare_optFrontier_inputs <- function(list_mat_pctDiff,
#                                        list_mat_loads_rot_sig,
#                                        signals_replace_items,
#                                        purge_cormat_noise){
#   
# }
#   #-------------------------------------------
#   mat_pctDiff <- list_mat_pctDiff[[1]]
# if(length(list_mat_pctDiff) > 1){
#   mat_pctDiff_test <- list_mat_pctDiff[[2]]
# }
# if(length(list_mat_loads_rot_sig) > 1){
#   mat_loads_rot_sig_test <- list_mat_loads_rot_sig[[2]]
# }
#   #-------------------------------------------
#   # Optimize over signals rather than portfolio items?
#   if(signals_replace_items){
#     if(class(mat_loads_rot_sig) == "numeric"){
#       # (If condition is true, then only one signal could be extracted.)
#       print("Only 1 signal. Too few to replace portfolio items with signals.")
#     }else{
#       if(is.null(fraction_train)){
#         mat_pctDiff <- list_out[[5]]
#         n_items <- ncol(mat_pctDiff)
#       }else{
#         mat_pctDiff <- list_out_train[[5]]
#         mat_pctDiff_test <- list_out_test[[5]]
#       }
#     }
#     
#   }
#   #-------------------------------------------
#   # Define cost target and gradient
#   # (By convention, the budget weights always add up to (a cost target of) 1, so that the cost gradient is equal to a vector of ones the length of portfolio items.)
#   C_targ <- 1
#   nab_C <- rep(1, n_items)
#   #-------------------------------------------
#   # Define expected returns
#   nab_mu_ret <- apply(mat_pctDiff, 2, function(x) prod(1 + x)) - 1
#   if(!is.null(fraction_train)){
#     nab_mu_ret_test <- apply(mat_pctDiff_test, 2, function(x) prod(1 + x)) - 1
#   }
#   #-------------------------------------------
#   # Correlation matrix
#   cormat <- cor(mat_pctDiff)
#   if(purge_cormat_noise){
#     if(signals_replace_items){
#       print("If signals_replace_items = T, then can't also have purge_cormat_noise = T. Using regular correlation matrix.")
#     }else{
#         cormat_data <- cormat
#         cormat <- round(mat_loads_rot_sig %*% t(mat_loads_rot_sig), 7)
#         #cormat <- round(mat_loads_sig %*% t(mat_loads_sig), 7)
#         mse <- round(mean((cormat - cormat_data)^2), 6)
#         print(paste("Purged correlation matrix mse:", mse))
#     if(!is.null(fraction_train)){
#         cormat_data_test <- cor(mat_pctDiff_test)
#         cormat_test <- round(mat_loads_rot_sig_test %*% t(mat_loads_rot_sig_test), 7)
#         #cormat <- round(mat_loads_sig %*% t(mat_loads_sig), 7)
#         mse <- round(mean((cormat_test - cormat_data_test)^2), 6)
#         print(paste("Purged test correlation matrix mse:", mse))
#         print(paste("Test correlation matrix kappa:", kappa(cormat_test)))
#       }
#     }
#   print(paste("Correlation matrix kappa:", kappa(cormat)))
#   #------------------------------------
#   # Expected returns vector
#   #ind_t_ret <- ind_ret_sinceYr:ind_ret_toYr
#   nab_pctDiff <- apply(mat_pctDiff, 2, function(x) prod(1 + x)) - 1
#   #------------------------------------
#   mat_nab <- cbind(nab_pctDiff, nab_C)
#   #------------------------------------
#   
#   
#   list_out <- get_optimal_frontier(cormat, mat_nab,
#                                    Rtarg_limits,
#                                    fig_title = fig_title_frontier_wBudget,
#                                    fun_env_get_optimal_frontier)
#   
#   
#=======================================================================
#=======================================================================
# Conventional risk-reward frontier
n_items <- ncol(mat_pctDiff)
C_targ <- 1
nab_C <- rep(1, n_items)
#------------------------------------
# Get pct return vec and correlation matrix only over a period equal in length to that of the test period.
ind_equal_test <- (nrow(mat_pctDiff_train) - length(ind_test)):nrow(mat_pctDiff_train)
#------------------------------------
# Correlation matrix
#cormat <- cor(mat_pctDiff_train[ind_equal_test, ])
cormat <- cor(mat_pctDiff_train)
cormat_test <- cor(mat_pctDiff_test)
#cormat <- round(mat_loads_rot_sig_train %*% t(mat_loads_rot_sig_train), 7)
#cormat <- round(mat_loads_sig_train %*% t(mat_loads_sig_train), 7)
# mse <- mean((cor(mat_pctDiff_train) - cormat)^2)
# mse
#------------------------------------
# Expected returns vector
#nab_pctRet_train <- apply(mat_pctDiff_train[ind_equal_test, ], 2, function(x) prod(1 + x)) - 1
nab_pctRet_train <- apply(mat_pctDiff_train, 2, function(x) prod(1 + x)) - 1
nab_pctRet_test <- apply(mat_pctDiff_test, 2, function(x) prod(1 + x)) - 1
#------------------------------------
mat_nab <- cbind(nab_pctRet_train, nab_C)
n_points_on_frontier <- 50
#------------------------------------
fun_env <- list(n_points_on_frontier,
                utility_interpretation = F,
                backtest_info = list(nab_pctRet_test, cormat_test),
                frontier_and_budget_plot = T,
                group_info)

list_out <- get_optimal_frontier(cormat, mat_nab,
                                 Rtarg_limits = c(0.01, 0.3),
                                 fig_title = "Figure 1: Optimal portfolio frontier",
                                 fun_env)

df_frontier <- list_out[[2]]
df_backtest <- list_out[[3]]
#------------------------------------


```

Minimum variance analysis is, in many ways, still a work in progress. A backtest of the optimal weights displayed in Figure ... indicates that the real return on investment in these optimal portfolios would have been much less, and the risk much higher, than the efficient fontier suggests.

```{r, fig.width=8, fig.height=4, fig.align='center', echo = FALSE}

#-------------------------------------------
# Frontier plot with backtest
plot_frontier_wBacktest(df_frontier,
                        df_backtest,
                        fig_title = "Figure 2: Optimal portfolio solution vs. backtest")
#-------------------------------------------


```

Recently, the notion of "eigenportfolios" has been explored..., in particular the leading eigenvector, which, as observed above, corresponds to the time series average. The eigenportfolios corresponding to the data are displayed in table ... They are all disastrous portfolios with respect to the efficient frontier.

```{r, echo=F}
#-----------------------------------------------------------------------
# # Get eigenportfolios
# mat_wEigen <- apply(mat_eigvecs_sig_train, 2, function(x) x / sum(x))
# # mat_loads_rot_sig_train_normd <- apply(mat_loads_rot_sig_train, 2, function(x) x / sum(x))
# # mat_wEigen <- apply(mat_loads_rot_sig_train_normd, 2, function(x) x / sum(x))
# V_eigen <- apply(mat_wEigen, 2, function(x) t(x) %*% cormat %*% x)
# R_eigen <- apply(mat_wEigen, 2, function(x) t(x) %*% nab_pctRet_train)
# V_eigen_test <- apply(mat_wEigen, 2, function(x) t(x) %*% cormat_test %*% x)
# R_eigen_test <- apply(mat_wEigen, 2, function(x) t(x) %*% nab_pctRet_test)
# 
# #mat_pctDiff_eigenportfolio <- mat_pctDiff_train %*% mat_wEigen
# #R_eigen <- apply(mat_pctDiff_eigenportfolio, 2, function(x) prod(1 + x)) - 1
# #CV_eigen <- abs(sqrt(V_eigen) / R_eigen)
# n_signals <- ncol(mat_pctDiff_sig_train)
# signal_names[1] <- "US / Emerg. Markets, Energy, Blockchain"
# df_eigenportfolio <- data.frame(Eigen_portfolio = as.character(c(1:n_signals)),
#                                 Characteristic = signal_names,
#                                 R_eigen, V_eigen,
#                                 R_eigen_test, V_eigen_test)
# colnames(df_eigenportfolio) <- c("Eigen-portfolio", "Description",
#                                  "Return target", "Risk target",
#                                  "Return backtest", "Risk backtest")
# df_eigenportfolio[, 3:6] <- round(df_eigenportfolio[, 3:6], 2)
# knitr::kable(df_eigenportfolio)
# #-----------------------------------------------------------------------



```


Analysis of train test dividing points...


```{r, fig.width=15, fig.height=12, fig.align='center', echo=FALSE}

#------------------------------------------------------------
fraction_train <- 2 / 3
total_length <- nrow(mat_pctDiff)
ind_total <- 1:total_length
ind_train <- 1:round(total_length * fraction_train)
ind_test <- setdiff(ind_total, ind_train)
mat_pctDiff_train <- mat_pctDiff[ind_train, ]
mat_pctDiff_test <- mat_pctDiff[ind_test, ]
#------------------------------------------------------------
fun_env = list(eigenvalue_density_plot = F,
               signals_plot = F,
               pca_var_plot = F,
               pctDiffd_plot = F,
               group_info = NULL,
               quietly = T)
#------------------------------------------------------------
# Get sig loads, train data
list_out <- signals_from_noise(mat_pctDiff_train, fun_env)
mat_loads_rot_sig_train <- list_out[[2]]
#mat_loads_train <- list_out[[3]]
#mat_loads_sig_train <- list_out[[1]]
mat_pctDiff_train <- list_out[[5]]
colnames(mat_pctDiff_train) <- paste("Signal", c(1:ncol(mat_pctDiff_train)))
mat_eigvecs_sig_train <- list_out[[7]]
#list_out = list(mat_loads_sig, mat_loads_rot_sig, mat_loads, mat_loads_rot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
signal_names <- c("US / Emerg. Markets,\nEnergy, Blockchain", "Precious metals", "FOREX", "US Bonds", "Agriculture", "Industrial metals")
#signal_names <- c("US / Emerg. Markets,\nEnergy, Blockchain", "Precious metals", "FOREX", "US Bonds", "Consumer Staples,\nUtilities, Real Estate")
fun_env <- list(group_info = list(list_groups, group_names), signal_names)
interpret_loadings(mat_loads_rot_sig_train, fun_env)
#------------------------------------------------------------
# Get sig loads, backtest data
# list_out <- signals_from_noise(mat_pctDiff_test, fun_env)
# mat_loads_rot_sig_test <- list_out[[2]]
# mat_loads_sig_test <- list_out[[1]]
# #mat_loads_test <- list_out[[3]]
mat_pctDiff_test <- mat_pctDiff_test %*% mat_eigvecs_sig_train
colnames(mat_pctDiff_test) <- signal_names
colnames(mat_pctDiff_train) <- signal_names
#colnames(mat_pctDiff_test) <- paste("Signal", c(1:ncol(mat_pctDiff_test)))
#------------------------------------------------------------
# Portfolio optimization
nab_pctRet_train <- apply(mat_pctDiff_train, 2, function(x) prod(1 + x)) - 1
nab_pctRet_test <- apply(mat_pctDiff_test, 2, function(x) prod(1 + x)) - 1
cormat_train <- cor(mat_pctDiff_train)
cormat_test <- cor(mat_pctDiff_test)
# cormat_train <- round(mat_loads_sig_train %*% t(mat_loads_sig_train), 7)
# cormat_test <- round(mat_loads_sig_test %*% t(mat_loads_sig_test), 7)
n_items <- ncol(mat_pctDiff_train)
C_targ <- 1
nab_C <- rep(1, n_items)
mat_nab_train <- cbind(nab_pctRet_train, nab_C)
#------------------------------------------------------------
fun_env <- list(n_points_on_frontier,
                utility_interpretation = F,
                backtest_info = list(nab_pctRet_test, cormat_test),
                frontier_and_budget_plot = T,
                group_info = NULL)
list_out <- get_optimal_frontier(cormat_train, mat_nab_train,
                                 Rtarg_limits = c(0.01, 0.15),
                                 fig_title = "Figure 1: Optimal portfolio frontier",
                                 fun_env)
df_frontier <- list_out[[2]]
df_backtest <- list_out[[3]]
diff_R_sq <- (df_backtest$`Return backtest` - df_frontier$`Return target`)^2
diff_V_sq <- (df_backtest$`Risk backtest` - df_frontier$`Risk (variance)`)^2
mse_R <- mean(diff_R_sq)
mse_V <- mean(diff_V_sq)
meanEucDist <- mean(sqrt(diff_R_sq + diff_V_sq))



plot_frontier_wBacktest(df_frontier,
                        df_backtest,
                        fig_title = "Figure 2: Optimal portfolio solution vs. backtest")


# optimal_divide_objFn <- function(mat_pctDiff, fraction_train = 2 / 3, fraction_N = 1){
# #------------------------------------------------------------
# #total_length <- nrow(mat_pctDiff)
# #rm_length <- round((1 - fraction_N) * total_length)
# #ind_rm <- 1:rm_length
# #mat_pctDiff <- mat_pctDiff[-ind_rm, ]
# total_length <- nrow(mat_pctDiff)
# ind_total <- 1:total_length
# ind_train <- 1:round(total_length * fraction_train)
# ind_test <- setdiff(ind_total, ind_train)
# mat_pctDiff_train <- mat_pctDiff[ind_train, ]
# mat_pctDiff_test <- mat_pctDiff[ind_test, ]
# #------------------------------------------------------------
# fun_env = list(eigenvalue_density_plot = F,
#                signals_plot = F,
#                pca_var_plot = F,
#                pctDiffd_plot = F,
#                group_info = NULL,
#                quietly = T)
# #------------------------------------------------------------
# # Get sig loads, train data
# list_out <- signals_from_noise(mat_pctDiff_train, fun_env)
# mat_loads_rot_sig_train <- list_out[[2]]
# #mat_loads_train <- list_out[[3]]
# mat_loads_sig_train <- list_out[[1]]
# mat_pctDiff_train <- list_out[[5]]
# mat_eigvecs_sig <- list_out[[7]]
# #list_out = list(mat_loads_sig, mat_loads_rot_sig, mat_loads, mat_loads_rot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
# #------------------------------------------------------------
# # Get sig loads, backtest data
# list_out <- signals_from_noise(mat_pctDiff_test, fun_env)
# mat_loads_rot_sig_test <- list_out[[2]]
# # mat_loads_sig_test <- list_out[[1]]
# # #mat_loads_test <- list_out[[3]]
# mat_pctDiff_test <- mat_pctDiff_test %*% mat_eigvecs_sig
# #------------------------------------------------------------
# # mse test-train first signal loadings
# n_compare <- min(ncol(mat_loads_rot_sig_test), ncol(mat_loads_rot_sig_train))
# mse_vec <- c()
# for(i in 1:n_compare){
#   mse_vec[i] <- mean((mat_loads_rot_sig_test[, i] - mat_loads_rot_sig_train[, i])^2)
# }
# mse_loads <- mean(mse_vec)
# #------------------------------------------------------------
# # Now for frontier mse
# nab_pctRet_train <- apply(mat_pctDiff_train, 2, function(x) prod(1 + x)) - 1
# nab_pctRet_test <- apply(mat_pctDiff_test, 2, function(x) prod(1 + x)) - 1
# cormat_train <- cor(mat_pctDiff_train)
# cormat_test <- cor(mat_pctDiff_test)
# # cormat_train <- round(mat_loads_sig_train %*% t(mat_loads_sig_train), 7)
# # cormat_test <- round(mat_loads_sig_test %*% t(mat_loads_sig_test), 7)
# n_items <- ncol(mat_pctDiff_train)
# C_targ <- 1
# nab_C <- rep(1, n_items)
# mat_nab_train <- cbind(nab_pctRet_train, nab_C)
# #------------------------------------------------------------
# fun_env <- list(n_points_on_frontier,
#                 utility_interpretation = T,
#                 backtest_info = list(nab_pctRet_test, cormat_test),
#                 frontier_and_budget_plot = T,
#                 group_info = list(list_groups, group_names))
# list_out <- get_optimal_frontier(cormat_train, mat_nab_train,
#                                  Rtarg_limits = c(0.01, 0.3),
#                                  fig_title = "Figure 1: Optimal portfolio frontier",
#                                  fun_env)
# df_frontier <- list_out[[2]]
# df_backtest <- list_out[[3]]
# diff_R_sq <- (df_backtest$`Return backtest` - df_frontier$`Return target`)^2
# diff_V_sq <- (df_backtest$`Risk backtest` - df_frontier$`Risk (variance)`)^2
# mse_R <- mean(diff_R_sq)
# mse_V <- mean(diff_V_sq)
# meanEucDist <- mean(sqrt(diff_R_sq + diff_V_sq))
# #------------------------------------------------------------
# out_vec <- c(mse_loads, mse_R, mse_V, meanEucDist)
# return(out_vec)
# 
# }
# #============================================================
# seq_fraction_train <- seq(0.45, 0.6, length.out = 50)
# seq_fraction_N <- seq(0.4, 1, length.out = 20)
# list_out <- list()
# t <- 0
# for(i in 1:length(seq_fraction_train)){
#   this_fraction_train <- seq_fraction_train[i]
#     print(this_fraction_train)
#     list_out[[i]] <- optimal_divide_objFn(mat_pctDiff,
#                                         fraction_train = this_fraction_train,
#                                         fraction_N = 1)
# 
#   # for(j in 1:length(seq_fraction_N)){
#   #       this_fraction_N <- seq_fraction_N[j]
#   #       print(this_fraction_N)
#   #       t <- t + 1
#   # list_out[[t]] <- optimal_divide_objFn(mat_pctDiff,
#   #                                       fraction_train = 1 / 2,#this_fraction_train,
#   #                                       fraction_N = this_fraction_N)
#   # }
# 
# }
# df <- as.data.frame(do.call(rbind, list_out))
# colnames(df) <- c("Loadings MSE", "Return MSE", "Risk MSE", "Euc. Distance")
# df$Fraction <- seq_fraction_train
# #df$Fraction <- seq_fraction_N
# df <- subset(df, `Euc. Distance` < 1)
# hist(df$`Euc. Distance`)
# hist(df$`Return MSE`)
# hist(df$`Risk MSE`)
# # plot(df$Fraction, df$`Euc. Distance`)
# # plot(df$`Loadings MSE`, df$`Euc. Distance`)
# 
# 
# 
# 
# plot_frontier_wBacktest(df_frontier,
#                         df_backtest,
#                         fig_title = "Figure 6: Optimal portfolio solution vs. backtest, utility interpretation, signals")


```



#### 4.1.4 Dealing with negative budget weights

One of the main problems limiting the usefulness of MV analysis is that the optimal weights usually include negative values. A negative sign on a budget weight indicates that one should invest in the inverse of the corresponding portfolio item. In the financial context, this is possible through short selling, or through investment in financial products that track the inverse of a given product. However, there remains a more serious problem: a portfolio with both negative and positive weights implies that the investor must spend beyond their budget---i.e., they must borrow---in order to take up the corresponding position on the efficient frontier. In the example above, some of the higher return positions on the frontier require that the investor borrow amounts that are equal to 30%-50% of their budget. (...This is part of the motivation for eigenportfolios, there is a theorem guaranteeing that the components of the leading eigenvector will always be positive if the correlation matrix has no negative elements.)

It is possible to enforce positive weights when solving for optimal portfolios using heuristic methods. However, these methods offer no guarantee that the solution found is a global optimum. An evolutionary algorithm is applied below...


```{r, echo=F}


# wLam <- exp(lamW::lambertW0(-nab_pctRet_train))
# wLam <- wLam / sum(wLam)
# t(nab_pctRet_train) %*% wLam
# #------------------------------------
# # Heuristic solution
# portfolio_heuristic_fn <- function(w_vec){
#   w_vec <- w_vec / sum(w_vec)
#   R <- as.numeric(t(nab_pctRet) %*% w_vec)
#   V <- as.numeric(t(w_vec) %*% cormat %*% w_vec)
#   obj_fn <- R
#   return(obj_fn)
# }
# env <- environment(fun = portfolio_heuristic_fn)
# env[["nab_pctRet"]] <- nab_pctRet_train
# env[["cormat"]] <- cormat
# env[["nab_pctRet"]] <- nab_pctRet_test
# env[["cormat"]] <- cormat_test
# #------------------------------------
# out_malschains <- Rmalschains::malschains(function(w_vec) {-portfolio_heuristic_fn(w_vec)},
#                                           lower = as.vector(rep(0, n_items)),
#                                           upper = as.vector(rep(1, n_items)),
#                                           verbosity = 0,
#                                           env = env)
# w_vec <- out_malschains$sol
# w_vec <- w_vec / sum(w_vec)
# R <- as.numeric(t(nab_pctRet_train) %*% w_vec)
# V <- as.numeric(t(w_vec) %*% cormat %*% w_vec)
# R
# V
# #------------------------------------


# n_signals <- ncol(mat_eigvecs_sig_train)
# mat_eigvecs_sig_train_aug <- cbind(mat_eigvecs_sig_train, nab_C)
# Q <- t(mat_eigvecs_sig_train_aug) %*% mat_eigvecs_train
# # M <- round(Q %*% diag(1 / eigvals) %*% t(mat_eigvecs) %*% mat_sig_eigvecs_aug, 6)
# M <- round(Q %*% diag(1 / eigvals_train) %*% t(Q), 7)
# M
# M_inv <- round(solve(M), 5)
# M_inv
# 
# targ_vec <-  c(0.2 * rep(1, n_signals), 1)
# #targ_vec <- c(0.06, 0.06, 0.02, 0.02, 1)
# x <- -2 * M_inv %*% targ_vec
# l_V <- 1 / x[1]
# lambdas <- l_V * x
# lambdas
# l_C <- lambdas[length(lambdas)]
# 
# wStar <- -1 / (2 * l_V) * mat_eigvecs_train %*% diag(1 / eigvals_train) %*% t(mat_eigvecs_train) %*% mat_eigvecs_sig_train_aug %*% lambdas
# pctDiff_wStar <- mat_pctDiff_test %*% wStar
# 
# backtest_ret <- prod(1 + pctDiff_wStar) - 1
# #sd(pctDiff_wStar)
# sum(wStar)
# 
# t(wStar) %*% cormat %*% wStar




```


Another way, not yet explored in the literature, is to interpret the optimal weights as utility weights rather than budget weights. Recall that utility is defined in terms of the   Utility is defined in terms of a first order relation such that increments in the quantity of a given portfolio item are valued in proportion to the amount of the item already accumulated.... Defined up to an affine transformation.


```{r, fig.width=8, fig.height=6, fig.align='center', echo = FALSE}

fun_env <- list(n_points_on_frontier,
                utility_interpretation = T,
                backtest_info = list(nab_pctRet_test, cormat_test),
                frontier_and_budget_plot = T,
                group_info)

list_out <- get_optimal_frontier(cormat, mat_nab,
                                 Rtarg_limits = c(0.01, 4),
                                 fig_title = "Figure 3: Optimal portfolio frontier, utility weights",
                                 fun_env)

df_frontier <- list_out[[2]]
df_backtest <- list_out[[3]]


```



```{r, fig.width=8, fig.height=4, fig.align='center', echo = FALSE}



#-------------------------------------------
# Frontier plot with backtest
plot_frontier_wBacktest(df_frontier,
                        df_backtest,
                        fig_title = "Figure 4: Optimal portfolio solution vs. backtest, utility interpretation")
#-------------------------------------------



```





```{r, echo=F}

#=======================================================================
# Signals correlation matrix risk-reward frontier
n_signals <- ncol(mat_loads_sig_train)
nab_C <- rep(1, n_signals)
#------------------------------------
# Correlation matrix
cormat <- cor(mat_pctDiff_sig_train)
cormat_test <- cor(mat_pctDiff_sig_test)
#------------------------------------
# Expected returns vector
#nab_pctRet_train <- apply(mat_pctDiff_sig_train[ind_equal_test, ], 2, function(x) prod(1 + x)) - 1
nab_pctRet_train <- apply(mat_pctDiff_sig_train, 2, function(x) prod(1 + x)) - 1
nab_pctRet_test <- apply(mat_pctDiff_sig_test, 2, function(x) prod(1 + x)) - 1
#------------------------------------
mat_nab <- cbind(nab_pctRet_train, nab_C)
row.names(mat_nab) <- signal_names
#------------------------------------
fun_env <- list(n_points_on_frontier,
                utility_interpretation = T,
                backtest_info = list(nab_pctRet_test, cormat_test),
                frontier_and_budget_plot = T,
                group_info = NULL)

list_out <- get_optimal_frontier(cormat, mat_nab,
                                 Rtarg_limits = c(0.1, 0.35),
                                 fig_title = "Figure 5: Optimal portfolio frontier, utility weights, signals",
                                 fun_env)

df_frontier <- list_out[[2]]
df_backtest <- list_out[[3]]


#------------------------------------



```


```{r, echo=F}

#-------------------------------------------
# Frontier plot with backtest
plot_frontier_wBacktest(df_frontier,
                        df_backtest,
                        fig_title = "Figure 6: Optimal portfolio solution vs. backtest, utility interpretation, signals")
#-------------------------------------------

```


## Example 2: Mod-MV analysis to determine optimal investments across strategic objectives in the AR4D context

The aim of investment here is not merely to share in the fortunes of the portfolio items, but rather to play a role in their shaping. High commodity price is an indication of demand outstripping supply. The aim of the CBOE speculator is to prosper from that scarcity and to aggravate it by removing from circulation as much of the commodity as possible, thereby raising the price further, the aim of the AR4D donor is the exact opposite of this. The donor invests in AR4D so as to increase production and quality of the crops and thereby to bring down their price.

Interpretation of "price" is different from the financial context. This could reflect 1) higher kcals per unit weight, 2) higher quality kcals, 3) higher demand, or 4) some combination of all of the above.

based on export price, because there are enough data to permit a backtest. but should really focus on farmgate price

The data is very low resolution. Results vary considerably depending on geographical focus. I do export price first in order to have enough data to apply backtests and then farmgate price. The backtests diverge considerably more from the optimal frontier than in the financial context.


```{r, echo=F}
#=======================================================================
# Define geographic areas of focus
area_vec <- c("World","Low Income Food Deficit Countries", "Net Food Importing Developing Countries",
              "Least Developed Countries", "Eastern Africa", "Western Africa", "South America",
              "Southern Asia", "Southern Africa", "Middle Africa", "Asia",
              "Sub-Saharan Africa")
# Define commodities of interest
cereal_vec <- c("Maize", "Wheat", "Sorghum", "Rice", "Millet") #"Rice, paddy"
pulses_oilcrops_vec <- c("Beans, dry", "Cow peas, dry", "Chick peas", "Lentils", "Soybeans", "Groundnuts, shelled") #"Groundnuts, with shell"
RnT_vec <- c("Cassava Equivalent", "Yams", "Potatoes", "Sweet potatoes")#"Cassava"
item_vec <- c(cereal_vec, pulses_oilcrops_vec, RnT_vec)
#------------------
list_groups <- list(cereal_vec, pulses_oilcrops_vec, RnT_vec)
group_names <- c("Cereals", "Pulses & Oilcrops", "Roots & Tubers")
group_info <- list(list_groups, group_names)
#------------------------------
#Prep export value data
df_expVal_raw <- read.csv("Trade_Crops_Livestock_E_All_Data.csv", stringsAsFactors = F)
#colnames(df_expVal_raw)
df_expVal_raw$Area.Code <- NULL
df_expVal_raw$Item.Code <- NULL
df_expVal_raw$Element.Code <-NULL
u <- colnames(df_expVal_raw)
#colnames(df_expVal_raw)
df_expVal_raw <- df_expVal_raw[, -grep("F", u)]
colnames(df_expVal_raw)[5:ncol(df_expVal_raw)] <- as.character(c(1961:(1961 + ncol(df_expVal_raw) - 5)))
df_expVal_raw <- gather(df_expVal_raw,Year,Value,`1961`:`2016`)
#------------------
#unique(df_expVal_raw$Item)[grep("cassava", unique(df_expVal_raw$Item), ignore.case = T)]
#unique(df_expVal_raw$Element)
#------------------
df_expVal <- subset(df_expVal_raw, Item %in% item_vec)
df_expVal$Unit <- NULL
#------------------
# Create SSA group
# unique(df_expVal$Area)[grep("africa", unique(df_expVal_raw$Area), ignore.case = T)]
SSA_vec <- c("South Africa", "Africa", "Eastern Africa", "Middle Africa", "Southern Africa", "Western Africa")
df_x <- subset(df_expVal, Area %in% SSA_vec)
df_x <- as.data.frame(df_x %>% group_by(Year, Item, Element) %>% summarise(Value = sum(Value, na.rm = T)))
df_x$Area <- "Sub-Saharan Africa"
df_x <- df_x[, colnames(df_expVal)]
df_expVal <- as.data.frame(rbind(df_expVal, df_x))
#------------------
df_expVal <- subset(df_expVal, Area %in% area_vec)
#unique(df_expVal_raw$Element)
element_vec <- c("Export Quantity", "Export Value")
df_expVal <- subset(df_expVal, Element %in% element_vec)
#df_expVal <- subset(df_expVal, Year > 1961)
df_expVal$Unit <- NULL
df_expVal <- df_expVal %>% spread(Element, Value)
df_expVal$`Export Price` <- df_expVal$`Export Value` / df_expVal$`Export Quantity`
#------------------
# Create group id variable for plotting
df_expVal$Group <- NA
u <- df_expVal$Item
df_expVal$Group[which(u %in% cereal_vec)] <- "Cereals"
df_expVal$Group[which(u %in% pulses_oilcrops_vec)] <- "Pulses and Oilcrops"
df_expVal$Group[which(u %in% RnT_vec)] <- "Roots & Tubers"
df_expVal$Group <- factor(df_expVal$Group)
#------------------
check_on_data = F
if(check_on_data){
  # Export value plots
  df_plot <- subset(df_expVal, Year == 2016)
  df_plot <- subset(df_plot, Area == "World")
  xx <- df_plot$Group
  df_plot$Item <- factor(df_plot$Item, levels = unique(df_plot$Item[order(xx)]))
  df_plot <- df_plot %>% gather_("Element", "Value", colnames(df_plot[4:6]))
  gg <- ggplot(df_plot, aes(x = Item, y = Value, fill = Group))
  gg <- gg + geom_bar(stat = "identity", position = "dodge")
  gg <- gg + facet_wrap(~Element, scales = "free_y")
  gg <- gg + theme(axis.title.x = element_blank(),
                   axis.text.x = element_text(angle = 60, hjust = 1),
                   axis.title.y = element_blank(),
                   legend.title = element_blank())
  #gg <- gg + coord_flip()
  gg
  
  df_plot <- subset(df_expVal, Year == 2016)
  df_plot$Area[grep("Least Developed Countries", df_plot$Area)] <- "Least Developed\nCountries"
  df_plot$Area[grep("Low Income Food Deficit Countries", df_plot$Area)] <- "Low Income\nFood Deficit Countries"
  xx <- df_plot$Group
  df_plot$Item <- factor(df_plot$Item, levels = unique(df_plot$Item[order(xx)]))
  gg <- ggplot(df_plot, aes(x = Item, y = `Export Price`, fill = Group))
  gg <- gg + geom_bar(stat = "identity", position = "dodge")
  gg <- gg + facet_wrap(~Area)
  gg <- gg + theme(axis.title.y = element_blank(),
                   axis.text.y = element_text(size = 7))
  gg <- gg + coord_flip()
  gg
  
  
  df_plot <- df_expVal
  df_plot$Year <- as.integer(df_plot$Year)
  xx <- df_plot$Group
  df_plot$Item <- factor(df_plot$Item, levels = unique(df_plot$Item[order(xx)]))
  gg <- ggplot(df_plot, aes(x = Year, y = `Export Price`, fill = Item))
  gg <- gg + geom_area(position = "stack")
  gg <- gg + facet_wrap(~Area, scales = "free")
  gg
  df_plot <- as.data.frame(df_plot %>% group_by(Area, Year) %>% mutate(sum_x = sum(`Export Price`)))
  df_plot$`Export Price (share)` <- df_plot$`Export Price` / df_plot$sum_x
  gg <- ggplot(df_plot, aes(x = Year, y = `Export Price (share)`, fill = Item))
  gg <- gg + geom_area(position = "stack")
  gg <- gg + facet_wrap(~Area)
  gg
  
  
}

#-----------------------------
# Portfolio optimization
#-----------------------------
ret_sinceYr <- 1967
ret_toYr <- 2016
fraction_train <- 2 / 3
#-----------------------------
df2 <- subset(df_expVal, Area == "Low Income Food Deficit Countries")
#"Low Income Food Deficit Countries"
#"Net Food Importing Developing Countries"
#"Sub-Saharan Africa"
#"South America"
#"Asia"
#"World"
#unique(df_expVal$Area)[grep("deficit", unique(df_expVal$Area), ignore.case = T)]
# df2 <- subset(df_expVal, Area == "Net Food Importing Developing Countries")
# list_groups_mod <- list_groups
#-----------------------------
# # If looking at Low Income Food Deficit Countries, have to remove Sweet potatoes
# df2 <- subset(df_expVal, Area == "Low Income Food Deficit Countries" & Item != "Sweet potatoes")
# list_groups_mod <- list_groups
# list_groups_mod[[3]] <- list_groups_mod[[3]][which(list_groups_mod[[3]] != "Sweet potatoes")] 
#-----------------------------


df2 <- df2[, c("Item", "Year", "Export Price")]
df2 <- df2 %>% spread(Item, `Export Price`)
ind_ret_sinceYr <- which(df2$Year == ret_sinceYr)
ind_ret_toYr <- which(df2$Year == ret_toYr)
df2 <- df2[ind_ret_sinceYr:ind_ret_toYr, ]
# nab_mu_ret_check <- apply(df2[, -1], 2, function(x) (x[ind_ret_toYr] - x[ind_ret_sinceYr]) / x[ind_ret_sinceYr])
mat2 <- as.matrix(df2[, -1])
mat2 <- na.approx(mat2)
mat_pctDiff <- diff(mat2) / mat2[-nrow(mat2), ]
year_vec <- df2$Year[-1]
ind_ret_sinceYr <- which(year_pctDiff_vec == ret_sinceYr)
ind_ret_toYr <- which(year_pctDiff_vec == ret_toYr)
nab_mu_ret <- apply(mat_pctDiff, 2, function(x) prod(1 + x[(ind_ret_sinceYr - 1):(ind_ret_toYr - 1)])) - 1
#nab_mu_ret_check - nab_mu_ret
row.names(mat_pctDiff) <- year_vec
mat_pctDiff <- mat_pctDiff
#=======================================================================
ind_train <- 1:round(nrow(mat_pctDiff) * fraction_train)
ind_test <- setdiff(1:nrow(mat_pctDiff), ind_train)
mat_pctDiff_train <- mat_pctDiff[ind_train, ]
mat_pctDiff_test <- mat_pctDiff[ind_test, ]
#--------------------------------------------------
list_out <- signals_from_noise(mat_pctDiff_train,
                               eigenvalue_density_plot = T,
                               signals_plot = T,
                               pca_var_plot = T,
                               pca_ind_plot = T,
                               group_info,
                               quietly = F)
# list_out <- list(mat_loads_sig, mat_loads_rot_sig, mat_loads, mat_loads_rot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
#mat_loads_sig <- list_out[[1]]
mat_loads_rot_sig_train <- list_out[[2]]
# mat_loads <- list_out[[3]]
# mat_laods_rot <- list_out[[4]]
# mat_pctDiff_sig <- list_out[[5]]
# eigvals_sig <- list_out[[6]]
# mat_eigvecs_sig <- list_out[[7]]
# eigvals <- list_out[[8]]
# mat_eigvecs <- list_out[[9]]

signal_names <- c(NULL)
interpret_loadings(mat_loads_rot_sig_train,
                   group_info,
                   signal_names = signal_names)
# Get test data
list_out <- signals_from_noise(mat_pctDiff_test,
                               eigenvalue_density_plot = T,
                               signals_plot = T,
                               pca_var_plot = T,
                               pca_ind_plot = T,
                               group_info,
                               quietly = F)
# list_out <- list(mat_loads_sig, mat_loads_rot_sig, mat_loads, mat_loads_rot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
#mat_loads_sig <- list_out[[1]]
mat_loads_rot_sig_test <- list_out[[2]]
signal_names <- c(NULL)
interpret_loadings(mat_loads_rot_sig_test,
                   group_info,
                   signal_names = signal_names)
cormat_test <- cor(mat_pctDiff_test)
nab_pctRet_test <- apply(mat_pctDiff_test, 2, function(x) prod(1 + x)) - 1
#=======================================================================
# Conventional risk-reward frontier
if(class(mat_loads_rot_sig_train) == "numeric"){n_items <- length(mat_loads_rot_sig_train)}
if(class(mat_loads_rot_sig_train) == "matrix"){n_items <- nrow(mat_loads_rot_sig_train)
}

C_targ <- 1
nab_C <- rep(1, n_items)
#------------------------------------
# Correlation matrix
cormat_train <- cor(mat_pctDiff_train)
#cormat <- round(mat_loads_rot_sig_train %*% t(mat_loads_rot_sig_train), 7)
#cormat <- round(mat_loads_sig_train %*% t(mat_loads_sig_train), 7)
# mse <- mean((cor(mat_pctDiff_train) - cormat)^2)
# mse
#------------------------------------
# Expected returns vector
#ind_t_ret <- ind_ret_sinceYr:ind_ret_toYr
nab_pctDiff_train <- apply(mat_pctDiff_train, 2, function(x) prod(1 + x)) - 1
#------------------------------------
mat_nab_train <- cbind(nab_pctDiff_train, nab_C)
n_points_on_frontier <- 50
#------------------------------------
list_out <- get_optimal_frontier(cormat_train, mat_nab_train,
                                 Rtarg_limits = c(3, 70),
                                 n_points_on_frontier = n_points_on_frontier,
                                 backtest_info = list(nab_pctRet_test, cormat_test),
                                 utility_interpretation = T,
                                 frontier_and_budget_plot = T,
                                 group_info = NULL,
                                 fig_title = NULL)

df_frontier <- list_out[[2]]
df_backtest <- list_out[[3]]


#-------------------------------------------
# Frontier plot with backtest
plot_frontier_wBacktest(df_frontier,
                        df_backtest,
                        fig_title = "Figure 10: Optimal portfolio solution vs. backtest, utility interpretation",
                        separate_plots = T)
#-------------------------------------------


```

So, in this back test, real returns to the optimal portfolio were much lower than those indicated by the optimal frontier, but nonetheless substantial (60%-90%). This is probably due to the low resolution of the data and the long time horizons. A lot can happen in 30 years. The usefulness of a backtest is  questionable, since the idea of investment is not merely to ride the fortunes of the portfolio items, but to play a role in shaping them.

The optimal portfolio looking ahead from 2016 (the last year of available data), based on the returns and correlations of the precceding 40 years of data, is computed below.


```{r, echo=F}

#the signals can be used in optimization
#defining the return window, and whether to define expected return as the return over a recent window or the average of returns over many small windows. In the development context, this 
#"Gross Prod. Value / MT\n(current USD)"
sigExtract_and_portfoliOptimization <- function(df_in,
                                                Rtarg_limit = c(0.1, 5),
                                                period = NULL,
                                                purge_cormat_noise = F,
                                                signals_replace_items = F,
                                                fraction_train = NULL,
                                                fig_title_frontier_wBudget = NULL,
                                                fun_env = NULL
){
  #===========================================
  # Unpack function environments
  if(is.null(fn_envs)){
    fun_env_signals_from_noise <- standard_env_signals_from_noise
    fun_env_interpret_loadings <- standard_env_interpret_loadings
    fun_env_get_optimal_frontier <- standard_env_get_optimal_frontier
  }else{
    fun_env_signals_from_noise <- fun_env[[1]]
    fun_env_interpret_loadings <- fun_env[[2]]
    fun_env_get_optimal_frontier <- fun_env[[3]]
  }
  #===========================================
  # Prepare data
  #-------------------------------------------
  # df_in must have dimensions nrow = length(time series), ncol = n_items + 1, with the first column as date vec (Year, in the case of FAO data).
  #-------------------------------------------
  # Restrict period
  if(!is.null(period)){
    ind_ret_sinceYr <- which(df$Year == period[1])
    ind_ret_toYr <- which(df$Year == period[2])
    df <- df[ind_ret_sinceYr:ind_ret_toYr, ]
  }
  #-------------------------------------------
  mat <- as.matrix(df[, -1])
  #-------------------------------------------
  # Replace NA with interplation
  mat <- na.approx(mat)
  #-------------------------------------------
  # Get percentage difference
  mat_pctDiff <- diff(mat) / mat[-nrow(mat), ]
  year_vec <- df$Year[-1]
  row.names(mat_pctDiff) <- year_vec
  #-------------------------------------------
  n_items <- ncol(mat_pctDiff)
  #===========================================
  # Extract signals from noise
  #-------------------------------------------
  if(is.null(fraction_train)){
    list_out <- signals_from_noise(mat_pctDiff, fun_env_signals_from_noise)
    #list_out <- list(mat_loads_sig, mat_loads_rot_sig, mat_loads, mat_loads_rot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
    mat_loads_rot_sig <- list_out[[2]]
    #-------------------------------------------
    # Interpret, characterize signals (Which items load the most onto each signal?)
    interpret_loadings(mat_loads_rot_sig, fun_env_interpret_loadings)
    #-------------------------------------------
  }else{
    #-------------------------------------------
    ind_train <- 1:round(nrow(mat_pctDiff) * fraction_train)
    ind_test <- setdiff(1:nrow(mat_pctDiff), ind_train)
    mat_pctDiff_train <- mat_pctDiff[ind_train, ]
    mat_pctDiff_test <- mat_pctDiff[ind_test, ]
    #-------------------------------------------
    list_out <- signals_from_noise(mat_pctDiff_train, fun_env_signals_from_noise)
    mat_loads_rot_sig <- list_out[[2]]
    interpret_loadings(mat_loads_rot_sig, fun_env_interpret_loadings)
    #-------------------------------------------
    list_out <- signals_from_noise(mat_pctDiff_test, fun_env_signals_from_noise)
    mat_loads_rot_sig_test <- list_out[[2]]
    interpret_loadings(mat_loads_rot_sig_test, fun_env_interpret_loadings)
    #-------------------------------------------
    df_loads_train <- data.frame(mat_loads_rot_sig)
    df_loads_test <- data.frame(mat_loads_rot_sig_test)
    mse_vec <- c()
    for(i in 1:min(ncol(df_loads_train), ncol(df_loads_test))){
      mse_vec[i] <- mean((df_loads_test[, i] - df_loads_train[, i])^2)
    }
    print("Test data loadings vs. train data loadings (mse):", mse_vec)
    mse_loads <- mse_vec[1]
  }
  #===========================================
  # Portfolio optimization
  
  get_optimal_frontier_wrapper()
  
}


#=======================================================================


```

Now we look at farmgate price.

```{r, echo=F}

#=======================================================================
# Have to rename some crops because names in the production and value of production datasets differ somewhat from those in the trade dataset used above.
cereal_vec <- c("Maize", "Wheat", "Sorghum", "Rice, paddy", "Millet")
pulses_oilcrops_vec <- c("Beans, dry", "Cow peas, dry", "Chick peas", "Lentils", "Soybeans", "Groundnuts, with shell")
RnT_vec <- c("Cassava", "Yams", "Potatoes", "Sweet potatoes")
item_vec <- c(cereal_vec, pulses_oilcrops_vec, RnT_vec)
#------------------------------------
list_groups <- list(cereal_vec, pulses_oilcrops_vec, RnT_vec)
group_names <- c("Cereals", "Pulses & Oilcrops", "Roots & Tubers")
group_info <- list(list_groups, group_names)
#------------------------------------
# Get total value of ag production data
#rm(df_vap_raw, df_vap)
#gc()
df_vap_raw <- read.csv("Value_of_Production_E_All_Data.csv", stringsAsFactors = F)
colnames(df_vap_raw)
df_vap_raw$Area.Code <- NULL
df_vap_raw$Item.Code <- NULL
df_vap_raw$Element.Code <-NULL
u <- colnames(df_vap_raw)
colnames(df_vap_raw)
df_vap_raw <- df_vap_raw[, -grep("F", u)]
colnames(df_vap_raw)[5:ncol(df_vap_raw)] <- as.character(c(1961:(1961 + ncol(df_vap_raw) - 5)))
df_vap_raw <- gather(df_vap_raw,Year,Value,`1961`:`2016`)
#------------------
#unique(df_vap_raw$Area)[grep("africa", unique(df_vap_raw$Area), ignore.case = T)]
#unique(df_vap_raw$Item)[grep("potato", unique(df_vap_raw$Item), ignore.case = T)]
#------------------
df_vap <- subset(df_vap_raw, Area %in% area_vec)
df_vap <- subset(df_vap, Item %in% item_vec)
#unique(df_vap_raw$Element)
element_vec <- c("Gross Production Value (current million US$)")
df_vap <- subset(df_vap, Element %in% element_vec)
df_vap <- subset(df_vap, Year > 1990)
df_vap$Unit <- NULL
df_vap$Element <- NULL
colnames(df_vap)[4] <- "Gross Production Value\n(current million USD)"
#------------------------------
# Get production data
df_prod_raw <- read.csv("Production_Crops_E_All_Data.csv", stringsAsFactors = F)
df_prod_raw <- subset(df_prod_raw, Item.Code != 2928)
df_prod_raw$Area.Code <- NULL
df_prod_raw$Item.Code <- NULL
df_prod_raw$Element.Code <-NULL
df_prod_raw$Unit <- NULL
u <- colnames(df_prod_raw)
df_prod_raw <- df_prod_raw[, -grep("F", u)]
last_yr <- (1961 + ncol(df_prod_raw) - 4)
colnames(df_prod_raw)[4:ncol(df_prod_raw)] <- as.character(c(1961:last_yr))
gathercols <- colnames(df_prod_raw)[4:ncol(df_prod_raw)]
df_prod_raw <- gather_(df_prod_raw, "Year", "Value", gathercols)
#------------------------------
#unique(df_prod_raw$Item)[grep("beans", unique(df_prod_raw$Item), ignore.case = T)]
df_prod <- subset(df_prod_raw, Item %in% item_vec)
df_prod <- subset(df_prod, Area %in% area_vec)
df_prod <- subset(df_prod, Element == "Production")
df_prod <- subset(df_prod, Year > 1990)
df_prod$Element <- NULL
colnames(df_prod)[4] <- "Production"
#-----------------------------
# df_prod$Group <- NA
# u <- df_prod$Item
# df_prod$Group[which(u %in% cereal_vec)] <- "Cereals"
# df_prod$Group[which(u %in% pulses_oilcrops_vec)] <- "Pulses and Oilcrops"
# df_prod$Group[which(u %in% RnT_vec)] <- "Roots & Tubers"
# df_prod$Group <- factor(df_prod$Group)
#-----------------------------
df <- merge(df_vap, df_prod, by = c("Area", "Year", "Item"))
#-----------------------------
# Get SSA region
df_x <- subset(df, Area %in% SSA_vec)
df_x <- as.data.frame(df_x %>% group_by(Year, Item) %>% summarise_at(c("Gross Production Value\n(current million USD)", "Production"), sum, na.rm = T))
df_x$Area <- "Sub-Saharan Africa"
df_x <- df_x[, colnames(df)]
df <- as.data.frame(rbind(df, df_x))
df$`Gross Prod. Value / MT\n(current USD)` <- 10^6 * df$`Gross Production Value\n(current million USD)` / df$Production
#-----------------------------
# Get grouping info
df$Group <- NA
u <- df$Item
df$Group[which(u %in% cereal_vec)] <- "Cereals"
df$Group[which(u %in% pulses_oilcrops_vec)] <- "Pulses and Oilcrops"
df$Group[which(u %in% RnT_vec)] <- "Roots & Tubers"
df$Group <- factor(df$Group)
#-----------------------------
# kcalMT_cereals_vec <- c(4.14, )
# df_kcal <- data.frame(Item = item_vec, kcal_per_MT = kcalMT_vec)
#-----------------------------
# VAP Plots
check_on_data <- F
if(check_on_data){
  df_plot <- subset(df, Year == 2016)
  df_plot <- subset(df_plot, Area == "World")
  xx <- df_plot$Group
  df_plot$Item <- factor(df_plot$Item, levels = unique(df_plot$Item[order(xx)]))
  df_plot$Production <- NULL
  df_plot <- df_plot %>% gather_("Element", "Value", colnames(df_plot[5:6]))
  gg <- ggplot(df_plot, aes(x = Item, y = Value, fill = Group))
  gg <- gg + geom_bar(stat = "identity", position = "dodge")
  gg <- gg + facet_wrap(~Element, scales = "free_y")
  gg <- gg + theme(axis.title.x = element_blank(),
                   axis.text.x = element_text(angle = 60, hjust = 1),
                   axis.title.y = element_blank(),
                   legend.title = element_blank())
  #gg <- gg + coord_flip()
  gg
  #-----------------------------
  df_plot <- subset(df, Year == 2016)
  df_plot$Area[grep("Least Developed Countries", df_plot$Area)] <- "Least Developed\nCountries"
  df_plot$Area[grep("Low Income Food Deficit Countries", df_plot$Area)] <- "Low Income\nFood Deficit Countries"
  xx <- df_plot$Group
  df_plot$Item <- factor(df_plot$Item, levels = unique(df_plot$Item[order(xx)]))
  gg <- ggplot(df_plot, aes(x = Item, y = `Gross Prod. Value / MT\n(current USD)`, fill = Group))
  gg <- gg + geom_bar(stat = "identity", position = "dodge")
  gg <- gg + facet_wrap(~Area)
  gg <- gg + theme(axis.title.y = element_blank(),
                   axis.text.y = element_text(size = 7))
  gg <- gg + coord_flip()
  gg
  
  
  df_plot <- subset(df_plot, Area == "World")
  gg <- ggplot(df_plot, aes(x = Item, y = `Gross Prod. Value / MT\n(current USD)`, fill = Group))
  gg <- gg + geom_bar(stat = "identity", position = "dodge")
  #gg <- gg + facet_wrap(~Area)
  gg <- gg + theme(axis.title.y = element_blank(),
                   axis.text.y = element_text(size = 7))
  #gg <- gg + coord_flip()
  gg
  #-----------------------------
  df_plot <- df
  df_plot$Year <- as.integer(df_plot$Year)
  xx <- df_plot$Group
  df_plot$Item <- factor(df_plot$Item, levels = unique(df_plot$Item[order(xx)]))
  gg <- ggplot(df_plot, aes(x = Year, y = `Gross Prod. Value / MT (current USD)`, fill = Item))
  gg <- gg + geom_area(position = "stack")
  gg <- gg + facet_wrap(~Area)
  gg
  df_plot <- as.data.frame(df_plot %>% group_by(Area, Year) %>% mutate(sum_vap = sum(`Gross Prod. Value / MT (current USD)`)))
  df_plot$`Gross Prod. Value / MT (share)` <- df_plot$`Gross Prod. Value / MT (current USD)` / df_plot$sum_vap
  gg <- ggplot(df_plot, aes(x = Year, y = `Gross Prod. Value / MT (share)`, fill = Item))
  gg <- gg + geom_area(position = "stack")
  gg <- gg + facet_wrap(~Area)
  gg
  #-----------------------------
  # Same on a per kcal basis
  #.....
}


#======================================================
#"Low Income Food Deficit Countries"
#"Net Food Importing Developing Countries"
#"Sub-Saharan Africa"
#"South America"
#"Asia"
#"World"
vap_area <- "World"
ret_sinceYr <- 1991
ret_toYr <- 2016
#======================================================
df2 <- subset(df, Area == vap_area)
if(vap_area == "South America"){df2 <- subset(df2, !(Item %in% c("Cow peas, dry", "Millet")))}
df2 <- df2[, c("Item", "Year", "Gross Prod. Value / MT\n(current USD)")]
df2 <- df2 %>% spread(Item, `Gross Prod. Value / MT\n(current USD)`)
ind_ret_sinceYr <- which(df2$Year == ret_sinceYr)
ind_ret_toYr <- which(df2$Year == ret_toYr)
df2 <- df2[ind_ret_sinceYr:ind_ret_toYr, ]
# nab_mu_ret_check <- apply(df2[, -1], 2, function(x) (x[ind_ret_toYr] - x[ind_ret_sinceYr]) / x[ind_ret_sinceYr])
mat2 <- as.matrix(df2[, -1])
mat2 <- na.approx(mat2)
mat_pctDiff <- diff(mat2) / mat2[-nrow(mat2), ]
year_vec <- df2$Year[-1]
ind_ret_sinceYr <- which(year_pctDiff_vec == ret_sinceYr)
ind_ret_toYr <- which(year_pctDiff_vec == ret_toYr)
nab_mu_ret <- apply(mat_pctDiff, 2, function(x) prod(1 + x[(ind_ret_sinceYr - 1):(ind_ret_toYr - 1)])) - 1
#nab_mu_ret_check - nab_mu_ret
row.names(mat_pctDiff) <- year_vec
mat_pctDiff <- mat_pctDiff



#this_ind <- (nrow(mat_pctDiff) - 40):nrow(mat_pctDiff)
list_out <- signals_from_noise(mat_pctDiff,#[this_ind, ],
                               eigenvalue_density_plot = T,
                               signals_plot = T,
                               pca_var_plot = T,
                               pca_ind_plot = T,
                               list_groups = list_groups,
                               group_names = group_names,
                               quietly = F)
mat_loads_rot_sig <- list_out[[2]]
signal_names <- c(NULL)
interpret_loadings(mat_loads_rot_sig,
                   list_groups = list_groups,
                   group_names = group_names,
                   signal_names = signal_names)

#=======================================================================
# Conventional risk-reward frontier
if(class(mat_loads_rot_sig) == "numeric"){n_items <- length(mat_loads_rot_sig)}
if(class(mat_loads_rot_sig) == "matrix"){n_items <- nrow(mat_loads_rot_sig)
}

C_targ <- 1
nab_C <- rep(1, n_items)
#------------------------------------
# Correlation matrix
cormat <- cor(mat_pctDiff)
#cormat <- round(mat_loads_rot_sig %*% t(mat_loads_rot_sig), 7)
#cormat <- round(mat_loads_sig %*% t(mat_loads_sig), 7)
# mse <- mean((cor(mat_pctDiff) - cormat)^2)
# mse
#------------------------------------
# Expected returns vector
#ind_t_ret <- ind_ret_sinceYr:ind_ret_toYr
nab_pctDiff <- apply(mat_pctDiff, 2, function(x) prod(1 + x)) - 1
#------------------------------------
mat_nab <- cbind(nab_pctDiff, nab_C)
n_points_on_frontier <- 50
#------------------------------------
if(vap_area == "South America"){these_Rtarg_limits <- c(2, 10)}
if(vap_area == "Sub-Saharan Africa"){these_Rtarg_limits <- c(9, 25)}
if(vap_area == "Asia"){these_Rtarg_limits <- c(12, 60)}
if(vap_area == "Low Income Food Deficit Countries"){these_Rtarg_limits <- c(1, 10)}
if(vap_area == "Net Food Importing Developing Countries"){these_Rtarg_limits <- c(2, 12)}
if(vap_area == "World"){these_Rtarg_limits <- c(1, 15)}
this_fig_title <- paste("Optimal Portfolio Frontier,", vap_area)
list_out <- get_optimal_frontier(cormat, mat_nab,
                                 Rtarg_limits = these_Rtarg_limits,
                                 n_points_on_frontier = n_points_on_frontier,
                                 backtest_info = NULL,
                                 utility_interpretation = T,
                                 frontier_and_budget_plot = T,
                                 list_groups = NULL,
                                 group_names = NULL,
                                 # list_groups = list_groups_mod,
                                 # group_names = group_names_mod,
                                 fig_title = this_fig_title)


```







## Example 3: Mod-MV analysis to determine optimal investments across SDGs




```{r, echo=FALSE}
#setwd("D:/OneDrive - CGIAR/Documents")
#-------------------------------------------------------------
remove_countries <- c("São Tomé and Principe", "Micronesia, Fed. Sts.")
WDI_country_classification <- read.csv("WDICountry.csv", stringsAsFactors = F)
Low_income_countries <- unique(WDI_country_classification$Table.Name[which(WDI_country_classification$Income.Group == "Low income")])

Lower_middle_income_countries <- unique(WDI_country_classification$Table.Name[which(WDI_country_classification$Income.Group == "Lower middle income")])
Upper_middle_income_countries <- unique(WDI_country_classification$Table.Name[which(WDI_country_classification$Income.Group == "Upper middle income")])

WDI_raw <- read.csv("WDIData.csv", stringsAsFactors = F)
WDI_raw$Country.Code <- NULL
WDI_raw$Indicator.Code <- NULL
WDI_raw$X <- NULL
colnames(WDI_raw)[1:2] <- c("Country", "Indicator")
WDI_raw$Country <- as.character(WDI_raw$Country)
WDI_raw$Indicator <- as.character(WDI_raw$Indicator)
#unique(WDI_raw$Country)
#unique(WDI_raw$Indicator)
unique(WDI_raw$Indicator[grep("basic", WDI_raw$Indicator, ignore.case = T)])
colnames(WDI_raw)[3:ncol(WDI_raw)] <- as.character(c(1960:2018))
WDI_long <- WDI_raw %>% gather(Year, Value, `1960`:`2018`)
#unique(WDI_raw$Country)
WDI_long <- as.data.frame(WDI_long %>% group_by(Year, Indicator) %>% mutate(world_mu = mean(Value, na.rm = T)))
ind_worldGini <- which(WDI_long$Country == "World" & WDI_long$Indicator == "GINI index (World Bank estimate)")
WDI_long$Value[ind_worldGini] <- WDI_long$world_mu[ind_worldGini]
ind_worldIncomeBottom <- which(WDI_long$Country == "World" & WDI_long$Indicator == "Income share held by lowest 20%")
WDI_long$Value[ind_worldIncomeBottom] <- WDI_long$world_mu[ind_worldIncomeBottom]
WDI_long$world_mu <- NULL
WDI_long <- as.data.frame(WDI_long %>% group_by(Year, Indicator) %>% mutate(world_sum = sum(Value, na.rm = T)))
ind_worldBatDeaths <- which(WDI_long$Country == "World" & WDI_long$Indicator == "Battle-related deaths (number of people)")
WDI_long$Value[ind_worldBatDeaths] <- WDI_long$world_sum[ind_worldBatDeaths]
WDI_long$world_sum <- NULL
WDI_long$Value[which(is.nan(WDI_long$Value))] <- NA
#unique(WDI_long$Indicator)
#unique(WDI_raw$Indicator)[grep("Average", unique(WDI_raw$Indicator))]
#-------------------------------------------------------------

```



```{r, fig.width=10, fig.height=4, fig.align='center', echo = FALSE}

environmental_indicators <- c("CO2 emissions (metric tons per capita)", 
                              #"Combustible renewables and waste (% of total energy)",
                              "CO2 intensity (kg per kg of oil equivalent energy use)",
                              "Forest area (% of land area)",
                              "GDP per unit of energy use (PPP $ per kg of oil equivalent)",
                              "Energy intensity level of primary energy (MJ/$2011 PPP GDP)",
                              "Renewable energy consumption (% of total final energy consumption)",
                              "Agricultural methane emissions (thousand metric tons of CO2 equivalent)",
                              "Agricultural nitrous oxide emissions (thousand metric tons of CO2 equivalent)",
                              "Methane emissions in energy sector (thousand metric tons of CO2 equivalent)",
                              "Nitrous oxide emissions in energy sector (thousand metric tons of CO2 equivalent)",
                              "Fertilizer consumption (kilograms per hectare of arable land)",
                              "Fertilizer consumption (% of fertilizer production)",
                              "Adjusted savings: net forest depletion (% of GNI)",
                              "Adjusted savings: mineral depletion (% of GNI)",
                              "Adjusted savings: energy depletion (% of GNI)",
                              "Level of water stress: freshwater withdrawal as a proportion of available freshwater resources",
                              #"Access to clean fuels and technologies for cooking (% of population)",
                              "Renewable internal freshwater resources per capita (cubic meters)"
                              #"Water productivity, total (constant 2010 US$ GDP per cubic meter of total freshwater withdrawal)"
)
#-------------------------------------------------------------
food_sec_indicators <- c("Agricultural land (% of land area)",
                         "Land under cereal production (hectares)",
                         "Prevalence of undernourishment (% of population)",
                         "Food imports (% of merchandise imports)",
                         "Agriculture, forestry, and fishing, value added per worker (constant 2010 US$)",
                         "Agriculture, value added per worker (constant 2010 US$)",
                         "Agriculture, forestry, and fishing, value added (% of GDP)",
                         #"Agriculture, forestry, and fishing, value added (annual % growth)",
                         "Food, beverages and tobacco (% of value added in manufacturing)"
)

economic_growth_indicators <- c(
  "Oil rents (% of GDP)",
  "Coal rents (% of GDP)",
  "Forest rents (% of GDP)",
  "Natural gas rents (% of GDP)",
  #"Total natural resources rents (% of GDP)",
  "Mineral rents (% of GDP)",
  #"Foreign direct investment, net (BoP, current US$)",
  "Foreign direct investment, net inflows (BoP, current US$)",
  #"Foreign direct investment, net inflows (% of GDP)",
  #"Export value index (2000 = 100)",
  #"Import value index (2000 = 100)",
  "GDP per capita (current US$)",
  "Net ODA received per capita (current US$)",
  "Trade (% of GDP)",
  #"GDP growth (annual %)",
  #"GDP per capita growth (annual %)",
  "Industry (including construction), value added (% of GDP)",
  #"Industry (including construction), value added (annual % growth)",
  "Manufacturing, value added (% of GDP)",
  #"Manufacturing, value added (annual % growth)",
  "Machinery and transport equipment (% of value added in manufacturing)",
  "Services, value added (% of GDP)",
  #"Services, value added (annual % growth)",
  "Trade in services (% of GDP)",
  #"Exports of goods and services (annual % growth)",
  #"Imports of goods and services (annual % growth)",
  #"Medium and high-tech Industry (including construction) (% manufacturing value added)",
  #"Gross capital formation (annual % growth)",
  #"Gross fixed capital formation (annual % growth)",
  #"Final consumption expenditure (annual % growth)",
  "Final consumption expenditure (constant 2010 US$)",
  "Employment in agriculture (% of total employment) (modeled ILO estimate)",
  "Employment in industry (% of total employment) (modeled ILO estimate)",
  "Employment in services (% of total employment) (modeled ILO estimate)",
  "Labor force participation rate, total (% of total population ages 15+) (modeled ILO estimate)",
  #"Unemployment, total (% of total labor force) (modeled ILO estimate)",
  "Industry, value added per worker (constant 2010 US$)",
  "Industry (including construction), value added per worker (constant 2010 US$)",
  "Services, value added per worker (constant 2010 US$)"
)
#-------------------------------------------------------------
peaceHRights_indicators <- c("Military expenditure (% of GDP)",
                             "Ratio of female to male labor force participation rate (%) (modeled ILO estimate)",
                             "Refugee population by country or territory of origin",
                             "Internally displaced persons, new displacement associated with conflict and violence (number of cases)",
                             "Internally displaced persons, total displaced by conflict and violence (number of people)",
                             "Battle-related deaths (number of people)",
                             "Literacy rate, youth (ages 15-24), gender parity index (GPI)",
                             "School enrollment, primary (gross), gender parity index (GPI)",
                             "School enrollment, primary and secondary (gross), gender parity index (GPI)",
                             "School enrollment, secondary (gross), gender parity index (GPI)",
                             "School enrollment, tertiary (gross), gender parity index (GPI)",
                             "Income share held by lowest 20%"
)
#-------------------------------------------------------------
# economic_equality_indicators <- c("Consumer price index (2010 = 100)",
#                                   "Income share held by lowest 10%",
#                                   "Income share held by lowest 20%",
#                                   #"GINI index (World Bank estimate)",
#                                   "Risk premium on lending (lending rate minus treasury bill rate, %)",
#                                   #"Poverty gap at $1.90 a day (2011 PPP) (%)",
#                                   "Poverty headcount ratio at $1.90 a day (2011 PPP) (% of population)",
#                                   "Number of people pushed below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure",
#                                   "Number of people pushed below the $3.10 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure",
#                                   "Number of people spending more than 10% of household consumption or income on out-of-pocket health care expenditure",
#                                   "Number of people spending more than 25% of household consumption or income on out-of-pocket health care expenditure",
#                                   "Mortality rate attributed to unsafe water, unsafe sanitation and lack of hygiene (per 100,000 population)",
#                                   "People using at least basic sanitation services (% of population)",
#                                   "People using at least basic sanitation services, rural (% of rural population)",
#                                   "People using at least basic sanitation services, urban (% of urban population)",
#                                   "People using safely managed sanitation services (% of population)",
#                                   "People using safely managed sanitation services, rural (% of rural population)",
#                                   "People using safely managed sanitation services, urban (% of urban population)",
#                                   "People using at least basic drinking water services (% of population)",
#                                   "People using at least basic drinking water services, rural (% of rural population)",
#                                   "People using at least basic drinking water services, urban (% of urban population)",
#                                   "Labor force with basic education, female (% of female working-age population with basic education)",
#                                   "Labor force with basic education, male (% of male working-age population with basic education)"
# )
#-------------------------------------------------------------
infrastructure_indicators <- c(#"Access to electricity (% of population)",
  "Access to electricity, urban (% of urban population)",
  "Access to electricity, rural (% of rural population)",
  "Air transport, freight (million ton-km)",
  "Air transport, passengers carried",
  "Railways, goods transported (million ton-km)",
  "Railways, passengers carried (million passenger-km)"
  #"Mobile cellular subscriptions (per 100 people)"
  #"Individuals using the Internet (% of population)"
)
#-------------------------------------------------------------
healthPop_indicators <- c("Mortality rate, under-5 (per 1,000 live births)",
                          #"Incidence of HIV (% of uninfected population ages 15-49)",
                          "Life expectancy at birth, total (years)",
                          # "Life expectancy at birth, male (years)",
                          # "Life expectancy at birth, female (years)"
                          "Prevalence of HIV, total (% of population ages 15-49)",
                          #"Birth rate, crude (per 1,000 people)",
                          #"Death rate, crude (per 1,000 people)",
                          "Urban population (% of total population)",
                          "Rural population (% of total population)",
                          #"Urban population growth (annual %)",
                          #"Population growth (annual %)",
                          #"Population ages 0-14, total",
                          #"Population ages 0-14 (% of total)",
                          #"Population ages 15-64, total",
                          #"Population ages 15-64 (% of total)",
                          #"Population ages 65 and above, total",
                          #"Population ages 65 and above (% of total)",
                          "Population density (people per sq. km of land area)",
                          "Population growth (annual %)",
                          "Physicians (per 1,000 people)",
                          # "Population, total",
                          #"Age dependency ratio, young (% of working-age population)",
                          #"Age dependency ratio, old (% of working-age population)",
                          "Age dependency ratio (% of working-age population)",
                          "Adolescent fertility rate (births per 1,000 women ages 15-19)"
                          
)
#-------------------------------------------------------------
educ_indicators <- c("Government expenditure on education, total (% of GDP)",
                     "School enrollment, primary (% gross)",
                     "School enrollment, secondary (% gross)",
                     "School enrollment, tertiary (% gross)",
                     #"Adjusted savings: education expenditure (current US$)",
                     "Adjusted savings: education expenditure (% of GNI)",
                     "Expenditure on primary education (% of government expenditure on education)",
                     "Expenditure on secondary education (% of government expenditure on education)",
                     "Expenditure on tertiary education (% of government expenditure on education)",
                     "Government expenditure on education, total (% of government expenditure)",
                     "Literacy rate, adult total (% of people ages 15 and above)",
                     "Literacy rate, youth total (% of people ages 15-24)"
)
#-------------------------------------------------------------
indicator_vec <- c(healthPop_indicators, food_sec_indicators, educ_indicators,
                   economic_growth_indicators, #economic_equality_indicators,
                   environmental_indicators, peaceHRights_indicators,
                   infrastructure_indicators)
list_groups <- list(healthPop_indicators, food_sec_indicators, educ_indicators,
                    economic_growth_indicators, #economic_equality_indicators,
                    environmental_indicators, peaceHRights_indicators,
                    infrastructure_indicators)
group_names <- c("Health/Population", "Food Security", "Education",
                 "Economic Growth", #"Economic Equality",
                 "Environmental Sustainability", "Equality, Peace, Human Rights", "Infrastructure")
names(list_groups) <- group_names
#-------------------------------------------------------------
#"Least developed countries: UN classification"
#"Sub-Saharan Africa (excluding high income)"
#"Lower middle income"
these_countries <- "World"
#-------------------------------------------------------------
df_WDI <- subset(WDI_long, Country %in% c(these_countries))
df_WDI <- subset(df_WDI, Indicator %in% indicator_vec)
df_WDI <- df_WDI %>% spread(Indicator, Value)
df_WDI$Year <- as.integer(df_WDI$Year)
df_WDI <- subset(df_WDI, Year >= 1990)
df_WDI <- subset(df_WDI, Year <= 2016)
#df_WDI$`Individuals using the Internet (% of population)`[which(df_WDI$Year < 1994)] <- 0
#-------------------------------------------------------------
year_vec <- df_WDI$Year
df_WDI <- df_WDI[, -c(1, 2)]
out <- apply(df_WDI, 2, function(x) length(which(is.na(x))))
#table(out)
ind_rm <- which(as.numeric(out) > 10)
#length(ind_rm)
df_WDI <- df_WDI[, -ind_rm]
#ncol(df_WDI)
ind_rp <- which(is.na(df_WDI[1, ]))
if(length(ind_rp) != 0){
  for(i in 1:length(ind_rp)){
    df_WDI[1, ind_rp[i]] <- mean(df_WDI[, ind_rp[i]], na.rm = T)
  }
}
ind_rp <- which(is.na(df_WDI[nrow(df_WDI), ]))
for(i in 1:length(ind_rp)){
  df_WDI[nrow(df_WDI), ind_rp[i]] <- mean(df_WDI[, ind_rp[i]], na.rm = T)
}

#df_WDI <- as.data.frame(na.approx(df_WDI[, -c(1, 2)]))
df_WDI <- as.data.frame(na.approx(df_WDI))
#class(df_WDI)
out <- apply(df_WDI, 2, function(x) length(which(is.na(x))))
table(out)
#which(as.numeric(out) > 0)
#colnames(df_WDI)[which(as.numeric(out) > 0)]
#-----------------------------------------------------------
track_inverse <- c("Agricultural methane emissions (thousand metric tons of CO2 equivalent)",
                   "Agricultural nitrous oxide emissions (thousand metric tons of CO2 equivalent)",
                   "Methane emissions in energy sector (thousand metric tons of CO2 equivalent)",
                   "Nitrous oxide emissions in energy sector (thousand metric tons of CO2 equivalent)",
                   "CO2 emissions (metric tons per capita)",
                   "CO2 intensity (kg per kg of oil equivalent energy use)",
                   "Adjusted savings: energy depletion (% of GNI)",
                   "Adjusted savings: net forest depletion (% of GNI)",
                   "Adjusted savings: mineral depletion (% of GNI)",
                   "Adolescent fertility rate (births per 1,000 women ages 15-19)",
                   "Age dependency ratio (% of working-age population)",
                   "Children out of school (% of primary school age)",
                   "Military expenditure (% of GDP)",
                   "Mortality rate, under-5 (per 1,000 live births)",
                   "Prevalence of undernourishment (% of population)",
                   "Refugee population by country or territory of origin",
                   "Prevalence of HIV, total (% of population ages 15-49)")
ind_track_inverse <- which(colnames(df_WDI) %in% track_inverse)
df_WDI[, ind_track_inverse] <- 1 / df_WDI[, ind_track_inverse]
colnames(df_WDI)[ind_track_inverse] <- paste("1 /", colnames(df_WDI)[ind_track_inverse])
for(i in 1:length(list_groups)){
  ind_chng <- which(list_groups[[i]] %in% track_inverse)
  list_groups[[i]][ind_chng] <- paste("1 /", list_groups[[i]][ind_chng])
  
}
#-----------------------------------------------------------
#mat_zWDI <- scale(df_WDI)
#mat_pctDiff <- diff(as.matrix(log(df_WDI)))
mat_pctDiff <- diff(as.matrix(df_WDI)) / as.matrix(df_WDI[-nrow(df_WDI), ])
row.names(mat_pctDiff) <- year_vec[-1]
# library(tidyquant)
# ema_per <- 2
# mat_pctDiff <- apply(df_WDI, 2, function(x) x - EMA(x, ema_per))
# row.names(mat_pctDiff) <- year_vec
# mat_pctDiff <- mat_pctDiff[-c(1:(ema_per - 1)), ]
# mat_pctDiff[is.nan(mat_pctDiff)] <- 0
# mat_pctDiff[is.infinite(mat_pctDiff)] <- 0
#-----------------------------------------------------------
df_plot <- as.data.frame(mat_pctDiff)
df_plot$Year <- year_vec[-1]
df_plot <- df_plot %>% gather_("Indicator", "Value", colnames(df_WDI))
df_plot$Type <- NA
df_plot$Type[which(df_plot$Indicator %in% healthPop_indicators)] <- "Health/Population"
#df_plot$Type[which(df_plot$Indicator %in% economic_equality_indicators)] <- "Economic Equality"
df_plot$Type[which(df_plot$Indicator %in% educ_indicators)] <- "Education"
df_plot$Type[which(df_plot$Indicator %in% economic_growth_indicators)] <- "Economic Growth"
df_plot$Type[which(df_plot$Indicator %in% environmental_indicators)] <- "Environmental Sustainability"
df_plot$Type[which(df_plot$Indicator %in% food_sec_indicators)] <- "Food Security"
df_plot$Type[which(df_plot$Indicator %in% infrastructure_indicators)] <- "Infrastructure"
df_plot$Type[which(df_plot$Indicator %in% peaceHRights_indicators)] <- "Equality, Peace, Human Rights"
#------------------------------------------------------------
# gg <- ggplot(df_plot, aes(x = Year, y = Value, group = Indicator, color = Indicator))
# gg <- gg + geom_line()
# gg <- gg + facet_wrap(~ Type, nrow = 3, scales = "free")
# gg <- gg + theme(legend.position = "none")
# gg
#------------------------------------------------------------
check_on_data <- F
if(check_on_data){
  this_type <- "Health/Population"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_health <- gg
  gg
  this_type <- "Education"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_educ <- gg
  gg
  this_type <- "Economic Growth"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_econ <- gg
  gg
  # this_type <- "Economic Equality"
  # df_plot2 <- subset(df_plot, Type == this_type)
  # gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  # gg <- gg + geom_line(lwd = 1.1)
  # gg <- gg + geom_point(size = 2)
  # gg <- gg + labs(title = this_type)
  # gg_employ <- gg
  # gg
  this_type <- "Environmental Sustainability"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_environ <- gg
  gg
  this_type <- "Food Security"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_pop <- gg
  gg
  this_type <- "Infrastructure"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_infrastr <- gg
  gg
  this_type <- "Equality, Peace, Human Rights"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_infrastr <- gg
  gg
  
}

```

```{r, fig.width=15, fig.height=12, fig.align='center', echo=FALSE}
#------------------------------------------------------------
list_out <- signals_from_noise(mat_pctDiff,
                               eigenvalue_density_plot = T,
                               signals_plot = T,
                               pca_var_plot = F,
                               pca_ind_plot = F,
                               list_groups = list_groups,
                               group_names = group_names,
                               quietly = F)
#list_out <- list(mat_loads_sig, mat_loads_rot_sig, mat_inData_sig, eigvals_sig, mat_eigvecs_sig)
mat_loads_sig <- list_out[[1]]
mat_loads_rot_sig <- list_out[[2]]


```


```{r, fig.width=15, fig.height=12, fig.align='center', echo=FALSE}

signal_names <- c("")
interpret_loadings(mat_loads_rot_sig,
                   list_groups = list_groups,
                   group_names = group_names,
                   signal_names = NULL)


```

```{r, fig.width=8, fig.height=6, fig.align='center', echo = FALSE}

#=======================================================================
# Conventional risk-reward frontier
n_items <- nrow(mat_loads_rot_sig)
C_targ <- 1
nab_C <- rep(1, n_items)
#------------------------------------
# Get pct return vec and correlation matrix only over a period equal in length to that of the test period.
#ind_equal_test <- (nrow(mat_pctDiff) - length(ind_test)):nrow(mat_pctDiff)
#------------------------------------
# Correlation matrix
#cormat <- cor(mat_pctDiff[ind_equal_test, ])
cormat <- round(cor(mat_pctDiff), 7)
#cormat_test <- cor(mat_pctDiff_test)
#cormat <- round(mat_loads_rot_sig %*% t(mat_loads_rot_sig), 7)
#cormat <- round(mat_loads_sig %*% t(mat_loads_sig), 7)
# mse <- mean((cor(mat_pctDiff) - cormat)^2)
# mse
#------------------------------------
# Expected returns vector
#nab_pctRet <- apply(mat_pctDiff[ind_equal_test, ], 2, function(x) prod(1 + x)) - 1
nab_pctRet <- apply(mat_pctDiff, 2, function(x) prod(1 + x)) - 1
#nab_pctRet_test <- apply(mat_pctDiff_test, 2, function(x) prod(1 + x)) - 1
#------------------------------------
mat_nab <- cbind(nab_pctRet, nab_C)
n_points_on_frontier <- 50
#------------------------------------
list_out <- get_optimal_frontier(cormat, mat_nab,
                                 Rtarg_limits = c(0.01, 3),
                                 n_points_on_frontier = n_points_on_frontier,
                                 utility_interpretation = F,
                                 backtest_info = NULL,
                                 frontier_and_budget_plot = T,
                                 list_groups = list_groups,
                                 group_names = group_names,
                                 fig_title = "Figure 8: Optimal portfolio frontier")

df_frontier <- list_out[[2]]
df_backtest <- list_out[[3]]
#------------------------------------
```




```{r, fig.width=8, fig.height=6, fig.align='center', echo = FALSE}

list_out <- get_optimal_frontier(cormat, mat_nab,
                                 Rtarg_limits = c(0.01, 25),
                                 n_points_on_frontier = n_points_on_frontier,
                                 utility_interpretation = T,
                                 backtest_info = NULL,
                                 frontier_and_budget_plot = T,
                                 list_groups = list_groups,
                                 group_names = group_names,
                                 fig_title = "Figure 3: Optimal portfolio frontier, utility weights")

df_frontier <- list_out[[2]]
df_backtest <- list_out[[3]]

```








## Discussion and conculsions

* When tackling complex, seemingly intangible, subjects such as "sustainability", or "resilience", or, in the case of this paper, "tradeoffs between strategic objectives", progress is often made by attempting to formalize---or otherwise bring down to earth---a longstanding, high level (and, at times, politically driven), conceptual narrative of the issue in terms of measurable inputs and outputs. Regardless of the degree of success in bringing the problem down to earth, the quantitative grappling with it forces us to examine it in greater granularity than is possible in the high level narrative. New problems, and/or connections to old problems, are invariably discovered along the way, unnoticed until now due to fixation on certain aspects of the problem that are emphasized in the conceptual narrative. [It is worth mentioning that this process is not subject to political pressures, and does not allow us to substitute real progress on the problem with conferences and rhetoric.]

* In the quantitative grappling presented here, it was discovered that, before the question of tradeoffs can even be asked, one must first address the question of dimensions. There is not just one set of tradeoffs, but a set for each dimension found in the data. The task of identifying and extracting dimensions is, then, a whole can of worms unto itself. The number of dimensions that can meaningfully be analyzed is a function both of the real structure that may or may not exist in the data, and of the amount of available data. Amazingly, there is a rigorous method of dimension extraction that takes all of this into account.

* Once dimensions are extracted, they must then be characterized in concrete terms. What aspect of reality is described by each dimension? This is achieved by applying a varimax rotation to the loadings matrix. If clear thematic trends fail to emerge after varimax rotation, this is motivation for a deep reassessment of underlying preconceptions and/or the quality of the data.

* Once dimensions have been characterized, the question of tradeoffs can be addressed.

* Having identified tradeoffs, the natural next question from a donor's perspective is: how to optimize investment across these tradeoffs? ...which then turns out to be intimately bound up with the issue of risk...portfolio optimization. An unsuspected opportunity.






## scraps:


```{r, echo=F}

Signals_from_noise <- function(mat_pctDiff, varimax_rot = T,
                               list_groups = NULL,
                               group_names = NULL,
                               pca_var_plot = F,
                               pca_ind_plot = F,
                               eigenvalue_density_plot = F,
                               loadings_plot = T,
                               sig_ts_plot = F,
                               sig_corr_plot = F,
                               yearly = F){
  varNames_ordered <- colnames(mat_pctDiff)
  date_vec <- row.names(mat_pctDiff)
  if(yearly){date_vec <- as.integer(date_vec)}
  n_obs <- ncol(mat_pctDiff)
  #----
  if(!is.null(list_groups)){
    group_vec <- rep(NA, n_obs)
    for(i in 1:length(list_groups)){
      this_group_vec <- list_groups[[i]]
      this_group_name <- group_names[i]
      group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
      
    }
  }
  #----
  if(pca_ind_plot){
    res <- FactoMineR::PCA(t(mat_pctDiff), graph = F)
    gg <- factoextra::fviz_pctDiffd(res, habillage = factor(group_vec), addEllipses = T)
    print(gg)
  }
  #----
  res <- FactoMineR::PCA(mat_pctDiff, ncp = ncol(mat_pctDiff), graph = F)
  #----
  if(pca_var_plot){
    gg <- factoextra::fviz_pca_var(res, habillage = factor(group_vec))
    print(gg)
  }
  # library(mclust)
  # mc <- Mclust(t(mat_pctDiff))
  # summary(mc)
  # View(mc$classification)
  # factoextra::fviz_cluster(mc, frame.type = "norm", geom = "text")
  #--Extraction of signals (main PCs)
  eigvals <- as.data.frame(res$eig)$eigenvalue
  eigval_max <- max(eigvals)
  N_t <- nrow(mat_pctDiff)
  N_c <- ncol(mat_pctDiff)
  Q <- N_t / N_c
  s_sq <- 1 - eigval_max / N_c
  #s_sq <- 1
  eigval_rand_max <- s_sq * (1 + 1 / Q + 2 / sqrt(Q))
  eigval_rand_min <- s_sq * (1 + 1 / Q - 2 / sqrt(Q))
  lam <- seq(eigval_rand_min, eigval_rand_max, 0.001)
  dens_rand <- Q / (2 * pi * s_sq) * sqrt((eigval_rand_max - lam) * (lam - eigval_rand_min)) / lam
  df_e <- data.frame(eigenvalues = eigvals)
  #--Eigenvalue density vs. random matrix eigenvalue density
  if(eigenvalue_density_plot){
    gg <- ggplot()
    gg <- gg + geom_density(data = df_e, aes(x = eigenvalues, color = "Correlation Matrix"), lwd = 1.1)
    gg <- gg + geom_line(data = data.frame(x = lam, y = dens_rand), aes(x = x, y = y, color = "Random matrix"), lwd = 1.1)
    gg <- gg + scale_colour_manual(name = "Eigenvalue density",
                                   values = c(`Correlation Matrix` = "blue", `Random matrix` = "orange"))
    print(gg)
  }
  #-----------------------------------------
  ind_deviating_from_noise <- which(eigvals > eigval_rand_max)# (eigval_rand_max + 5 * 10^-1))
  mat_Loads <- res$var$coord
  mat_signal_Loads <- mat_Loads[, ind_deviating_from_noise]
  mat_Loads_rot <- varimax(mat_Loads)[[1]]
  mat_signal_Loads_rot <- mat_Loads_rot[, ind_deviating_from_noise]
  mat_eigvecs <-  mat_Loads %*% diag(1 / sqrt(eigvals))
  eigvals_sig <- eigvals[ind_deviating_from_noise]
  n_signals <- length(eigvals_sig)
  print(paste("Number of signals: ", n_signals))
  mat_signal_eigvecs <- mat_eigvecs[, ind_deviating_from_noise]
  mat_signal_ts <- mat_pctDiff %*% mat_signal_eigvecs
  if(n_signals == 1){
    mat_signal_ts <- mat_signal_ts / eigvals_sig
  }else{
    mat_signal_ts <- mat_signal_ts %*% diag(1 / eigvals_sig)
  }
  # Set sign of eigenvectors such that they
  # best conform to the input time series
  #inData_avg <- mat_pctDiff %*% rep(1, n_obs) * 1 / n_obs
  inData_avg <- rowMeans(mat_pctDiff)
  if(n_signals == 1){
    #sse <- sum((mat_signal_ts - inData_avg)^2)
    #sse_neg <- sum((-mat_signal_ts - inData_avg)^2)
    mse <- mean((mat_signal_ts - inData_avg)^2)
    mse_neg <- mean((-mat_signal_ts - inData_avg)^2)
    if(mse_neg < mse){
      mat_signal_eigvecs <- -mat_signal_eigvecs
      mat_signal_ts <- -mat_signal_ts
    }
  }else{
    for(i in 1:n_signals){
      # sse <- sum((mat_signal_ts[, i] - inData_avg)^2)
      # sse_neg <- sum((-mat_signal_ts[, i] - inData_avg)^2)
      # sse_vec <- c(sse, sse_neg)
      mse <- mean((mat_signal_ts[, i] - inData_avg)^2)
      mse_neg <- mean((-mat_signal_ts[, i] - inData_avg)^2)
      if(mse_neg < mse){
        mat_signal_eigvecs[, i] <- -mat_signal_eigvecs[, i]
        mat_signal_ts[, i] <- -mat_signal_ts[, i]
      }
    }
    
  }
  #------------------------------------------------------------
  # Plot loadings barcharts
  if(varimax_rot){
    df_plot <- data.frame(ts_id = varNames_ordered, mat_signal_Loads_rot)
  }else{
    df_plot <- data.frame(ts_id = varNames_ordered, mat_signal_Loads)
  }
  signal_id <- paste("Signal", c(1:n_signals))
  colnames(df_plot)[2:(n_signals + 1)] <- signal_id
  gathercols <- as.character(signal_id)
  df_plot <- gather_(df_plot, "Signal", "Loading", gathercols)
  #--
  if(!is.null(list_groups)){
    # df_plot$Type <- NA
    # u <- as.character(df_plot$ts)
    # for(i in 1:length(list_groups)){
    #   ind <- which(u %in% list_groups[[i]])
    #   df_plot$Type[ind] <- group_names[i]
    # }
    # df_plot$Type <- as.factor(df_plot$Type)
    df_plot$Type <- factor(group_vec)
    xx <- df_plot$Type
    df_plot$ts_id <- factor(df_plot$ts_id, levels = unique(df_plot$ts_id[order(xx)]))
    gg <- ggplot(df_plot, aes(x = ts_id, y = Loading, fill = Type))
  }else{
    gg <- ggplot(df_plot, aes(x = ts_id, y = Loading))
  }
  #--
  gg <- gg + geom_bar(stat = "identity", position = "dodge")
  gg <- gg + facet_wrap(~ Signal, nrow = 1)
  #gg <- gg + labs(title = )
  gg <- gg + theme(axis.text.y = element_text(face = "bold", size = 10),
                   axis.text.x = element_text(face = "bold", size = 10),
                   axis.title.y = element_text(face = "bold", size = 10),
                   axis.title.x = element_text(face = "bold", size = 10))
  gg <- gg + coord_equal()
  # if(N_c <= 50){
  #   gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1),
  #                    axis.title.y = element_blank())
  # }else{
  #   gg <- gg + theme(axis.text.x = element_blank(),
  #                    axis.title.y = element_blank())
  # }
  gg <- gg + coord_flip()
  if(loadings_plot){print(gg)}
  #---------------------------------
  # Plot signal ts against average
  df_plot1 <- data.frame(Date = date_vec, inData_avg)
  df_plot2 <- data.frame(Date = date_vec, mat_signal_ts)
  df_plot1$Date <- factor(format(df_plot1$Date, format = "%y-%m-%d%"), ordered = T)
  df_plot2$Date <- factor(format(df_plot2$Date, format = "%y-%m-%d%"), ordered = T)
  xAxis_labels <- df_plot1$Date[seq(1, nrow(df_plot1), length.out = 10)]
  signal_id <- paste("Signal", c(1:n_signals))
  colnames(df_plot2)[2:(n_signals + 1)] <- signal_id
  gathercols <- signal_id
  df_plot2 <- df_plot2 %>% gather_("Signal", "Value", gathercols)
  gg <- ggplot()
  gg <- gg + geom_line(data = df_plot1, aes(x = Date, y = inData_avg, group = 1), color = "orange", lwd = 2)
  gg <- gg + geom_line(data = df_plot2, aes(x = Date, y = Value, group = 1))
  gg <- gg + scale_x_discrete(breaks = xAxis_labels)
  gg <- gg + facet_wrap(~ Signal, ncol = 1)
  gg <- gg + theme(axis.title.y = element_blank())
  gg
  if(sig_ts_plot){print(gg)}
  #--Correlation matrix
  if(sig_corr_plot){
    # t(mat_signal_ts) %*% mat_signal_ts
    # t(mat_signal_eigvecs) %*% mat_signal_eigvecs
    rcorr_out <- rcorr(mat_signal_ts)
    cormat <- rcorr_out$r
    print(cormat)
    pmat <- rcorr_out$P
    corrplot(cormat, type="upper", order="hclust", p.mat = pmat, sig.level = 0.01, insig = "blank", tl.col = "black", tl.srt = 45)
    
  }
  #---------------------------------
  df_sig_ts <- data.frame(date = date_vec, ts_sigs = mat_signal_ts)
  #df_sig_eigvecs <- data.frame(Loadings_sigs_rot = mat_signal_Loads_rot)
  list_out <- list(df_sig_ts, mat_signal_eigvecs, mat_eigvecs, mat_signal_Loads, mat_signal_Loads_rot, eigvals_sig, eigvals)
  return(list_out)
}



# wStar_normd <- wStar_sig1 / max(abs(wStar_sig1))
# #w_star_normd <- wStar_trad / max(abs(wStar_trad))
# varNames_ordered <- colnames(mat_pctDiff)
# df_plot <- data.frame(ts_id = varNames_ordered, mat_Loads_rot, wStar_normd)
# signal_id <- paste("Signal", c(1:n_sigs))
# colnames(df_plot)[2:(n_sigs + 1)] <- signal_id
# colnames(df_plot)[ncol(df_plot)] <- "Portfolio weights"
# gathercols <- colnames(df_plot)[-1]
# df_plot <- gather_(df_plot, "Signal", "Loading", gathercols)
# #--
# n_obs <- ncol(mat_pctDiff)
# group_vec <- rep(NA, n_obs)
# for(i in 1:length(list_groups)){
#   this_group_vec <- list_groups[[i]]
#   this_group_name <- group_names[i]
#   group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
# 
# }
# 
# df_plot$Type <- factor(group_vec)
# xx <- df_plot$Type
# df_plot$ts_id <- factor(df_plot$ts_id, levels = unique(df_plot$ts_id[order(xx)]))
# gg <- ggplot(df_plot, aes(x = ts_id, y = Loading, fill = Type))
# #--
# gg <- gg + geom_bar(stat = "identity", position = "dodge")
# gg <- gg + facet_wrap(~ Signal, nrow = 1)
# #gg <- gg + labs(title = )
# gg <- gg + theme(axis.text.y = element_text(face = "bold", size = 10),
#                  axis.text.x = element_text(face = "bold", size = 10),
#                  axis.title.y = element_text(face = "bold", size = 10),
#                  axis.title.x = element_text(face = "bold", size = 10))
# gg <- gg + coord_flip()
# gg



# 

```


<!-- Little or no care is taken, however, to address the tradeoffs that exist between investments in each of these thematic areas. It is well known, for example, that agriculture is often detrimental to the environment. This implies a tradeoff between the food security and environmental objectives. Economic growth, in its turn, is inversely related to both agricultural and environmental objectives. In many parts of the world, there is also a longstanding tradeoff between economic growth and economic equality. Investments in one thematic area can thus offset returns to investments in other thematic areas. -->

<!-- Moreover, conventional -->

<!-- ### I.ii Proposed solution: a PCA approach -->

<!-- In this concept note, I propose a rigorous, precise method for identifying and quantifying tradeoffs and synergies between strategic objectives (SOs). The method is based on principle components analysis (PCA) of a large dataset of development indicators spanning the usual thematic areas. (For a good overview of PCA, see Abdi & Williams (2010).) A variant of this approach has been applied in the analysis of financial market time series (Gopikrishnan et al., 2001). Here I enhance such precedents by also leveraging a rigorous signal selecting technique developed in the study of physical systems (Dehesa et al., 1983). -->
