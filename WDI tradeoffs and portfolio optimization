---
title: "Tradeoffs, risk, and optimization of investments across strategic objectives"
author: "Ben Schiek"
date: "August 4, 2019"
output:
  pdf_document:
    toc: true
    toc_depth: 4
    latex_engine: xelatex
mainfont: Calibri Light

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## 1. Introduction
### 1.1 Problem background
At the most abstract levels of strategic planning, it is customary for large development agencies and donor organizations to carefully define a manageable number of thematic areas for investment that broadly cover all aspects of development. The 17 Sustainable Development Goals (SDG), for example, are grouped  Persons (poverty reduction and human rights), Environmental Sustainability, Economic Prosperity, Peace, and Alliances ().

* Economic growth

* Economic equality, poverty reduction

* Food and nutritional security

* Environmental protection, conservation, reduced C02 emissions, etc.

* Health


...

### 1.2 Proposed solution

In this paper, I propose a method to identify and quantify such tradeoffs in the SDG Tracker historical data. I then combine this with methods from financial analysis to determine a donor's optimal allocation of investments across the indicators given the donor's level of risk tolerance.

#### 1.2.1 Eigendecomposition of SDG Tracker data

The method for identifying and quantifying tradeoffs is based on the eigendecomposition of the correlation matrix of SDG Tracker data. First, signals are separated from noise using a method developed in the analysis of physical systems (). The components of the retained eigenvectors are then interpreted as the magnitude and direction of the influence (a.k.a. "loading" or "score") of each SDG Tracker over the average movement of the dataset. Loadings with opposite signs indicate a tradeoff, while loadings with the same sign are indicative of synergy.

Beyond tradeoffs, this method is also useful for identifying just a few key crosscutting tendencies that are broadly characteristic of the entire dataset. In other words, the loadings on each eigenvector tend to be thematically organized, such that each eigenvector captures a particular aspect of the overall evolution of the system. This effectively reduces the problem of making sense of several dozen complexly interacting indicators to the more mangeable problem of making sense of just a few crosscutting trends.

#### 1.2.2 Risk adjusted portfolio optimization

The identification of tradeoffs and key crosscutting trends is useful in and of itself for stimulating and orienting high level policy discussion. However, longstanding methods used in the analogous decisionmaking context of finance are motivation to aim for more. In particular, a variant of Minimum Variance (MV) analysis can be applied to determine the optimal allocation of a budget across the---potentially several dozen---SDG indicators, given a certain risk tolerance.

It is worth taking a moment to emphasize that this goes far beyond the "priority setting", with which many donors, development agencies, and research for development (R4D) institutions may be familiar. The method proposed here, adapted from finance, determines, _precisely_, the portion of the donor's/development agency's/R4D research institution's budget that must be allocated in order to maximize net benefit, given a certain risk tolerance specified by the donor. (The budget can be a budget in the literal sense, or an "attention" or "enthusiasm", etc., budget.)

MV analysis currently suffers under longstanding methodological issues that severely limit its usefulness in any context, financial or otherwise (). In a nutshell, noise in the data makes accurate estimation of returns and correlations difficult. This means that one is optimizing over a great deal of noise. In the present adaptation of MV analyis, I propose to define the returns vector as a linear combination of the retained correlation matrix eigenvectors. This, in turn, automatically purges the correlation matrix of noisy elements. The problem is then set up for optimization over signal, not noise.

non-market valuation of each of the cross-cutting tendencies associated with each eigenvector---i.e. it endogenously determines the development agency's implicit willingness to pay for more of this or that cross-cutting tendency. This could be used to determine a given agency's investment preferences based on their observed spending. It could also become important later on in the formation of SDG markets (analogous to carbon markets) and/or initial coin offerings.

Many in the international development community may be unfamiliar with MV analysis. To build motivation and intuition for its application in the development context, I first walk through an example of its application in the financial context where it originated.

### 1.3 Summary of results

In the SDG Tracker data, three signals are pulled from the noise. Loadings on the first eigenvector are evenly distributed. Loadings on the second eigenvector eigenvector captures agricultural and/or natural resource intensive development, associated with relatively higher emissions and low industrial development, trade, and overall GDP growth.
...

Before addressing the problem of tradeoffs and optimization within the motivating context of this paper, it is instructive to first address these issues in the financial context where the methods employed here are rooted.


## 2. Methodological precedents

### 2.1 Separating signal from noise

Much of the present work is inspired by two papers in particular. In the first of these, so and so () showed how the important eigenvectors from noisy.... Their work, in turn, is adapted from the work of so and so (), who were interested in identifying the meaningful components of complex physical systems. A key theorem that emerged from this work says that the distribution of the eigenvalues of any random correlation matrix is a function of the dimensions of the underlying matrix (). It is therefore easy to compare the eigenvalue distribution of a given data correlation matrix against that of a randomly generated matrix. The eigenvalues of the data correlation matrix that lie outside of the random matrix eigenvalue distribution can be identified as corresponding to components of the system that can be meaningfully distinguished from noise.

An important implication of the theorem is that signal extraction is as much a function of data quality (i.e. whether there really is structure in the observed system, and how well this is captured in the data) as it is of data quantity. If none or very few of the eigenvalues of a given data correlation matrix can be distinguished from noise, this may be either because there is no structure to be found in the observed system, or because there are not enough observations to fully flesh out the structure. Just as a low resolution image of a person is difficult to distinguish from an image of randomly shaded pixels, underlying structure in the SDG Tracker data may be difficult to discern due to the low number of observations currently available. As resolution (number of observations) increases, the components of the system become more clearly rendered. 

It is surprising that the rigorous extraction method described above has not seen more uptake among data scientists. The problem of signal extraction extends far beyond the specific physical and financial settings addressed by so and so and so and so. It comes up every time someone conducts a principle components analysis (PCA) or factor analysis (FA). Moreover, the extraction methods appearing in published, peer reviewed PCA and FA studies are, by comparison, little more than arbitrary rules of thumb. "Keep only the signals that describe 90% of the variation", for example, or "the elbow rule", or "keep only those with eigenvalues greater than 1". The perils of such cavalier methods have been documented (), and yet they persist. Their persistence is made more perpliexing by the availability---since the 1960s---of the rigorous extraction method described above.

### 2.2 Signal characterization

So and so () did not go on to interpret the signals after presenting their method for extracting them. In the second paper inspiring the present work, so and so () addressed this question .

Here I adapt and combine these two advances into a single workflow.

### 2.3 Minimum variance analysis, and adaptation

$$R = \mathbf{w \cdot r}, \:\:\: C=\mathbf{w\cdot1}, \:\:\: V = \mathbf{w\cdot K \cdot w} \tag{1}$$


$$\max_{\mathbf{w}}{R} \:\:\:\:s.t. \:\:\: C=\overline{C} \:, \:\:\: V = \overline{V} \tag{2}$$

$$\mathcal{L} = R - \lambda_C(C - \overline{C}) - \lambda_{V}(V - \overline{V}) \tag{3}$$

$$\nabla \mathcal{L} = \mathbf{r} - \lambda_C \mathbf{1} - 2 \lambda_{V} K \cdot \mathbf{w} = \mathbf{0} \tag{4}$$

$$R^* = \lambda_C \overline{C} + 2 \lambda_V \overline{V} \tag{5}$$
Note this implies that the risk shadow price is proportional to the expected reward to risk ratio.

$$\lambda_V = \frac{1}{2} \frac{R^* - \lambda_C \overline{C}}{\overline{V}} \tag{6}$$


$$K^{-1} \cdot (\mathbf{r} - \lambda_C \mathbf{1}) = 2 \lambda_{V} \mathbf{w}^* \tag{5}$$

$$K^{-1} \cdot [\mathbf{r}, \: \mathbf{1}] \left[\begin{matrix} 1 \\ -\lambda_C \\ \end{matrix} \right] = 2 \lambda_{V} \mathbf{w}^* \tag{6}$$

$$ M\left[\begin{matrix}
1 \\
-\lambda_C \\
\end{matrix} \right] = 2 \lambda_{V} \left[\begin{matrix}
R \\
\bar{C} \\
\end{matrix} \right] \tag{7}$$

where 

$$M = [\mathbf{r}, \: \mathbf{1}]' \cdot K^{-1} \cdot [\mathbf{r}, \: \mathbf{1}] \tag{8}$$

$$\left[\begin{matrix}
1 \\
-\lambda_C \\
\end{matrix} \right] = 2 \lambda_{V} M^{-1} \left[\begin{matrix}
R \\
\bar{C} \\
\end{matrix} \right] \tag{9}$$

However, mean returns are notoriously difficult to measure.

$$\mathbf{r} = \tilde{P} \tilde{\Lambda} \cdot \mathbf{1}= \lambda_{1} \mathbf{p_1} + \lambda_{2} \mathbf{p_2} + \cdots +\lambda_{k} \mathbf{p_k} \tag{10}$$

$$K = P \Lambda P' \tag{11}$$

$$M = [\tilde{P}, \: \mathbf{1}]' \cdot K^{-1} \cdot [\tilde{P}, \: \mathbf{1}] \tag{12}$$

or

$$M = \left[\begin{matrix} 1/ \lambda_{1} & 0 & \cdots & 0 & 1/ \lambda_1 \mathbf{p_1} \cdot \mathbf{1} \\ 0 & 1/ \lambda_{2} & \cdots & 0 & 1/ \lambda_2 \mathbf{p_2} \cdot \mathbf{1} \\ \vdots & \vdots & \ddots & 0 & \vdots \\ 0 & 0 & 0 & 1/ \lambda_{k} & 1/ \lambda_k \mathbf{p_k} \cdot \mathbf{1}  \\ 1/ \lambda_1 \mathbf{1} \cdot \mathbf{p_1} & 1/ \lambda_2 \mathbf{1} \cdot \mathbf{p_2} & \cdots & 1/ \lambda_k \mathbf{1} \cdot \mathbf{p_k} & \mathbf{1} \cdot K \cdot \mathbf{1} \end{matrix} \right]$$

<!-- $$\left[\begin{matrix} \gamma_{1} \\ \gamma_{2} \\ \vdots \\ \gamma_{k} \\ -\gamma_C \\ \end{matrix} \right] = 2 \gamma_v \tilde{\Lambda} \left[\begin{matrix} r_1 \\ r_2 \\ \vdots \\ r_k \\ \bar{C} \\ \end{matrix} \right] - \gamma_c \left[ \begin{matrix} \mathbf{p}_1 \cdot \mathbf{1} \end{matrix} \right] \tag{13}$$ -->


$$\gamma_i = 2 \gamma_v \lambda_i r_i + \gamma_c \mathbf{p}_i \cdot \mathbf{1} \: ; \:\:\: i=1,2,\cdots,k \tag{}$$

$$\gamma_c =  \gamma_1 / \lambda_1 \mathbf{1} \cdot \mathbf{p_1} + \gamma_2 / \lambda_2 \mathbf{1} \cdot \mathbf{p_2} + \cdots + \gamma_k / \lambda_k \mathbf{1} \cdot \mathbf{p_k} + \gamma_c \mathbf{1} \cdot K \cdot \mathbf{1}$$

$$ -\frac{1}{2 \gamma_v}[\tilde{P}\tilde{\Lambda}^{-1}, \mathbf{1}] \left[\begin{matrix} \gamma_{1} \\ \gamma_{2} \\ \vdots \\ \gamma_{k} \\ -\gamma_c \\ \end{matrix} \right] = \mathbf{w}^* \tag{14}$$

$$-\frac{1}{2 \gamma_v}[\tilde{P}\tilde{\Lambda}^{-1}, \mathbf{1}] \cdot \mathbf{\gamma} =  \mathbf{w}^* \tag{5}$$


## 3. Data

Yahoo data ...The data below are intended to be representative of what a broad-minded investor might have on their radar screen these days, covering a wide range of asset classes.

WDI data . This is the data that the UN (SDG Tracker) uses to track progress towards SDGs. As mentioned above...low resolution...we are coming to a point where there are enough years of enough variables that we can begin to glimpse the contours of the main components. It will be exciting...as the picture comes slowly into greater and greater focus.


```{r, fig.width=10, fig.height=8, fig.align='center', echo = FALSE}
#setwd("D:/OneDrive - CGIAR/Documents")
#options(warn = -1); options(scipen = 999)
#-------------------------------------------------------------
#devtools::install_github("thomasp85/patchwork")
library(plyr)
# library(tidyverse)
# library(ggplot2)
library(zoo)
library(FactoMineR)
library(factoextra)
# library(Hmisc)
# library(corrplot)
library(tidyquant)
library(patchwork)
#=======================================================================
#=======================================================================
# Define functions
#=======================================================================
signals_from_noise <- function(mat_PCA_in,
                               eigenvalue_density_plot = T,
                               signals_plot = T,
                               pca_var_plot = F,
                               pca_ind_plot = F,
                               list_groups = NULL,
                               group_names = NULL,
                               quietly = F){
  #---------------------------------------------------------
  # Separate signals from noise
  #---------------------------------------------------------
  res <- FactoMineR::PCA(mat_PCA_in, ncp = ncol(mat_PCA_in), graph = F)
  eigvals <- as.data.frame(res$eig)$eigenvalue
  eigval_max <- max(eigvals)
  mat_loads <- res$var$coord
  mat_loads_rot <- varimax(mat_loads)[[1]]
  mat_eigvecs <- mat_loads %*% diag(1 / sqrt(eigvals))
  #---------------------------------------------------------
  # Apply random matrix theory () to determine eigenvalue distribution of a 
  # correlation matrix of random data.
  n_obs <- nrow(mat_PCA_in)
  n_var <- ncol(mat_PCA_in)
  Q <- n_obs / n_var
  s_sq <- 1 - eigval_max / n_var
  #s_sq <- 1
  eigval_rand_max <- s_sq * (1 + 1 / Q + 2 / sqrt(Q))
  eigval_rand_min <- s_sq * (1 + 1 / Q - 2 / sqrt(Q))
  lam <- seq(eigval_rand_min, eigval_rand_max, 0.001)
  eigval_rand_density <- Q / (2 * pi * s_sq) * sqrt((eigval_rand_max - lam) * (lam - eigval_rand_min)) / lam
  df_plot <- data.frame(Eigenvalues = eigvals)
  
  #   N_t <- nrow(mat_PCA_in)
  # N_c <- ncol(mat_PCA_in)
  # Q <- N_t / N_c
  # s_sq <- 1 - eigval_max / N_c
  # #s_sq <- 1
  # eigval_rand_max <- s_sq * (1 + 1 / Q + 2 / sqrt(Q))
  # eigval_rand_min <- s_sq * (1 + 1 / Q - 2 / sqrt(Q))
  # lam <- seq(eigval_rand_min, eigval_rand_max, 0.001)
  # dens_rand <- Q / (2 * pi * s_sq) * sqrt((eigval_rand_max - lam) * (lam - eigval_rand_min)) / lam
  # df_e <- data.frame(eigenvalues = eigvals)
  
  
  #---------------------------------------------------------
  # Plot eigenvalue density vs. random matrix eigenvalue density
  if(eigenvalue_density_plot){
    gg <- ggplot()
    gg <- gg + geom_density(data = df_plot, aes(x = Eigenvalues, color = "Correlation Matrix"), lwd = 1.1)
    gg <- gg + geom_line(data = data.frame(x = lam, y = eigval_rand_density), aes(x = x, y = y, color = "Random matrix"), lwd = 1.1)
    gg <- gg + scale_colour_manual(name = "Eigenvalue density", 
                                   values = c(`Correlation Matrix` = "blue", `Random matrix` = "magenta"))
    gg_eigenval_density <- gg
    if(!signals_plot){print(gg_eigenval_density)}
  }
  #---------------------------------------------------------
  # Which data eigenvalues can be meaningfully distinguished from noise?
  ind_deviating_from_noise <- which(eigvals > eigval_rand_max) # (eigval_rand_max + 5 * 10^-1))
  #---------------------------------------------------------
  # Extract signal loadings matrix from noise
  mat_loads_sig <- mat_loads[, ind_deviating_from_noise]
  eigvals_sig <- eigvals[ind_deviating_from_noise]
  mat_loads_rot_sig <- mat_loads_rot[, ind_deviating_from_noise]
  #---------------------------------------------------------
  n_signals <- length(eigvals_sig)
  if(!quietly){print(paste("Number of signals: ", n_signals))}
  #---------------------------------------------------------
  # Get dimensionally reduced version of original input data
  mat_eigvecs_sig <- mat_eigvecs[, ind_deviating_from_noise]
  mat_inData_sig <- mat_PCA_in %*% mat_eigvecs_sig
  if(n_signals == 1){
    mat_inData_sig <- mat_inData_sig / eigvals_sig
  }else{
    mat_inData_sig <- mat_inData_sig %*% diag(1 / eigvals_sig)
  }
  #---------------------------------------------------------
  # Set sign of eigenvectors such that they
  # best conform to the input time series
  inData_avg <- rowMeans(mat_PCA_in)
  if(n_signals == 1){
    mse <- mean((mat_inData_sig - inData_avg)^2)
    mse_neg <- mean((-mat_inData_sig - inData_avg)^2)
    if(mse_neg < mse){
      mat_eigvecs <- -mat_eigvecs
      mat_inData_sig <- -mat_inData_sig
    }
  }else{
    for(i in 1:n_signals){
      mse <- mean((mat_inData_sig[, i] - inData_avg)^2)
      mse_neg <- mean((-mat_inData_sig[, i] - inData_avg)^2)
      if(mse_neg < mse){
        mat_eigvecs_sig[, i] <- -mat_eigvecs_sig[, i]
        mat_inData_sig[, i] <- -mat_inData_sig[, i]
      }
    }
    
  }
  #---------------------------------------------------------
  # Other plots:
  # -Dimensionally reduced plot of data (signal plots)
  # -Cluster plots (PCA)
  #---------------------------------------------------------
  # These plots include grouping info, if available
  varNames_ordered <- row.names(mat_loads_sig)
  if(!is.null(list_groups)){
    group_vec <- rep(NA, n_var)
    for(i in 1:length(list_groups)){
      this_group_vec <- list_groups[[i]]
      this_group_name <- group_names[i]
      group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
      
    }
  }
  #---------------------------------------------------------
  # Plot signal data against average
  if(signals_plot){ 
    date_vec <- row.names(mat_PCA_in)
    df_plot1 <- data.frame(Date = date_vec, inData_avg)
    df_plot2 <- data.frame(Date = date_vec, mat_inData_sig)
    df_plot1$Date <- factor(format(df_plot1$Date, format = "%y-%m-%d%"), ordered = T)
    df_plot2$Date <- factor(format(df_plot2$Date, format = "%y-%m-%d%"), ordered = T)
    xAxis_labels <- df_plot1$Date[seq(1, nrow(df_plot1), length.out = 10)]
    signal_id <- paste("Signal", c(1:n_signals))
    colnames(df_plot2)[2:(n_signals + 1)] <- signal_id
    gathercols <- signal_id
    df_plot2 <- df_plot2 %>% gather_("Signal", "Value", gathercols)
    gg <- ggplot()
    gg <- gg + geom_line(data = df_plot1, aes(x = Date, y = inData_avg, group = 1), color = "orange", lwd = 2)
    gg <- gg + geom_line(data = df_plot2, aes(x = Date, y = Value, group = 1))
    gg <- gg + scale_x_discrete(breaks = xAxis_labels)
    gg <- gg + facet_wrap(~ Signal, ncol = 1)
    gg <- gg + theme(axis.title.y = element_blank(),
                     axis.text.x = element_text(angle = 60, hjust = 1))
    gg_signals <- gg
    if(!eigenvalue_density_plot){
      print(gg_signals)
    }else{
      gg_together <- ggpubr::ggarrange(gg_eigenval_density, gg_signals,
                                       #align = "v",
                                       #labels = c("A", "B"),
                                       ncol = 2, nrow = 1)
      print(gg_together)
    }
  }
  #---------------------------------------------------------
  # PCA cluster plots to examine natural grouping in the data
  #---------------------------------------------------------
  # By variable
  if(pca_var_plot){
    gg <- factoextra::fviz_pca_var(res, habillage = factor(group_vec))
    print(gg)
  }
  #---------------------------------------------------------
  # By individual
  if(pca_ind_plot){
    res <- FactoMineR::PCA(t(mat_PCA_in), graph = F)
    gg <- factoextra::fviz_pca_ind(res, habillage = factor(group_vec), addEllipses = T)
    print(gg)
  }
  #---------------------------------------------------------
  # Cluster plot using Mclust()
  # mc <- mclust::Mclust(t(mat_PCA_in))
  # summary(mc)
  # View(mc$classification)
  # factoextra::fviz_cluster(mc, frame.type = "norm", geom = "text")
  #---------------------------------------------------------
  list_out <- list(mat_loads_sig, mat_loads_rot_sig, mat_loads, mat_loads_rot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
  return(list_out)
  
}
#=======================================================================
# plot_signals <- function(mat_PCA_in, mat_eigvecs_sig, eigvecs_sig){
#   #---------------------------------------------------------
#   inData_avg <- rowMeans(mat_PCA_in)
#   n_signals <- length(eigvecs_sig)
#   mat_inData_sig <- mat_PCA_in %*% mat_eigvecs_sig
#   if(n_signals == 1){
#     mat_inData_sig <- mat_inData_sig / eigvals_sig
#   }else{
#     mat_inData_sig <- mat_inData_sig %*% diag(1 / eigvals_sig)
#   }
#   #---------------------------------------------------------
#   # Plot signal data against average
#   date_vec <- row.names(mat_PCA_in)
#   df_plot1 <- data.frame(Date = date_vec, inData_avg)
#   df_plot2 <- data.frame(Date = date_vec, mat_inData_sig)
#   df_plot1$Date <- factor(format(df_plot1$Date, format = "%y-%m-%d%"), ordered = T)
#   df_plot2$Date <- factor(format(df_plot2$Date, format = "%y-%m-%d%"), ordered = T)
#   xAxis_labels <- df_plot1$Date[seq(1, nrow(df_plot1), length.out = 10)]
#   signal_id <- paste("Signal", c(1:n_signals))
#   colnames(df_plot2)[2:(n_signals + 1)] <- signal_id
#   gathercols <- signal_id
#   df_plot2 <- df_plot2 %>% gather_("Signal", "Value", gathercols)
#   gg <- ggplot()
#   gg <- gg + geom_line(data = df_plot1, aes(x = Date, y = inData_avg, group = 1), color = "orange", lwd = 2)
#   gg <- gg + geom_line(data = df_plot2, aes(x = Date, y = Value, group = 1))
#   gg <- gg + scale_x_discrete(breaks = xAxis_labels)
#   gg <- gg + facet_wrap(~ Signal, ncol = 1)
#   gg <- gg + theme(axis.title.y = element_blank(),
#                    axis.text.x = element_text(angle = 60))
#   gg
#   print(gg)
#   
# }
#=======================================================================
interpret_loadings <- function(mat_loads_rot_sig,
                               list_groups = NULL,
                               group_names = NULL,
                               signal_names = NULL){
  #---------------------------------------------------------
  n_var <- nrow(mat_loads_rot_sig)
  n_signals <- ncol(mat_loads_rot_sig)
  varNames_ordered <- row.names(mat_loads_rot_sig)
  #------------------------------------------------------------
  # Plot loadings barcharts
  df_plot <- data.frame(id = varNames_ordered, mat_loads_rot_sig)
  #--------------
  # Name the signals, if names provided
  if(is.null(signal_names)){
    signal_id <- paste("Signal", c(1:n_signals))
  }else{
    signal_id <- signal_names
  }
  #--------------
  colnames(df_plot)[2:(n_signals + 1)] <- signal_id
  gathercols <- as.character(signal_id) 
  df_plot <- gather_(df_plot, "Signal", "Loading", gathercols)
  df_plot <- transform(df_plot,
                       Signal = factor(Signal,levels = gathercols))
  #--------------
  # Group the vars if group info is provided
  if(!is.null(list_groups)){
    group_vec <- rep(NA, n_var)
    for(i in 1:length(list_groups)){
      this_group_vec <- list_groups[[i]]
      this_group_name <- group_names[i]
      group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
    }
    #--------------
    df_plot$Type <- factor(group_vec)
    xx <- df_plot$Type
    df_plot$id <- factor(df_plot$id, levels = unique(df_plot$id[order(xx)]))
    gg <- ggplot(df_plot, aes(x = id, y = Loading, fill = Type))
  }else{
    gg <- ggplot(df_plot, aes(x = id, y = Loading))
  }
  gg <- gg + geom_bar(stat = "identity", position = "dodge")
  gg <- gg + facet_wrap(~ Signal, nrow = 1)
  gg <- gg + theme(axis.text.y = element_text(face = "bold", size = 10),
                   axis.text.x = element_text(face = "bold", size = 10),
                   axis.title.y = element_blank(),
                   axis.title.x = element_text(face = "bold", size = 10))
  gg <- gg + coord_equal()
  gg <- gg + coord_flip()
  print(gg)
  
}
#=======================================================================
optimize_portfolio <- function(cormat, mat_nab, targ_vec, mat_pctDiff_test = NULL){
  cormat_inv <- solve(cormat)
  M <- t(mat_nab) %*% cormat_inv %*% mat_nab
  M_inv <- solve(M)
  x <- -2 * M_inv %*% targ_vec
  l_V <- 1 / x[1]
  lambdas <- l_V * x
  wStar <- -1 / (2 * l_V) * cormat_inv %*% mat_nab %*% lambdas
  sum(wStar)
  if(!is.null(mat_pctDiff_test)){ts_wStar <- mat_pctDiff_test %*% wStar}else{ts_wStar <- NULL}
  V <- t(wStar) %*% cormat %*% wStar
  list_out <- list(wStar, lambdas, ts_wStar, V, l_V)
  return(list_out)
}
#=======================================================================
# End function definition
#=======================================================================
#=======================================================================

spy_sector_symbs <- c("XLF", "XLC", "XLY", "XLP", "XLV", "XLK", "RWR",
                      "XLU", "XLI", "XBI", "IYT") #"TTEK"
spy_sector_detail <- c("Financials", "Communications", "Luxury goods", "Staple goods",
                       "Healthcare", "Technology", "Real estate", "Utilities", "Industrial",
                       "Biotechnology", "Transportation") #"Gov. foreign aid"
minerals_symbs <- c("GLD", "SLV", "PPLT", "JJC", "JJM") #"XME"
minerals_detail <- c("Gold", "Silver", "Platinum", "Copper", "Industrial metals") #"US metals and mining"
agriculture_symbs <- c("JJG", "BAL", "SOYB", "SGG", "JO", "NIB", "DBA")
agriculture_detail <- c("Grains", "Cotton", "Soybean", "Sugar", "Coffee", "Cacao", "General agriculture")
energy_symbs <- c("WTI", "FUE", "WOOD", "ICLN", "KOL", "UNG")
energy_detail <- c("Oil (W&T Offshore Inc.)", "Biofuels", "Timber", "Clean energy", "Coal", "US natural gas")
#currency_symbs <- c("EMLC", "UUP", "FXE", "FXY", "FXF", "FXC", "FXB", "FXA")
#currency_detail <- c("Emerging mkt currencies", "USD", "EUR", "JPY", "CHF", "CND", "GBP", "AUD")
currency_symbs <- c("EURUSD=X", "JPY=X", "CHF=X", "CAD=X",
                    "GBPUSD=X", "AUDUSD=X", "INR=X")
currency_detail <- c("EUR/USD", "USD/JPY",
                     "USD/CHF", "USD/CAD", "GBP/USD", "AUD/USD", "USD/INR")
emerg_mkt_symbs <- c("EMLC", "ELD", "BKF", "VWOB")
emerg_mkt_detail <- c("Emerg mkts debt", "Emerg mkts gov. bonds", "BRIC countries", "Emerging mkt currencies")
crypto_symbs <- c("BLOK", "LEGR", "BCNA")
crypto_detail <- c("Blockchain tech.", "Blockchain companies", "Blockchain in China")
Tbond_symbs <- c("IEI", "IEF", "TLT")#, "BIL"
Tbond_detail <- c("T-bond 3-7 yrs", "T-bond 7-10 yrs", "T-bond 20+ yrs") #"T-bond 1-3 months"

ts_symb_vec <- c(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
                 currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
ts_detail_vec <- c(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                   currency_detail, emerg_mkt_detail, crypto_detail, Tbond_detail)
df_symb_detail <- data.frame(symbol = ts_symb_vec, detail = ts_detail_vec)
#--
length(ts_symb_vec)
#--
# list_groups <- list(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
#                      currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
list_groups <- list(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                    currency_detail, emerg_mkt_detail, crypto_detail, Tbond_detail)
group_names <- c("US Sectors", "Minerals", "Agriculture", "Energy", "Major Currency Pairs",
                 "Emerging Markets", "Cryptocurrencies/Blockchain", "T-Bonds")
#-------------------------------------------
#per_ema <- 13
#fromdate = Sys.Date() - 360
fromdate = "2018-06-20"
tbl_ohlcv <- tq_get(ts_symb_vec, get = "stock.prices", from = fromdate)
df_ohlcv <- as.data.frame(tbl_ohlcv)
#--
# Crypto ts can have duplicate date entries sometimes (BTC-USD)
# Get rid of these duplicates, if any
df_ohlcv <- as.data.frame(df_ohlcv %>% group_by(symbol) %>% mutate(dup = duplicated(date)))
df_ohlcv <- subset(df_ohlcv, dup == F)
df_ohlcv$dup <- NULL
#--

#df_ohlcv$p <- rowSums(df_ohlcv[, c(4:6)]) / 3
#--
#df <- df_ohlcv[, c("date", "symbol", "p")]
# df <- df_ohlcv %>% group_by(symbol) %>% tq_transmute(select = adjusted, 
#                      mutate_fun = periodReturn, 
#                      period     = "monthly")
#--
df <- df_ohlcv[, c("date", "symbol", "adjusted")]
df <- df %>% spread(symbol, adjusted)
ind_shift <- which(colnames(df) %in% currency_symbs)
df[, ind_shift] <- rbind(rep(NA, length(ind_shift)), df[-nrow(df), ind_shift])
df <- df[-c(1, nrow(df)), ]
date_vec <- df$date
mat_ts_dy <- na.approx(df[, -1])

#o <- apply(mat_ts_dy, 2, function(x) length(which(is.na(x))))
#table(o)
#which(o==1)
#xts_ts_dy <- xts(mat_ts_dy, date_vec)
#date_vec <- index(xts_ts_dy)
#-----
mat_pctDiff_dy <- diff(mat_ts_dy) / mat_ts_dy[-nrow(mat_ts_dy), ]
#mat_pctDiff_dy <- diff(log(mat_ts_dy))
row.names(mat_pctDiff_dy) <- as.character(date_vec[-1])
ts_avg_dy <- rowMeans(mat_pctDiff_dy)
mu_ret_dy <- colMeans(mat_pctDiff_dy)
sd_ret_dy <- apply(mat_pctDiff_dy, 2, sd)

mat_ts_wk <- to.weekly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_wk <- diff(mat_ts_wk) / mat_ts_wk[-nrow(mat_ts_wk), ]
mat_pctDiff_wk <- as.matrix(mat_pctDiff_wk[-1, ])
ts_avg_wk <- rowMeans(mat_pctDiff_wk)
mu_ret_wk <- colMeans(mat_pctDiff_wk)
sd_ret_wk <- apply(mat_pctDiff_wk, 2, sd)

mat_ts_mo <- to.monthly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_mo <- diff(mat_ts_mo) / mat_ts_mo[-nrow(mat_ts_mo), ]
mat_pctDiff_mo <- as.matrix(mat_pctDiff_mo[-1, ])
ts_avg_mo <- rowMeans(mat_pctDiff_mo)
mu_ret_mo <- colMeans(mat_pctDiff_mo)
sd_ret_mo <- apply(mat_pctDiff_mo, 2, sd)
#----------------------------------------------
# Compare histograms of daily, weekly, and monthly returns
# df_plot <- data.frame(ts_id = names(mu_ret_dy), mu_ret_dy, mu_ret_wk, mu_ret_mo)
# colnames(df_plot)[-1] <- c("Mean Daily Return", "Mean Weekly Return", "Mean Monthly Return")
# gathercols <- colnames(df_plot)[-1]
# df_plot <- df_plot %>% gather_("Return Type", "Value", gathercols)
# #gg <- ggplot(df_plot, aes(Value, fill = `Return Type`))
# gg <- ggplot(df_plot, aes(Value))
# gg <- gg + geom_density(alpha = 0.4)
# gg <- gg + facet_wrap(~`Return Type`, nrow = 1, scales = "free")
# #gg <- gg + coord_cartesian(xlim = c(-0.02, 0.02))
# gg
#----------------------------------------------
#ind_rm_ema <- 1:(per_ema - 1)
# mat_PCA_in <- apply(mat_PCA_in, 2, function(x) x - EMA(x, per_ema))
# mat_PCA_in <- mat_PCA_in[-ind_rm_ema, ]
# date_vec <- df$date[-c(ind_rm_ema, ind_rm_na)]
#----------------------------------------------
mat_ts_in <- mat_ts_dy
ts_avg_in <- ts_avg_dy
mat_pctDiff_in <- mat_pctDiff_dy
# nab_mu_ret_in <- mu_ret_dy
# sd_ret_in <- sd_ret_dy
#----------------------------------------------
mat_PCA_in <- mat_pctDiff_in
date_vec <- row.names(mat_PCA_in)
#length(date_vec)
row.names(mat_PCA_in) <- as.character(date_vec)
colnames(mat_PCA_in) <- as.character(df_symb_detail$detail[order(df_symb_detail$symbol)])
check_on_data <- F
if(check_on_data){
  df_plot <- as.data.frame(mat_PCA_in)
  gathercols <- colnames(df_plot)
  df_plot$Date <- date_vec
  df_plot <- df_plot %>% gather_("Security", "Weekly change (%)", gathercols)
  df_plot$Type <- NA
  df_plot$Type[which(df_plot$Security %in% spy_sector_detail)] <- "US Sectors"
  df_plot$Type[which(df_plot$Security %in% minerals_detail)] <- "Minerals"
  df_plot$Type[which(df_plot$Security %in% agriculture_detail)] <- "Agriculture"
  df_plot$Type[which(df_plot$Security %in% energy_detail)] <- "Energy"
  df_plot$Type[which(df_plot$Security %in% currency_detail)] <- "Major Currency Pairs"
  df_plot$Type[which(df_plot$Security %in% emerg_mkt_detail)] <- "Emerging Markets"
  df_plot$Type[which(df_plot$Security %in% crypto_detail)] <- "Cryptocurrencies/Blockchain"
  df_plot$Type[which(df_plot$Security %in% Tbond_detail)] <- "T-bonds"
  #------------------------------------------------------------
  # gg <- ggplot(df_plot, aes(x = Year, y = Value, group = Security, color = Security))
  # gg <- gg + geom_line()
  # gg <- gg + facet_wrap(~ Type, nrow = 3, scales = "free")
  # gg <- gg + theme(legend.position = "none")
  # gg
  #------------------------------------------------------------
  this_type <- "US Sectors"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Minerals"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Agriculture"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Energy"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Major Currency Pairs"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Emerging Markets"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Cryptocurrencies/Blockchain"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "T-bonds"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
}

```


## 4. Application to real world data

### 4.1 An example from the financial context

#### 4.1.1 Extraction of signals from noise

In figure ..., a density plot of the correlation matrix eigenvalues is compared against one of eigenvalues derived from a random matrix. In this plot, it is evident that most eigenvalues are small and cannot be distinguished from noise, but a few extend beyond the random matrix eigenvalue density plot. These correspond to the eigenvectors that can be meaningfully distinguished from noise.

The signal time series are plotted against the data average in figure .... Here it is evident that the first signal reflects the average, while the subsequent signals reflect forces pushing and pulling on the average.

```{r, fig.width=12, fig.height=8, fig.align='center', echo=FALSE}
ind_train <- 1:round(nrow(mat_PCA_in) * 2 / 3)
ind_test <- setdiff(1:nrow(mat_PCA_in), ind_train)
mat_pctDiff_train <- mat_PCA_in[ind_train, ]
mat_pctDiff_test <- mat_PCA_in[ind_test, ]
ts_avg_test <- ts_avg_in[ind_test]
date_vec_test <- date_vec[ind_test]
list_out <- signals_from_noise(mat_pctDiff_train,
                               eigenvalue_density_plot = T,
                               signals_plot = T,
                               pca_var_plot = F,
                               pca_ind_plot = F,
                               list_groups = list_groups,
                               group_names = group_names,
                               quietly = F)
# list_out <- list(mat_loads_sig, mat_loads_rot_sig, mat_loads, mat_loads_rot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
mat_loads_sig_train <- list_out[[1]]
mat_loads_rot_sig_train <- list_out[[2]]
# mat_loads_train <- list_out[[3]]
# mat_laods_rot_train <- list_out[[4]]
mat_pctDiff_sig_train <- list_out[[5]]
eigvals_sig_train <- list_out[[6]]
mat_eigvecs_sig_train <- list_out[[7]]
eigvals_train <- list_out[[8]]
mat_eigvecs_train <- list_out[[9]]
```


#### 4.1.2 Interpretation and characterization of the signals

Average, mostly US sectors and.... Signal two 

```{r, fig.width=15, fig.height=12, fig.align='center', echo=FALSE}

signal_names <- c("US / Emerg. Markets,\nEnergy, Blockchain", "Precious metals", "FOREX", "US Bonds", "Agriculture", "Industrial metals")
interpret_loadings(mat_loads_rot_sig_train,
                   list_groups = list_groups,
                   group_names = group_names,
                   signal_names = signal_names)

# ind_train <- 1:round(nrow(mat_PCA_in) * 2 / 3)
# ind_test <- setdiff(1:nrow(mat_PCA_in), ind_train)
# mat_pctDiff_train <- mat_PCA_in[ind_train, ]
# mat_pctDiff_test <- mat_PCA_in[ind_test, ]
# ts_avg_test <- ts_avg_in[ind_test]
# date_vec_test <- date_vec[ind_test]
# list_sigs <- Signals_from_noise(mat_pctDiff_train, varimax_rot = T,
#                                 list_groups = list_groups,
#                                 group_names = group_names,
#                                 pca_var_plot = F,
#                                 pca_ind_plot = F,
#                                 eigenvalue_density_plot = T,
#                                 loadings_plot = F,
#                                 sig_ts_plot = F,
#                                 sig_corr_plot = F,
#                                 yearly = F)

```


#### 4.1.3 Portfolio optimization

The train data is 2/3 the whole data. The expected returns are calculated over a period equal in length to that of the test data.


```{r, fig.width=15, fig.height=12, fig.align='center', echo=FALSE}

# Get backtest data
list_out <- signals_from_noise(mat_PCA_in,
                               eigenvalue_density_plot = F,
                               signals_plot = F,
                               pca_var_plot = F,
                               pca_ind_plot = F,
                               list_groups = list_groups,
                               group_names = group_names,
                               quietly = T)
mat_loads_sig_test <- list_out[[1]]
mat_loads_rot_sig_test <- list_out[[2]]
mat_pctDiff_sig_test <- list_out[[5]][ind_test, ]
# mat_pctDiff_sig_test <- mat_pctDiff_test %*% mat_loads_rot_sig_test
# mat_loads_rot_sig_normd <- apply(mat_loads_rot_sig_test, 2, function(x) x / sum(x))
# mat_eigenportfolio_test <- mat_pctDiff_test %*% mat_loads_rot_sig_normd


```


```{r, fig.width=8, fig.height=6, fig.align='center', echo = FALSE}
#=======================================================================
# Define portfolio optimization function
optimize_portfolio <- function(cormat, mat_nab, targ_vec,
                               mat_pctDiff_backtest = NULL,
                               utility_interpretation = F){
  cormat_inv <- solve(cormat)
  M <- t(mat_nab) %*% cormat_inv %*% mat_nab
  M_inv <- solve(M)
  x <- -2 * M_inv %*% targ_vec
  # Risk shadow price
  l_V <- 1 / x[1]
  # Budget shadow price (l_C = lambdas[2], l_R normalized to = 1)
  lambdas <- l_V * x
  # Optimal budget weights
  wStar <- -1 / (2 * l_V) * cormat_inv %*% mat_nab %*% lambdas
  # sum(wStar)
  Rtarg <- targ_vec[1]
  # Portfolio variance
  V <- t(wStar) %*% cormat %*% wStar
  #----------------------------------------------------
  # Utility function interpretation of equations
  # (Makes all budget weights positive)
  if(utility_interpretation){
    leverage_factor <- sum(abs(wStar))
    #wStar <- wStar / leverage_factor
    Exp_wStar <- exp(-10 * wStar)
    K <- sum(Exp_wStar)
    #wStar <- -Exp_wStar / K * leverage_factor
    wStar <- -Exp_wStar / K
  } 
  #----------------------------------------------------
  # Backtest
  if(!is.null(mat_pctDiff_backtest)){
    # Straight returns
    # portfolio_pctDiff_t <- mat_pctDiff_backtest %*% wStar
    # backtest_pctRet <- prod(1 + portfolio_pctDiff_t) - 1
    backtest_pctRet <- (apply(mat_pctDiff_backtest, 2, function(x) prod(1 + x)) - 1) %*% wStar
    backtest_V <- t(wStar) %*% cor(mat_pctDiff_backtest) %*% wStar
    # Performance (returns compared to a benchmark portfolio (the 1/n portfolio for eg.))
    n_items <- ncol(cormat)
    wBench <- rep(1 / n_items, n_items)
    portfolio_benchmark_pctDiff_t <- mat_pctDiff_backtest %*% wBench
    backtest_pctRet_benchmark <- prod(1 + portfolio_benchmark_pctDiff_t) - 1
    backtest_V_benchmark <- t(wBench) %*% cor(mat_pctDiff_backtest) %*% wBench
    performance <- backtest_pctRet - backtest_pctRet_benchmark
  }else{
    #portfolio_pctDiff_t <- NULL
    backtest_pctRet <- NULL
    backtest_V <- NULL
    backtest_pctRet_benchmark <- NULL
    backtest_V_benchmark <- NULL
    performance <- NULL
  }
  #----------------------------------------------------
  list_out <- list(wStar, lambdas, V, l_V, backtest_pctRet, backtest_V, backtest_pctRet_benchmark, backtest_V_benchmark, Rtarg)
  return(list_out)
}



#=======================================================================
plot_frontier <- function(df_frontier,
                          fig_num = NULL,
                          remove_xAxis = F,
                          include_backtest_frontier = F){
  #-------------------------------------------
  n_points_on_frontier <- nrow(df_frontier)
  #-------------------------------------------
  str_title <- "Optimal Portfolio Frontier"
  if(!is.null(fig_num)){
    str_title <- paste(paste("Figure", fig_num), str_title)
    }
  
  if(!include_backtest_frontier){
    df_plot <- df_frontier[, c("Risk (variance)", "Return target")]
    gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = `Return target`))
    gg <- gg + labs(title = str_title)
    if(remove_xAxis){
      gg <- gg + theme(axis.title.x = element_blank(),
                       axis.text.x = element_blank())
    }
    gg <- gg + geom_point()
    gg_frontier <- gg
    print(gg_frontier)
  }else{
    df_plot1 <- df_frontier[, c("Risk (variance)", "Return target")]
    df_plot2 <- df_frontier[, c("Risk backtest", "Return backtest")]
    df_plot1$Type <- "Optimized Portfolio Target"
    df_plot2$Type <- "Optimized Portfolio Backtest"
    colnames(df_plot1)[2] <- "Return"
    colnames(df_plot2)[1:2] <- c("Risk (variance)", "Return")
    df_plot <- do.call(rbind, list(df_plot1, df_plot2))
    gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = `Return`, group = Type, color = Type))
    gg <- gg + geom_point()
    gg <- gg + labs(title = str_title)
    gg <- gg + theme(legend.title = element_blank())
    if(remove_xAxis){
      gg <- gg + theme(axis.title.x = element_blank(),
                       axis.text.x = element_blank())
    }
    gg_frontier_wBacktest <- gg
    print(gg_frontier_wBacktest)
  }
  
}


#=======================================================================
plot_frontier_and_budget <- function(df_frontier, df_w_plot,
                                     n_points_on_frontier,
                                     varNames_ordered,
                                     fig_num = NULL,
                                     list_groups = NULL,
                                     group_names = NULL){
  #-------------------------------------------
  # Frontier plot
  str_title <- "Optimal Portfolio Frontier"
  if(!is.null(fig_num)){
    str_title <- paste(paste("Figure", fig_num), str_title)
    }
    df_plot <- df_frontier[, c("Risk (variance)", "Return target")]
    gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = `Return target`))
    gg <- gg + labs(title = str_title)
    if(remove_xAxis){
      gg <- gg + theme(axis.title.x = element_blank(),
                       axis.text.x = element_blank())
    }
    gg <- gg + geom_point()
    gg_frontier <- gg
  #-------------------------------------------
  # Budget weights plot
  # Note df_w_in should have just the target risk column (i.e. don't also include the backtest risk column). At any rate, the budget risk column should be the same as the one used in the frontier plot.
    df_plot <- df_w_plot
    gathercols <- colnames(df_plot)
    df_plot$portfolio_id <- 1:n_points_on_frontier
    df_match_V <- df_plot[, c("portfolio_id", "Risk (variance)")]
    df_plot <- df_plot %>% gather_("Item", "Budget weights", gathercols)
    if(!is.null(list_groups)){
      n_var <- nrow(mat_nab)
      group_vec <- rep(NA, n_var)
      for(i in 1:length(list_groups)){
        this_group_vec <- list_groups[[i]]
        this_group_name <- group_names[i]
        group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
      }
      df_match_group <- data.frame(Item = varNames_ordered, Type = group_vec)
      df_plot <- merge(df_plot, df_match_group, by = "Item")
      df_plot <- df_plot %>% group_by(portfolio_id, Type) %>% summarise(`Budget weights` = sum(`Budget weights`))
      df_plot <- merge(df_plot, df_match_V, by = "portfolio_id")
      colnames(df_plot)[2] <- "Item"
    }
    df_plot <- df_plot %>% group_by(Item) %>% mutate(mu = mean(`Budget weights`)) %>% as.data.frame(df_plot)
    df_plot$Item <- factor(df_plot$Item,
                           levels = unique(df_plot$Item[order(df_plot$mu, df_plot$Item, decreasing = T)]),
                           ordered = T)
    #df_plot <- arrange(df_plot, Item, `Risk (variance)`)
    gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = `Budget weights`, fill = Item))
    gg <- gg + geom_area(position = "stack")
    gg <- gg + theme(legend.title = element_blank())
    if(length(unique(df_plot$Item)) > 15){gg <- gg + theme(legend.position = "none")}
    gg_weights <- gg
    #-------------------------------------------
    gg_together <- gg_frontier + gg_weights + plot_layout(ncol = 1)
    print(gg_together)
  
}


#=======================================================================
get_optimal_frontier <- function(cormat, mat_nab,
                                 Rtarg_limits = c(0.001, 0.3),
                                 n_points_on_frontier = 50,
                                 mat_pctDiff_backtest,
                                 utility_interpretation = F,
                                 frontier_and_budget_plot = F,
                                 list_groups = NULL,
                                 group_names = NULL,
                                 fig_num = NULL
){
  #-------------------------------------------
  Rtarg_vec <- seq(Rtarg_limits[1], Rtarg_limits[2], length.out = n_points_on_frontier)
  list_wStar <- list()
  lC_vec <- c()
  V_vec <- c()
  lV_vec <- c()
  backtest_pctRet_vec <- c()
  backtest_V_vec <- c()
  backtest_pctRet_benchmark_vec <- c()
  backtest_V_benchmark_vec <- c()
  Rtarg_out_vec <- c()
  #-------------------------------------------
  for(i in 1:length(Rtarg_vec)){
    this_Rtarg <- Rtarg_vec[i]
    targ_vec <- c(this_Rtarg, C_targ)
    list_out <- optimize_portfolio(cormat, mat_nab, targ_vec,
                                   mat_pctDiff_backtest,
                                   utility_interpretation)
    #      list_out <- list(wStar, lambdas, V, l_V, backtest_pctRet, backtest_V, backtest_pctRet_benchmark, backtest_V_benchmark, portfolio_pctDiff_t, Rtarg)
    
    list_wStar[[i]] <- list_out[[1]]
    lambdas <- list_out[[2]]
    lC_vec[i] <- lambdas[2]
    V_vec[i] <- list_out[[3]]
    lV_vec[i] <- list_out[[4]]
    backtest_pctRet_vec[i] <- list_out[[5]]
    backtest_V_vec[i] <- list_out[[6]]
    backtest_pctRet_benchmark_vec[i] <- list_out[[7]]
    backtest_V_benchmark_vec[i] <- list_out[[8]]
    Rtarg_out_vec[i] <- list_out[[9]]
    
  }
  #-------------------------------------------
  varNames_ordered <- row.names(mat_nab)
  #-------------------------------------------
  if(is.null(mat_pctDiff_backtest)){
    df_frontier <- data.frame(R_targ = Rtarg_out_vec,
                              V_targ = V_vec,
                              lV = lV_vec, lC = lC_vec)
    colnames(df_frontier) <- c("Return target",
                               "Risk (variance)",
                               "Risk shadow price",
                               "Budget shadow price")
    df_w <- data.frame(df_frontier$`Risk (variance)`, t(do.call(cbind, list_wStar)))
    colnames(df_w) <- c("Risk (variance)", varNames_ordered)
    
  }else{
    df_frontier <- data.frame(R_targ = Rtarg_out_vec,
                              R_backtest = backtest_pctRet_vec,
                              R_backtest_benchmark = backtest_pctRet_benchmark_vec,
                              V_targ = V_vec,
                              V_backtest = backtest_V_vec,
                              V_backtest_benchmark = backtest_V_benchmark_vec,
                              lV = lV_vec, lC = lC_vec)
    colnames(df_frontier) <- c("Return target", "Return backtest",
                               "Return backtest 1/n portfolio",
                               "Risk (variance)", "Risk backtest",
                               "Risk backtest 1/n portfolio",
                               "Risk shadow price",
                               "Budget shadow price")
    df_w <- data.frame(df_frontier$`Risk (variance)`,
                       df_frontier$`Risk backtest`,
                       t(do.call(cbind, list_wStar)))
    colnames(df_w) <- c("Risk (variance)", "Risk (variance) backtest", varNames_ordered)
    
  }
  #--------------------------------------
  if(frontier_and_budget_plot){
      if(!is.null(mat_pctDiff_backtest)){df_w_plot <- df_w[-2]}else{df_w_plot <- df_w}
  plot_frontier_and_budget(df_frontier, df_w_plot,
                           n_points_on_frontier = n_points_on_frontier,
                           varNames_ordered = varNames_ordered,
                           fig_num = fig_num,
                           list_groups = NULL,
                           group_names = NULL)
  }
  #--------------------------------------
  list_out <- list(df_frontier, df_w)
  return(list_out)
}
#=======================================================================
# Conventional risk-reward frontier
n_vars <- nrow(mat_loads_sig_train)
C_targ <- 1
nab_C <- rep(1, n_vars)
#------------------------------------
# Correlation matrix
cormat <- cor(mat_pctDiff_train)
#cormat <- round(mat_loads_rot_sig_train %*% t(mat_loads_rot_sig_train), 7)
#cormat <- round(mat_loads_sig_train %*% t(mat_loads_sig_train), 7)
# mse <- mean((cor(mat_pctDiff_train) - cormat)^2)
# mse
#------------------------------------
# Expected returns vector
ind_t_ret <- (nrow(mat_pctDiff_train) - length(ind_test)):nrow(mat_pctDiff_train)
nab_mu_ret_train <- apply(mat_pctDiff_train[ind_t_ret, ], 2, function(x) prod(1 + x)) - 1
#nab_mu_ret_train <- 1 + colMeans(mat_pctDiff_train)
#nab_mu_ret_train <- 1 + colMeans(mat_pctDiff_train_mo)
#------------------------------------
mat_pctDiff_backtest <- mat_pctDiff_test
#------------------------------------
mat_nab <- cbind(nab_mu_ret_train, nab_C)
n_points_on_frontier <- 50
#------------------------------------
list_out <- get_optimal_frontier(cormat, mat_nab,
                                 Rtarg_limits = c(0.01, 0.25),
                                 n_points_on_frontier = n_points_on_frontier,
                                 mat_pctDiff_backtest = mat_pctDiff_backtest,
                                 utility_interpretation = F,
                                 frontier_budget_plot = F,
                                 list_groups = list_groups,
                                 group_names = group_names,
                                 fig_num = )




df_frontier <- list_out[[1]]
df_w <- list_out[[2]]
#------------------------------------


```

Minimum variance analysis is, in many ways, still a work in progress. A backtest of the optimal weights displayed in Figure ... indicates that the real return on investment in these optimal portfolios would have been much less, and the risk much higher, than the efficient fontier suggests.

```{r, fig.width=8, fig.height=4, fig.align='center', echo = FALSE}

  #-------------------------------------------
  # Frontier plot with backtest
  plot_frontier(df_frontier,
                n_points_on_frontier,
                fig_num = NULL,
                include_backtest_frontier = T)
  #-------------------------------------------


```

Recently, the notion of "eigenportfolios" has been explored..., in particular the leading eigenvector, which, as observed above, corresponds to the time series average. The eigenportfolios corresponding to the data are displayed in table ... They are all disastrous portfolios with respect to the efficient frontier.

```{r, echo=F}

# Get eigenportfolios
mat_wEigen <- apply(mat_eigvecs_sig_train, 2, function(x) x / sum(x))
# mat_loads_rot_sig_train_normd <- apply(mat_loads_rot_sig_train, 2, function(x) x / sum(x))
# mat_wEigen <- apply(mat_loads_rot_sig_train_normd, 2, function(x) x / sum(x))
V_eigen <- apply(mat_wEigen, 2, function(x) t(x) %*% cormat %*% x)
mat_pctDiff_eigenportfolio <- mat_pctDiff_train %*% mat_wEigen
R_eigen <- apply(mat_pctDiff_eigenportfolio, 2, function(x) prod(1 + x) - 1)
CV_eigen <- abs(sqrt(V_eigen) / R_eigen)
df_eigenportfolio <- data.frame(R_eigen, V_eigen, CV_eigen)
#kableExtra::kable()
#-----------------------------------------------------------------------
# Get monthly train data in case you want to use mean monthly returns in optimization
# mo_yr_endTrain <- format(as.Date(row.names(mat_pctDiff_dy)[nrow(mat_pctDiff_train)]), "%b %Y")
# ind_train_mo <- 1:which(row.names(mat_pctDiff_mo) == mo_yr_endTrain)
# mat_pctDiff_train_mo <- mat_pctDiff_mo[ind_train_mo, ]



```

#### 4.1.4 Dealing with negative budget weights

One of the main problems limiting the usefulness of MV analysis is that the optimal weights usually include negative values. A negative sign on a budget weight indicates that one should invest in the inverse of the corresponding portfolio item. In the financial context, this is possible through short selling, or through investment in financial products that track the inverse of a given product. However, there remains a more serious problem: a portfolio with both negative and positive weights implies that the investor must spend beyond their budget---i.e., they must borrow---in order to take up the corresponding position on the efficient frontier. Some of the higher return positions on the frontier above require that the investor borrow amounts that are equal to 30%-50% of their budget.

In order to enforce positive weights only when solving for optimal portfolios, the elegance of Merton's approach must be replaced with heuristic methods. An evolutionary algorithm is applied below...


```{r, echo=F}

wLam <- exp(lamW::lambertW0(-nab_mu_ret_train))
wLam <- wLam / sum(wLam)
t(nab_mu_ret_train) %*% wLam
#------------------------------------
# nab_mu_ret_train <<- nab_mu_ret_train
# cormat <<- cormat
portfolio_heuristic_fn <- function(w_vec){
  w_vec <- w_vec / sum(w_vec)
  R <- as.numeric(t(nab_mu_ret_train) %*% w_vec)
  V <- as.numeric(t(w_vec) %*% cormat %*% w_vec)
  obj_fn <- R
  return(obj_fn)
}
env <- environment(fun = portfolio_heuristic_fn)
env[["nab_mu_ret_train"]] <- nab_mu_ret_train
env[["cormat"]] <- cormat
#------------------------------------
out_malschains <- Rmalschains::malschains(function(w_vec) {-portfolio_heuristic_fn(w_vec)},
                                          lower = as.vector(rep(0, n_vars)),
                                          upper = as.vector(rep(1, n_vars)),
                                          verbosity = 0,
                                          env = env)
w_vec <- out_malschains$sol
w_vec <- w_vec / sum(w_vec)
R <- as.numeric(t(nab_mu_ret_train) %*% w_vec)
V <- as.numeric(t(w_vec) %*% cormat %*% w_vec)
R
V

# nab_mu_ret_test <- apply(mat_pctDiff_backtest, 2, function(x) prod(1 + x)) - 1
# R_btest <- t(w_vec) %*% nab_mu_ret_test
R_btest <- prod(1 + mat_pctDiff_backtest %*% w_vec) - 1
V_btest <- t(w_vec) %*% cor(mat_pctDiff_backtest) %*% w_vec
R_btest
V_btest


out_GenSA <- GenSA::GenSA(par = NULL, function(w_vec, nab_mu_ret_train, cormat) {-portfolio_heuristic_fn(w_vec, nab_mu_ret_train, cormat)},
                          lower = as.vector(rep(0, n_vars)),
                          upper = (rep(0.9, n_vars)),
                          control=list(max.call = 1e5),
                          cormat, nab_mu_ret_train)

w_vec <- out_GenSA$par
w_vec <- w_vec / sum(w_vec)
R <- as.numeric(t(nab_mu_ret_train) %*% w_vec)
V <- as.numeric(t(w_vec) %*% cormat %*% w_vec)
R
V
#------------------------------------


# n_signals <- ncol(mat_eigvecs_sig_train)
# mat_eigvecs_sig_train_aug <- cbind(mat_eigvecs_sig_train, nab_C)
# Q <- t(mat_eigvecs_sig_train_aug) %*% mat_eigvecs_train
# # M <- round(Q %*% diag(1 / eigvals) %*% t(mat_eigvecs) %*% mat_sig_eigvecs_aug, 6)
# M <- round(Q %*% diag(1 / eigvals_train) %*% t(Q), 7)
# M
# M_inv <- round(solve(M), 5)
# M_inv
# 
# targ_vec <-  c(0.2 * rep(1, n_signals), 1)
# #targ_vec <- c(0.06, 0.06, 0.02, 0.02, 1)
# x <- -2 * M_inv %*% targ_vec
# l_V <- 1 / x[1]
# lambdas <- l_V * x
# lambdas
# l_C <- lambdas[length(lambdas)]
# 
# wStar <- -1 / (2 * l_V) * mat_eigvecs_train %*% diag(1 / eigvals_train) %*% t(mat_eigvecs_train) %*% mat_eigvecs_sig_train_aug %*% lambdas
# pctDiff_wStar <- mat_pctDiff_test %*% wStar
# 
# backtest_ret <- prod(1 + pctDiff_wStar) - 1
# #sd(pctDiff_wStar)
# sum(wStar)
# 
# t(wStar) %*% cormat %*% wStar




```





```{r, echo=F}

#=======================================================================
# Signals correlation matrix risk-reward frontier
n_signals <- ncol(mat_loads_sig_train)
nab_C <- rep(1, n_signals)
#------------------------------------
# Correlation matrix
cormat <- round(cor(mat_pctDiff_sig_train), 7)
#------------------------------------
# Expected returns vector
nab_mu_ret_train <- apply(mat_pctDiff_sig_train[ind_t_ret, ], 2, function(x) prod(1 + x)) - 1
#nab_mu_ret_train <- mat_eigvecs_sig_train %*% rep(1, n_signals) #------------------------------------
mat_pctDiff_backtest <- mat_pctDiff_sig_test
#------------------------------------
mat_nab <- cbind(nab_mu_ret_train, nab_C)
row.names(mat_nab) <- signal_names
#------------------------------------
list_out <- get_optimal_frontier(cormat, mat_nab,
                                 Rtarg_limits = c(0.01, 0.25),
                                 n_points_on_frontier = 50,
                                 mat_pctDiff_backtest,
                                 utility_interpretation = F,
                                 frontier_budget_plot = T,
                                 budget_weights_plot = T,
                                 list_groups = NULL,
                                 group_names = NULL)

#df_w_look <- list_out[[2]]
#------------------------------------



```








## Example 2: Mod-MV analysis to determine optimal investments across strategic objectives in the development context




```{r, echo=FALSE}
#setwd("D:/OneDrive - CGIAR/Documents")
#-------------------------------------------------------------
remove_countries <- c("So Tom and Principe", "Micronesia, Fed. Sts.")
WDI_country_classification <- read.csv("WDICountry.csv", stringsAsFactors = F)
Low_income_countries <- unique(WDI_country_classification$Table.Name[which(WDI_country_classification$Income.Group == "Low income")])

Lower_middle_income_countries <- unique(WDI_country_classification$Table.Name[which(WDI_country_classification$Income.Group == "Lower middle income")])
Upper_middle_income_countries <- unique(WDI_country_classification$Table.Name[which(WDI_country_classification$Income.Group == "Upper middle income")])

WDI_raw <- read.csv("WDIData.csv", stringsAsFactors = F)
WDI_raw$Country.Code <- NULL
WDI_raw$Indicator.Code <- NULL
WDI_raw$X <- NULL
colnames(WDI_raw)[1:2] <- c("Country", "Indicator")
WDI_raw$Country <- as.character(WDI_raw$Country)
WDI_raw$Indicator <- as.character(WDI_raw$Indicator)
#unique(WDI_raw$Country)
#unique(WDI_raw$Indicator)
unique(WDI_raw$Indicator[grep("basic", WDI_raw$Indicator, ignore.case = T)])
colnames(WDI_raw)[3:ncol(WDI_raw)] <- as.character(c(1960:2018))
WDI_long <- WDI_raw %>% gather(Year, Value, `1960`:`2018`)
#unique(WDI_raw$Country)
WDI_long <- as.data.frame(WDI_long %>% group_by(Year, Indicator) %>% mutate(world_mu = mean(Value, na.rm = T)))
ind_worldGini <- which(WDI_long$Country == "World" & WDI_long$Indicator == "GINI index (World Bank estimate)")
WDI_long$Value[ind_worldGini] <- WDI_long$world_mu[ind_worldGini]
ind_worldIncomeBottom <- which(WDI_long$Country == "World" & WDI_long$Indicator == "Income share held by lowest 20%")
WDI_long$Value[ind_worldIncomeBottom] <- WDI_long$world_mu[ind_worldIncomeBottom]
WDI_long$world_mu <- NULL
WDI_long <- as.data.frame(WDI_long %>% group_by(Year, Indicator) %>% mutate(world_sum = sum(Value, na.rm = T)))
ind_worldBatDeaths <- which(WDI_long$Country == "World" & WDI_long$Indicator == "Battle-related deaths (number of people)")
WDI_long$Value[ind_worldBatDeaths] <- WDI_long$world_sum[ind_worldBatDeaths]
WDI_long$world_sum <- NULL
WDI_long$Value[which(is.nan(WDI_long$Value))] <- NA
#unique(WDI_long$Indicator)
#unique(WDI_raw$Indicator)[grep("Average", unique(WDI_raw$Indicator))]
#-------------------------------------------------------------

```



```{r, fig.width=10, fig.height=4, fig.align='center', echo = FALSE}

environmental_indicators <- c("CO2 emissions (metric tons per capita)", 
                              #"Combustible renewables and waste (% of total energy)",
                              "CO2 intensity (kg per kg of oil equivalent energy use)",
                              "Forest area (% of land area)",
                              "GDP per unit of energy use (PPP $ per kg of oil equivalent)",
                              "Energy intensity level of primary energy (MJ/$2011 PPP GDP)",
                              "Renewable energy consumption (% of total final energy consumption)",
                              "Agricultural methane emissions (thousand metric tons of CO2 equivalent)",
                              "Agricultural nitrous oxide emissions (thousand metric tons of CO2 equivalent)",
                              "Methane emissions in energy sector (thousand metric tons of CO2 equivalent)",
                              "Nitrous oxide emissions in energy sector (thousand metric tons of CO2 equivalent)",
                              "Fertilizer consumption (kilograms per hectare of arable land)",
                              "Fertilizer consumption (% of fertilizer production)",
                              "Adjusted savings: net forest depletion (% of GNI)",
                              "Adjusted savings: mineral depletion (% of GNI)",
                              "Adjusted savings: energy depletion (% of GNI)",
                              "Level of water stress: freshwater withdrawal as a proportion of available freshwater resources",
                              #"Access to clean fuels and technologies for cooking (% of population)",
                              "Renewable internal freshwater resources per capita (cubic meters)"
                              #"Water productivity, total (constant 2010 US$ GDP per cubic meter of total freshwater withdrawal)"
)
#-------------------------------------------------------------
food_sec_indicators <- c("Agricultural land (% of land area)",
                         "Land under cereal production (hectares)",
                         "Prevalence of undernourishment (% of population)",
                         "Food imports (% of merchandise imports)",
                         "Agriculture, forestry, and fishing, value added per worker (constant 2010 US$)",
                         "Agriculture, value added per worker (constant 2010 US$)",
                         "Agriculture, forestry, and fishing, value added (% of GDP)",
                         #"Agriculture, forestry, and fishing, value added (annual % growth)",
                         "Food, beverages and tobacco (% of value added in manufacturing)"
)

economic_growth_indicators <- c(
  "Oil rents (% of GDP)",
  "Coal rents (% of GDP)",
  "Forest rents (% of GDP)",
  "Natural gas rents (% of GDP)",
  #"Total natural resources rents (% of GDP)",
  "Mineral rents (% of GDP)",
  #"Foreign direct investment, net (BoP, current US$)",
  "Foreign direct investment, net inflows (BoP, current US$)",
  #"Foreign direct investment, net inflows (% of GDP)",
  #"Export value index (2000 = 100)",
  #"Import value index (2000 = 100)",
  "GDP per capita (current US$)",
  "Net ODA received per capita (current US$)",
  "Trade (% of GDP)",
  #"GDP growth (annual %)",
  #"GDP per capita growth (annual %)",
  "Industry (including construction), value added (% of GDP)",
  #"Industry (including construction), value added (annual % growth)",
  "Manufacturing, value added (% of GDP)",
  #"Manufacturing, value added (annual % growth)",
  "Machinery and transport equipment (% of value added in manufacturing)",
  "Services, value added (% of GDP)",
  #"Services, value added (annual % growth)",
  "Trade in services (% of GDP)",
  #"Exports of goods and services (annual % growth)",
  #"Imports of goods and services (annual % growth)",
  #"Medium and high-tech Industry (including construction) (% manufacturing value added)",
  #"Gross capital formation (annual % growth)",
  #"Gross fixed capital formation (annual % growth)",
  #"Final consumption expenditure (annual % growth)",
  "Final consumption expenditure (constant 2010 US$)",
  "Employment in agriculture (% of total employment) (modeled ILO estimate)",
  "Employment in industry (% of total employment) (modeled ILO estimate)",
  "Employment in services (% of total employment) (modeled ILO estimate)",
  "Labor force participation rate, total (% of total population ages 15+) (modeled ILO estimate)",
  #"Unemployment, total (% of total labor force) (modeled ILO estimate)",
  "Industry, value added per worker (constant 2010 US$)",
  "Industry (including construction), value added per worker (constant 2010 US$)",
  "Services, value added per worker (constant 2010 US$)"
)
#-------------------------------------------------------------
peaceHRights_indicators <- c("Military expenditure (% of GDP)",
                             "Ratio of female to male labor force participation rate (%) (modeled ILO estimate)",
                             "Refugee population by country or territory of origin",
                             "Internally displaced persons, new displacement associated with conflict and violence (number of cases)",
                             "Internally displaced persons, total displaced by conflict and violence (number of people)",
                             "Battle-related deaths (number of people)",
                             "Literacy rate, youth (ages 15-24), gender parity index (GPI)",
                             "School enrollment, primary (gross), gender parity index (GPI)",
                             "School enrollment, primary and secondary (gross), gender parity index (GPI)",
                             "School enrollment, secondary (gross), gender parity index (GPI)",
                             "School enrollment, tertiary (gross), gender parity index (GPI)",
                             "Income share held by lowest 20%"
)
#-------------------------------------------------------------
# economic_equality_indicators <- c("Consumer price index (2010 = 100)",
#                                   "Income share held by lowest 10%",
#                                   "Income share held by lowest 20%",
#                                   #"GINI index (World Bank estimate)",
#                                   "Risk premium on lending (lending rate minus treasury bill rate, %)",
#                                   #"Poverty gap at $1.90 a day (2011 PPP) (%)",
#                                   "Poverty headcount ratio at $1.90 a day (2011 PPP) (% of population)",
#                                   "Number of people pushed below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure",
#                                   "Number of people pushed below the $3.10 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure",
#                                   "Number of people spending more than 10% of household consumption or income on out-of-pocket health care expenditure",
#                                   "Number of people spending more than 25% of household consumption or income on out-of-pocket health care expenditure",
#                                   "Mortality rate attributed to unsafe water, unsafe sanitation and lack of hygiene (per 100,000 population)",
#                                   "People using at least basic sanitation services (% of population)",
#                                   "People using at least basic sanitation services, rural (% of rural population)",
#                                   "People using at least basic sanitation services, urban (% of urban population)",
#                                   "People using safely managed sanitation services (% of population)",
#                                   "People using safely managed sanitation services, rural (% of rural population)",
#                                   "People using safely managed sanitation services, urban (% of urban population)",
#                                   "People using at least basic drinking water services (% of population)",
#                                   "People using at least basic drinking water services, rural (% of rural population)",
#                                   "People using at least basic drinking water services, urban (% of urban population)",
#                                   "Labor force with basic education, female (% of female working-age population with basic education)",
#                                   "Labor force with basic education, male (% of male working-age population with basic education)"
# )
#-------------------------------------------------------------
infrastructure_indicators <- c(#"Access to electricity (% of population)",
  "Access to electricity, urban (% of urban population)",
  "Access to electricity, rural (% of rural population)",
  "Air transport, freight (million ton-km)",
  "Air transport, passengers carried",
  "Railways, goods transported (million ton-km)",
  "Railways, passengers carried (million passenger-km)"
  #"Mobile cellular subscriptions (per 100 people)"
  #"Individuals using the Internet (% of population)"
)
#-------------------------------------------------------------
healthPop_indicators <- c("Mortality rate, under-5 (per 1,000 live births)",
                          #"Incidence of HIV (% of uninfected population ages 15-49)",
                          "Life expectancy at birth, total (years)",
                          # "Life expectancy at birth, male (years)",
                          # "Life expectancy at birth, female (years)"
                          "Prevalence of HIV, total (% of population ages 15-49)",
                          #"Birth rate, crude (per 1,000 people)",
                          #"Death rate, crude (per 1,000 people)",
                          "Urban population (% of total population)",
                          "Rural population (% of total population)",
                          #"Urban population growth (annual %)",
                          #"Population growth (annual %)",
                          #"Population ages 0-14, total",
                          #"Population ages 0-14 (% of total)",
                          #"Population ages 15-64, total",
                          #"Population ages 15-64 (% of total)",
                          #"Population ages 65 and above, total",
                          #"Population ages 65 and above (% of total)",
                          "Population density (people per sq. km of land area)",
                          "Population growth (annual %)",
                          "Physicians (per 1,000 people)",
                          # "Population, total",
                          #"Age dependency ratio, young (% of working-age population)",
                          #"Age dependency ratio, old (% of working-age population)",
                          "Age dependency ratio (% of working-age population)",
                          "Adolescent fertility rate (births per 1,000 women ages 15-19)"
                          
)
#-------------------------------------------------------------
educ_indicators <- c("Government expenditure on education, total (% of GDP)",
                     "School enrollment, primary (% gross)",
                     "School enrollment, secondary (% gross)",
                     "School enrollment, tertiary (% gross)",
                     #"Adjusted savings: education expenditure (current US$)",
                     "Adjusted savings: education expenditure (% of GNI)",
                     "Expenditure on primary education (% of government expenditure on education)",
                     "Expenditure on secondary education (% of government expenditure on education)",
                     "Expenditure on tertiary education (% of government expenditure on education)",
                     "Government expenditure on education, total (% of government expenditure)",
                     "Literacy rate, adult total (% of people ages 15 and above)",
                     "Literacy rate, youth total (% of people ages 15-24)"
)
#-------------------------------------------------------------
indicator_vec <- c(healthPop_indicators, food_sec_indicators, educ_indicators,
                   economic_growth_indicators, #economic_equality_indicators,
                   environmental_indicators, peaceHRights_indicators,
                   infrastructure_indicators)
list_groups <- list(healthPop_indicators, food_sec_indicators, educ_indicators,
                    economic_growth_indicators, #economic_equality_indicators,
                    environmental_indicators, peaceHRights_indicators,
                    infrastructure_indicators)
group_names <- c("Health/Population", "Food Security", "Education",
                 "Economic Growth", #"Economic Equality",
                 "Environmental Sustainability", "Equality, Peace, Human Rights", "Infrastructure")
names(list_groups) <- group_names
#-------------------------------------------------------------
#"Least developed countries: UN classification"
#"Sub-Saharan Africa (excluding high income)"
#"Lower middle income"
these_countries <- "World"
#-------------------------------------------------------------
df_WDI <- subset(WDI_long, Country %in% c(these_countries))
df_WDI <- subset(df_WDI, Indicator %in% indicator_vec)
df_WDI <- df_WDI %>% spread(Indicator, Value)
df_WDI$Year <- as.integer(df_WDI$Year)
df_WDI <- subset(df_WDI, Year >= 1990)
df_WDI <- subset(df_WDI, Year <= 2016)
#df_WDI$`Individuals using the Internet (% of population)`[which(df_WDI$Year < 1994)] <- 0
#-------------------------------------------------------------
year_vec <- df_WDI$Year
df_WDI <- df_WDI[, -c(1, 2)]
out <- apply(df_WDI, 2, function(x) length(which(is.na(x))))
#table(out)
ind_rm <- which(as.numeric(out) > 10)
#length(ind_rm)
df_WDI <- df_WDI[, -ind_rm]
#ncol(df_WDI)
ind_rp <- which(is.na(df_WDI[1, ]))
if(length(ind_rp) != 0){
  for(i in 1:length(ind_rp)){
    df_WDI[1, ind_rp[i]] <- mean(df_WDI[, ind_rp[i]], na.rm = T)
  }
}
ind_rp <- which(is.na(df_WDI[nrow(df_WDI), ]))
for(i in 1:length(ind_rp)){
  df_WDI[nrow(df_WDI), ind_rp[i]] <- mean(df_WDI[, ind_rp[i]], na.rm = T)
}

#df_WDI <- as.data.frame(na.approx(df_WDI[, -c(1, 2)]))
df_WDI <- as.data.frame(na.approx(df_WDI))
#class(df_WDI)
out <- apply(df_WDI, 2, function(x) length(which(is.na(x))))
table(out)
#which(as.numeric(out) > 0)
#colnames(df_WDI)[which(as.numeric(out) > 0)]
#mat_zWDI <- scale(df_WDI)
#mat_PCA_in <- diff(as.matrix(log(df_WDI)))
mat_PCA_in <- diff(as.matrix(df_WDI)) / as.matrix(df_WDI[-nrow(df_WDI), ])
row.names(mat_PCA_in) <- year_vec[-1]
# library(tidyquant)
# ema_per <- 2
# mat_PCA_in <- apply(df_WDI, 2, function(x) x - EMA(x, ema_per))
# row.names(mat_PCA_in) <- year_vec
# mat_PCA_in <- mat_PCA_in[-c(1:(ema_per - 1)), ]
# mat_PCA_in[is.nan(mat_PCA_in)] <- 0
# mat_PCA_in[is.infinite(mat_PCA_in)] <- 0
#-----------------------------------------------------------
df_plot <- as.data.frame(mat_PCA_in)
df_plot$Year <- year_vec[-1]
df_plot <- df_plot %>% gather_("Indicator", "Value", colnames(df_WDI))
df_plot$Type <- NA
df_plot$Type[which(df_plot$Indicator %in% healthPop_indicators)] <- "Health/Population"
df_plot$Type[which(df_plot$Indicator %in% economic_equality_indicators)] <- "Economic Equality"
df_plot$Type[which(df_plot$Indicator %in% educ_indicators)] <- "Education"
df_plot$Type[which(df_plot$Indicator %in% economic_growth_indicators)] <- "Economic Growth"
df_plot$Type[which(df_plot$Indicator %in% environmental_indicators)] <- "Environmental Sustainability"
df_plot$Type[which(df_plot$Indicator %in% food_sec_indicators)] <- "Food Security"
df_plot$Type[which(df_plot$Indicator %in% infrastructure_indicators)] <- "Infrastructure"
df_plot$Type[which(df_plot$Indicator %in% peaceHRights_indicators)] <- "Equality, Peace, Human Rights"
#------------------------------------------------------------
# gg <- ggplot(df_plot, aes(x = Year, y = Value, group = Indicator, color = Indicator))
# gg <- gg + geom_line()
# gg <- gg + facet_wrap(~ Type, nrow = 3, scales = "free")
# gg <- gg + theme(legend.position = "none")
# gg
#------------------------------------------------------------
check_on_data <- F
if(check_on_data){
  this_type <- "Health/Population"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_health <- gg
  gg
  this_type <- "Education"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_educ <- gg
  gg
  this_type <- "Economic Growth"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_econ <- gg
  gg
  # this_type <- "Economic Equality"
  # df_plot2 <- subset(df_plot, Type == this_type)
  # gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  # gg <- gg + geom_line(lwd = 1.1)
  # gg <- gg + geom_point(size = 2)
  # gg <- gg + labs(title = this_type)
  # gg_employ <- gg
  # gg
  this_type <- "Environmental Sustainability"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_environ <- gg
  gg
  this_type <- "Food Security"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_pop <- gg
  gg
  this_type <- "Infrastructure"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_infrastr <- gg
  gg
  this_type <- "Equality, Peace, Human Rights"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Year, y = Value, group = Indicator, color = Indicator))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg_infrastr <- gg
  gg
  
}

```

```{r, fig.width=15, fig.height=12, fig.align='center', echo=FALSE}
#------------------------------------------------------------
list_out <- signals_from_noise(mat_PCA_in,
                               eigenvalue_density_plot = T,
                               signals_plot = T,
                               pca_var_plot = F,
                               pca_ind_plot = F,
                               list_groups = list_groups,
                               group_names = group_names,
                               quietly = F)
#list_out <- list(mat_loads_sig, mat_loads_rot_sig, mat_inData_sig, eigvals_sig, mat_eigvecs_sig)
mat_loads_sig <- list_out[[1]]
mat_loads_rot_sig <- list_out[[2]]


# list_sigs <- Signals_from_noise(mat_PCA_in, varimax_rot = T,
#                                 list_groups = list_groups,
#                                 group_names = group_names,
#                                 pca_var_plot = F,
#                                 pca_ind_plot = F,
#                                 eigenvalue_density_plot = T,
#                                 sig_corr_plot = T,
#                                 yearly = F)

# colMeans(df_sigs[, -1])
# sig_vec <- apply(df_sigs[, -1], 2, sd)
# covmat <- diag(sig_vec) %*% cor(df_sigs[, -1]) %*% diag(sig_vec)
# print(covmat)

```


```{r, fig.width=15, fig.height=12, fig.align='center', echo=FALSE}

signal_names <- c("")
interpret_loadings(mat_loads_rot_sig,
                   list_groups = list_groups,
                   group_names = group_names,
                   signal_names = NULL)


```

```{r, fig.width=12, fig.height=4, fig.align='center', echo=FALSE}


mat_pctDiff_train_sig <- mat_pctDiff_train %*% mat_Loads_rot
#mat_pctDiff_test_sig <- mat_pctDiff_test %*% mat_Loads_rot
#mat_eigenportfolio_test <- mat_pctDiff_test %*% mat_Loads_rot_normd
#mat_pctDiff_sig_in <- mat_pctDiff_in %*% mat_Loads_rot
mat_Loads <- list_sigs[[4]]
mat_Loads_rot <- list_sigs[[5]]
mat_Loads_rot_normd <- apply(mat_Loads_rot, 2, function(x) x / sum(x))
mat_pctDiff_train_sig <- mat_pctDiff_train %*% mat_Loads_rot
#mat_pctDiff_test_sig <- mat_pctDiff_test %*% mat_Loads_rot
#mat_eigenportfolio_test <- mat_pctDiff_test %*% mat_Loads_rot_normd
#mat_pctDiff_sig_in <- mat_pctDiff_in %*% mat_Loads_rot
nab_mu_ret_sigs <- colMeans(mat_pctDiff_train_sig)
sd_ret_sigs <- apply(mat_pctDiff_train_sig, 2, sd)
cv_ret_sigs <- sd_ret_sigs / nab_mu_ret_sigs
sum_ret_sigs <- colSums(mat_pctDiff_train_sig)
nab_mu_ret_sigs
cv_ret_sigs
sum_ret_sigs
#t(mat_Loads) %*% mat_Loads
mse <- mean((cor(mat_pctDiff_train) - mat_Loads %*% t(mat_Loads))^2)
sse <- sum((cor(mat_PCA_in) - mat_Loads %*% t(mat_Loads))^2)
mse
sse

mat_sig_eigvecs <- list_sigs[[2]]
n_sigs <- ncol(mat_sig_eigvecs)
mat_eigvecs <- as.matrix(list_sigs[[3]])
eigvals_sig <- eigvals <- list_sigs[[6]]
eigvals <- list_sigs[[7]]
ones_vec <- rep(1, nrow(mat_eigvecs))
mat_sig_eigvecs_aug <- cbind(mat_sig_eigvecs, ones_vec)

#------------------------------------------------------------
# df_plot1 <- data.frame(Year = as.integer(names(inData_avg)), inData_avg)
# df_plot2 <- data.frame(Year = as.integer(names(inData_avg)), mat_signal_ts)
# signal_id <- paste("Signal", c(1:n_signals))
# colnames(df_plot2)[2:(n_signals + 1)] <- signal_id
# gathercols <- signal_id
# df_plot2 <- df_plot2 %>% gather_("Signal", "Value", gathercols)
# gg <- ggplot()
# gg <- gg + geom_line(data = df_plot1, aes(x = Year, y = inData_avg), color = "orange", lwd = 1.2)
# gg <- gg + geom_line(data = df_plot2, aes(x = Year, y = Value))
# gg <- gg + facet_wrap(~ Signal, nrow = 1)
# gg <- gg + theme(axis.title.y = element_blank())
# gg

```

```{r, echo=FALSE}
#-----------------------------------------------
ret_targ <- 0.005
C_targ <- 1
targ_vec <- c(ret_targ, C_targ)
nab_C <- rep(1, nrow(mat_sig_eigvecs))
nab_C_sigs <- rep(1, n_sigs)
#-----------------------------------------------
cormat <- round(cor(mat_pctDiff_train), 7)
nab_mu_ret_in <- nab_mu_ret
mat_nab <- cbind(nab_mu_ret_in, nab_C)
#mat_nab <- nab_mu_ret_in
list_out <- optimizePortfolio(cormat, mat_nab, targ_vec, mat_pctDiff_test = NULL)
wStar_conv <- list_out[[1]]
lambdas_conv <- list_out[[2]]
#ts_wStar_conv <- list_out[[3]]
lV_conv <- list_out[[5]]
wStar_conv_e <- exp(wStar_conv) / sum(exp(wStar_conv))
# ts_wStar_conv_e <- mat_pctDiff_test %*% wStar_conv_e
# sd_conv_e <- NA#sqrt(t(wStar_conv_e) %*% cormat %*% wStar_conv_e)
#-----------------------------------------------
cormat <- cor(mat_pctDiff_train_sig)
mat_nab <- cbind(nab_mu_ret_sigs, nab_C_sigs)
#mat_nab <- cbind(nab_mu_ret_sigs)
list_out <- optimizePortfolio(cormat, mat_nab, targ_vec, mat_pctDiff_test = NULL)
wStar_sigs <- list_out[[1]]
lambdas_sigs <- list_out[[2]]
ts_wStar_sigs <- list_out[[3]]
wStar_sigs_e <- exp(wStar_sigs) / sum(exp(wStar_sigs))
#ts_wStar_sigs_e <- mat_pctDiff_test_sig %*% wStar_sigs_e
#sd_sigs_e <- NA#sqrt(t(wStar_sigs_e) %*% cormat %*% wStar_sigs_e)
#-----------------------------------------------
```


```{r, echo=F}

R_vec <- c()
V_conv_vec <- c()
lV_conv_vec <- c()
wStar_conv_list <- list()
V_sigs_vec <- c()
lV_sigs_vec <- c()
wStar_sigs_list <- list()
backtest_muRet_conv_vec <- c()
backtest_muRet_sigs_vec <- c()
backtest_sdRet_conv_vec <- c()
backtest_sdRet_sigs_vec <- c()
backtest_muRet_sigs_e_vec <- c()
backtest_sdRet_sigs_e_vec <- c()
wStar_sigs_e_list <- list()
backtest_muRet_conv_e_vec <- c()
backtest_sdRet_conv_e_vec <- c()
wStar_conv_e_list <- list()
Rtarg_vec <- seq(0.001, 0.01, length.out = 50)
for(i in 1:length(Rtarg_vec)){
  this_Rtarg <- Rtarg_vec[i]
  targ_vec <- c(this_Rtarg, C_targ)
  #targ_vec <- c(this_Rtarg)
  #-----------------------------------------------
  cormat <- round(cor(mat_pctDiff_train), 7)
  #cormat <- round(mat_Loads %*% t(mat_Loads), 7)
  mat_nab <- cbind(nab_mu_ret_in, nab_C)
  #mat_nab <- cbind(nab_mu_ret_in)
  list_out <- optimizePortfolio(cormat, mat_nab, targ_vec, mat_pctDiff_test = NULL)
  wStar_conv <- list_out[[1]]
  lambdas_conv <- list_out[[2]]
  #ts_wStar_conv <- list_out[[3]]
  V_conv <- list_out[[4]]
  lV_conv <- list_out[[5]]
  V_conv_vec[i] <- V_conv
  lV_conv_vec[i] <- lV_conv
  wStar_conv_list[[i]] <- wStar_conv
  #backtest_muRet_conv_vec[i] <- mean(ts_wStar_conv)
  #backtest_sdRet_conv_vec[i] <- sd(ts_wStar_conv)
  wStar_conv_e <- exp(wStar_conv) / sum(exp(wStar_conv))
  #ts_wStar_conv_e <- mat_pctDiff_test %*% wStar_conv_e
  wStar_conv_e_list[[i]] <- wStar_conv_e
  #backtest_muRet_conv_e_vec[i] <- mean(ts_wStar_conv_e)
  #backtest_sdRet_conv_e_vec[i] <- sd(ts_wStar_conv_e)
  #-----------------------------------------------
  cormat <- cor(mat_pctDiff_train_sig)
  mat_nab <- cbind(nab_mu_ret_sigs, nab_C_sigs)
  #mat_nab <- cbind(nab_mu_ret_sigs)
  list_out <- optimizePortfolio(cormat, mat_nab, targ_vec, mat_pctDiff_test = NULL)
  wStar_sigs <- list_out[[1]]
  lambdas_sigs <- list_out[[2]]
  #ts_wStar_sigs <- list_out[[3]]
  V_sigs <- list_out[[4]]
  lV_sigs <- list_out[[5]]
  V_sigs_vec[i] <- V_sigs
  lV_sigs_vec[i] <- lV_sigs
  wStar_sigs_list[[i]] <- wStar_sigs
  # backtest_muRet_sigs_vec[i] <- mean(ts_wStar_sigs)
  # backtest_sdRet_sigs_vec[i] <- sd(ts_wStar_sigs)
  wStar_sigs_e <- exp(wStar_sigs) / sum(exp(wStar_sigs))
  # ts_wStar_sigs_e <- mat_pctDiff_test_sig %*% wStar_sigs_e
  wStar_sigs_e_list[[i]] <- wStar_sigs_e
  # backtest_muRet_sigs_e_vec[i] <- mean(ts_wStar_sigs_e)
  # backtest_sdRet_sigs_e_vec[i] <- sd(ts_wStar_sigs_e)
  #-----------------------------------------------
  # print(lambdas_conv[2])
  # print(lambdas_sigs[2])
  # print(lV_conv)
  # print(lV_sigs)
}

df_plot <- data.frame(Return = Rtarg_vec, Conventional = sqrt(V_conv_vec), Experiment = sqrt(V_sigs_vec))
# df_plot <- data.frame(Return = Rtarg_vec, Conventional = backtest_sdRet_conv_e_vec, Experiment = backtest_sdRet_sigs_e_vec)
df_plot <- df_plot %>% gather(Type, `Risk (s.d.)`, Conventional:Experiment)
gg <- ggplot(df_plot, aes(x = `Risk (s.d.)`, y = Return, group = Type, color = Type))
gg <- gg + geom_point()
gg


df_x1 <- data.frame(R = Rtarg_vec,Conventional = backtest_muRet_conv_e_vec, Experiment = backtest_muRet_sigs_e_vec)
df_x1 <- df_x1 %>% gather(Type, Backtest_mu, Conventional:Experiment)
df_x2 <- data.frame(R = Rtarg_vec, Conventional = backtest_sdRet_conv_vec, Experiment = backtest_sdRet_sigs_vec)
df_x2 <- df_x2 %>% gather(Type, Backtest_sd, Conventional:Experiment)
df_plot <- merge(df_x1, df_x2, by = c("R", "Type"))

gg <- ggplot(df_plot, aes(x = Backtest_sd, y = Backtest_mu, group = Type, color = Type))
gg <- gg + geom_point()
gg

# df_plot <- data.frame(Return = Rtarg_vec, Conventional = backtest_muRet_conv_e_vec, Experiment = backtest_muRet_sigs_e_vec)
# df_plot <- df_plot %>% gather(Type, Backtest, Conventional:Experiment)
# gg <- ggplot(df_plot, aes(x = Return, y = Backtest, group = Type, color = Type))
# gg <- gg + geom_point()
# gg
# 
# df_plot <- data.frame(Return = Rtarg_vec, Conventional = backtest_sdRet_conv_e_vec, Experiment = backtest_sdRet_sigs_e_vec)
# df_plot <- df_plot %>% gather(Type, Backtest, Conventional:Experiment)
# gg <- ggplot(df_plot, aes(x = Return, y = Backtest, group = Type, color = Type))
# gg <- gg + geom_point()
# gg



# df_x1 <- data.frame(R = Rtarg_vec, Conventional = backtest_sdRet_conv_vec, Experiment = backtest_sdRet_sigs_vec)
# df_x1 <- df_x1 %>% gather(Type, Backtest_sd, Conventional:Experiment)
# df_x2 <- data.frame(R = Rtarg_vec, Conventional = sqrt(V_conv_vec), Experiment = sqrt(V_sigs_vec))
# df_x2 <- df_x2 %>% gather(Type, Calculated_sd, Conventional:Experiment)
# df_plot <- merge(df_x1, df_x2, by = c("R", "Type"))
# gg <- ggplot(df_plot, aes(x = Calculated_sd, y = Backtest_sd, group = Type, color = Type))
# gg <- gg + geom_point()
# gg

df_look <- data.frame(Return = Rtarg_vec, Conventional = backtest_sdRet_conv_vec, Conventional_sd = sqrt(V_conv_vec), Experiment = backtest_sdRet_sigs_vec, Experiment_sd = sqrt(V_sigs_vec))
df_look$Conventional / df_look$Conventional_sd
df_look$Experiment / df_look$Experiment_sd

df_look <- data.frame(Return = Rtarg_vec, Conventional = backtest_muRet_conv_vec, Experiment = backtest_muRet_sigs_vec)
df_look$Conventional / df_look$Return
df_look$Experiment / df_look$Return




df_wStack <- data.frame(sd = NA, R = NA, Security = NA, w = NA)
for(i in 1:length(Rtarg_vec)){df_wStack <- rbind(df_wStack, data.frame(sd = backtest_sdRet_sigs_e_vec[i], R = Rtarg_vec[i], Security = as.character(c(1:n_sigs)), w = c(wStar_sigs_e_list[[i]])))}
df_wStack <- df_wStack[-1, ]


# df_wStack <- data.frame(sd = NA, R = NA, Security = NA, w = NA)
# for(i in 1:length(Rtarg_vec)){df_wStack <- rbind(df_wStack, data.frame(sd = backtest_sdRet_conv_e_vec[i], R = Rtarg_vec[i], Security = row.names(mat_Loads), w = c(wStar_conv_e_list[[i]])))}
# df_wStack <- df_wStack[-1, ]


gg <- ggplot(df_wStack, aes(x = sd, y = w)) + geom_area(aes(fill = Security))
#gg <- gg + geom_vline(xintercept = df$Risk[ind_opt], color = "green", lwd = 2)
gg <- gg + labs(x = "Potfolio Risk (s.d.)", y = "Project Budget Weights")
#gg <- gg + theme(legend.position = "none")
gg


```

## Discussion and conculsions

* When tackling complex, seemingly intangible, subjects such as "sustainability", or "resilience", or, in the case of this paper, "tradeoffs between strategic objectives", progress is often made by attempting to formalize---or otherwise bring down to earth---a longstanding, high level (and, at times, politically driven), conceptual narrative of the issue in terms of measurable inputs and outputs. Regardless of the degree of success in bringing the problem down to earth, the quantitative grappling with it forces us to examine it in greater granularity than is possible in the high level narrative. New problems, and/or connections to old problems, are invariably discovered along the way, unnoticed until now due to fixation on certain aspects of the problem that are emphasized in the conceptual narrative. [It is worth mentioning that this process is not subject to political pressures, and does not allow us to substitute real progress on the problem with conferences and rhetoric.]

* In the quantitative grappling presented here, it was discovered that, before the question of tradeoffs can even be asked, one must first address the question of dimensions. There is not just one set of tradeoffs, but a set for each dimension found in the data. The task of identifying and extracting dimensions is, then, a whole can of worms unto itself. The number of dimensions that can meaningfully be analyzed is a function both of the real structure that may or may not exist in the data, and of the amount of available data. Amazingly, there is a rigorous method of dimension extraction that takes all of this into account.

* Once dimensions are extracted, they must then be characterized in concrete terms. What aspect of reality is described by each dimension? This is achieved by applying a varimax rotation to the loadings matrix. If clear thematic trends fail to emerge after varimax rotation, this is motivation for a deep reassessment of underlying preconceptions and/or the quality of the data.

* Once dimensions have been characterized, the question of tradeoffs can be addressed.

* Having identified tradeoffs, the natural next question from a donor's perspective is: how to optimize investment across these tradeoffs? ...which then turns out to be intimately bound up with the issue of risk...portfolio optimization. An unsuspected opportunity.






## scraps:


```{r, echo=F}

Signals_from_noise <- function(mat_PCA_in, varimax_rot = T,
                               list_groups = NULL,
                               group_names = NULL,
                               pca_var_plot = F,
                               pca_ind_plot = F,
                               eigenvalue_density_plot = F,
                               loadings_plot = T,
                               sig_ts_plot = F,
                               sig_corr_plot = F,
                               yearly = F){
  varNames_ordered <- colnames(mat_PCA_in)
  date_vec <- row.names(mat_PCA_in)
  if(yearly){date_vec <- as.integer(date_vec)}
  n_obs <- ncol(mat_PCA_in)
  #----
  if(!is.null(list_groups)){
    group_vec <- rep(NA, n_obs)
    for(i in 1:length(list_groups)){
      this_group_vec <- list_groups[[i]]
      this_group_name <- group_names[i]
      group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
      
    }
  }
  #----
  if(pca_ind_plot){
    res <- FactoMineR::PCA(t(mat_PCA_in), graph = F)
    gg <- factoextra::fviz_pca_ind(res, habillage = factor(group_vec), addEllipses = T)
    print(gg)
  }
  #----
  res <- FactoMineR::PCA(mat_PCA_in, ncp = ncol(mat_PCA_in), graph = F)
  #----
  if(pca_var_plot){
    gg <- factoextra::fviz_pca_var(res, habillage = factor(group_vec))
    print(gg)
  }
  # library(mclust)
  # mc <- Mclust(t(mat_PCA_in))
  # summary(mc)
  # View(mc$classification)
  # factoextra::fviz_cluster(mc, frame.type = "norm", geom = "text")
  #--Extraction of signals (main PCs)
  eigvals <- as.data.frame(res$eig)$eigenvalue
  eigval_max <- max(eigvals)
  N_t <- nrow(mat_PCA_in)
  N_c <- ncol(mat_PCA_in)
  Q <- N_t / N_c
  s_sq <- 1 - eigval_max / N_c
  #s_sq <- 1
  eigval_rand_max <- s_sq * (1 + 1 / Q + 2 / sqrt(Q))
  eigval_rand_min <- s_sq * (1 + 1 / Q - 2 / sqrt(Q))
  lam <- seq(eigval_rand_min, eigval_rand_max, 0.001)
  dens_rand <- Q / (2 * pi * s_sq) * sqrt((eigval_rand_max - lam) * (lam - eigval_rand_min)) / lam
  df_e <- data.frame(eigenvalues = eigvals)
  #--Eigenvalue density vs. random matrix eigenvalue density
  if(eigenvalue_density_plot){
    gg <- ggplot()
    gg <- gg + geom_density(data = df_e, aes(x = eigenvalues, color = "Correlation Matrix"), lwd = 1.1)
    gg <- gg + geom_line(data = data.frame(x = lam, y = dens_rand), aes(x = x, y = y, color = "Random matrix"), lwd = 1.1)
    gg <- gg + scale_colour_manual(name = "Eigenvalue density",
                                   values = c(`Correlation Matrix` = "blue", `Random matrix` = "orange"))
    print(gg)
  }
  #-----------------------------------------
  ind_deviating_from_noise <- which(eigvals > eigval_rand_max)# (eigval_rand_max + 5 * 10^-1))
  mat_Loads <- res$var$coord
  mat_signal_Loads <- mat_Loads[, ind_deviating_from_noise]
  mat_Loads_rot <- varimax(mat_Loads)[[1]]
  mat_signal_Loads_rot <- mat_Loads_rot[, ind_deviating_from_noise]
  mat_eigvecs <-  mat_Loads %*% diag(1 / sqrt(eigvals))
  eigvals_sig <- eigvals[ind_deviating_from_noise]
  n_signals <- length(eigvals_sig)
  print(paste("Number of signals: ", n_signals))
  mat_signal_eigvecs <- mat_eigvecs[, ind_deviating_from_noise]
  mat_signal_ts <- mat_PCA_in %*% mat_signal_eigvecs
  if(n_signals == 1){
    mat_signal_ts <- mat_signal_ts / eigvals_sig
  }else{
    mat_signal_ts <- mat_signal_ts %*% diag(1 / eigvals_sig)
  }
  # Set sign of eigenvectors such that they
  # best conform to the input time series
  #inData_avg <- mat_PCA_in %*% rep(1, n_obs) * 1 / n_obs
  inData_avg <- rowMeans(mat_PCA_in)
  if(n_signals == 1){
    #sse <- sum((mat_signal_ts - inData_avg)^2)
    #sse_neg <- sum((-mat_signal_ts - inData_avg)^2)
    mse <- mean((mat_signal_ts - inData_avg)^2)
    mse_neg <- mean((-mat_signal_ts - inData_avg)^2)
    if(mse_neg < mse){
      mat_signal_eigvecs <- -mat_signal_eigvecs
      mat_signal_ts <- -mat_signal_ts
    }
  }else{
    for(i in 1:n_signals){
      # sse <- sum((mat_signal_ts[, i] - inData_avg)^2)
      # sse_neg <- sum((-mat_signal_ts[, i] - inData_avg)^2)
      # sse_vec <- c(sse, sse_neg)
      mse <- mean((mat_signal_ts[, i] - inData_avg)^2)
      mse_neg <- mean((-mat_signal_ts[, i] - inData_avg)^2)
      if(mse_neg < mse){
        mat_signal_eigvecs[, i] <- -mat_signal_eigvecs[, i]
        mat_signal_ts[, i] <- -mat_signal_ts[, i]
      }
    }
    
  }
  #------------------------------------------------------------
  # Plot loadings barcharts
  if(varimax_rot){
    df_plot <- data.frame(ts_id = varNames_ordered, mat_signal_Loads_rot)
  }else{
    df_plot <- data.frame(ts_id = varNames_ordered, mat_signal_Loads)
  }
  signal_id <- paste("Signal", c(1:n_signals))
  colnames(df_plot)[2:(n_signals + 1)] <- signal_id
  gathercols <- as.character(signal_id)
  df_plot <- gather_(df_plot, "Signal", "Loading", gathercols)
  #--
  if(!is.null(list_groups)){
    # df_plot$Type <- NA
    # u <- as.character(df_plot$ts)
    # for(i in 1:length(list_groups)){
    #   ind <- which(u %in% list_groups[[i]])
    #   df_plot$Type[ind] <- group_names[i]
    # }
    # df_plot$Type <- as.factor(df_plot$Type)
    df_plot$Type <- factor(group_vec)
    xx <- df_plot$Type
    df_plot$ts_id <- factor(df_plot$ts_id, levels = unique(df_plot$ts_id[order(xx)]))
    gg <- ggplot(df_plot, aes(x = ts_id, y = Loading, fill = Type))
  }else{
    gg <- ggplot(df_plot, aes(x = ts_id, y = Loading))
  }
  #--
  gg <- gg + geom_bar(stat = "identity", position = "dodge")
  gg <- gg + facet_wrap(~ Signal, nrow = 1)
  #gg <- gg + labs(title = )
  gg <- gg + theme(axis.text.y = element_text(face = "bold", size = 10),
                   axis.text.x = element_text(face = "bold", size = 10),
                   axis.title.y = element_text(face = "bold", size = 10),
                   axis.title.x = element_text(face = "bold", size = 10))
  gg <- gg + coord_equal()
  # if(N_c <= 50){
  #   gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1),
  #                    axis.title.y = element_blank())
  # }else{
  #   gg <- gg + theme(axis.text.x = element_blank(),
  #                    axis.title.y = element_blank())
  # }
  gg <- gg + coord_flip()
  if(loadings_plot){print(gg)}
  #---------------------------------
  # Plot signal ts against average
  df_plot1 <- data.frame(Date = date_vec, inData_avg)
  df_plot2 <- data.frame(Date = date_vec, mat_signal_ts)
  df_plot1$Date <- factor(format(df_plot1$Date, format = "%y-%m-%d%"), ordered = T)
  df_plot2$Date <- factor(format(df_plot2$Date, format = "%y-%m-%d%"), ordered = T)
  xAxis_labels <- df_plot1$Date[seq(1, nrow(df_plot1), length.out = 10)]
  signal_id <- paste("Signal", c(1:n_signals))
  colnames(df_plot2)[2:(n_signals + 1)] <- signal_id
  gathercols <- signal_id
  df_plot2 <- df_plot2 %>% gather_("Signal", "Value", gathercols)
  gg <- ggplot()
  gg <- gg + geom_line(data = df_plot1, aes(x = Date, y = inData_avg, group = 1), color = "orange", lwd = 2)
  gg <- gg + geom_line(data = df_plot2, aes(x = Date, y = Value, group = 1))
  gg <- gg + scale_x_discrete(breaks = xAxis_labels)
  gg <- gg + facet_wrap(~ Signal, ncol = 1)
  gg <- gg + theme(axis.title.y = element_blank())
  gg
  if(sig_ts_plot){print(gg)}
  #--Correlation matrix
  if(sig_corr_plot){
    # t(mat_signal_ts) %*% mat_signal_ts
    # t(mat_signal_eigvecs) %*% mat_signal_eigvecs
    rcorr_out <- rcorr(mat_signal_ts)
    cormat <- rcorr_out$r
    print(cormat)
    pmat <- rcorr_out$P
    corrplot(cormat, type="upper", order="hclust", p.mat = pmat, sig.level = 0.01, insig = "blank", tl.col = "black", tl.srt = 45)
    
  }
  #---------------------------------
  df_sig_ts <- data.frame(date = date_vec, ts_sigs = mat_signal_ts)
  #df_sig_eigvecs <- data.frame(Loadings_sigs_rot = mat_signal_Loads_rot)
  list_out <- list(df_sig_ts, mat_signal_eigvecs, mat_eigvecs, mat_signal_Loads, mat_signal_Loads_rot, eigvals_sig, eigvals)
  return(list_out)
}



# wStar_normd <- wStar_sig1 / max(abs(wStar_sig1))
# #w_star_normd <- wStar_trad / max(abs(wStar_trad))
# varNames_ordered <- colnames(mat_PCA_in)
# df_plot <- data.frame(ts_id = varNames_ordered, mat_Loads_rot, wStar_normd)
# signal_id <- paste("Signal", c(1:n_sigs))
# colnames(df_plot)[2:(n_sigs + 1)] <- signal_id
# colnames(df_plot)[ncol(df_plot)] <- "Portfolio weights"
# gathercols <- colnames(df_plot)[-1]
# df_plot <- gather_(df_plot, "Signal", "Loading", gathercols)
# #--
# n_obs <- ncol(mat_PCA_in)
# group_vec <- rep(NA, n_obs)
# for(i in 1:length(list_groups)){
#   this_group_vec <- list_groups[[i]]
#   this_group_name <- group_names[i]
#   group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
# 
# }
# 
# df_plot$Type <- factor(group_vec)
# xx <- df_plot$Type
# df_plot$ts_id <- factor(df_plot$ts_id, levels = unique(df_plot$ts_id[order(xx)]))
# gg <- ggplot(df_plot, aes(x = ts_id, y = Loading, fill = Type))
# #--
# gg <- gg + geom_bar(stat = "identity", position = "dodge")
# gg <- gg + facet_wrap(~ Signal, nrow = 1)
# #gg <- gg + labs(title = )
# gg <- gg + theme(axis.text.y = element_text(face = "bold", size = 10),
#                  axis.text.x = element_text(face = "bold", size = 10),
#                  axis.title.y = element_text(face = "bold", size = 10),
#                  axis.title.x = element_text(face = "bold", size = 10))
# gg <- gg + coord_flip()
# gg



# 

```


<!-- Little or no care is taken, however, to address the tradeoffs that exist between investments in each of these thematic areas. It is well known, for example, that agriculture is often detrimental to the environment. This implies a tradeoff between the food security and environmental objectives. Economic growth, in its turn, is inversely related to both agricultural and environmental objectives. In many parts of the world, there is also a longstanding tradeoff between economic growth and economic equality. Investments in one thematic area can thus offset returns to investments in other thematic areas. -->

<!-- Moreover, conventional -->

<!-- ### I.ii Proposed solution: a PCA approach -->

<!-- In this concept note, I propose a rigorous, precise method for identifying and quantifying tradeoffs and synergies between strategic objectives (SOs). The method is based on principle components analysis (PCA) of a large dataset of development indicators spanning the usual thematic areas. (For a good overview of PCA, see Abdi & Williams (2010).) A variant of this approach has been applied in the analysis of financial market time series (Gopikrishnan et al., 2001). Here I enhance such precedents by also leveraging a rigorous signal selecting technique developed in the study of physical systems (Dehesa et al., 1983). -->
